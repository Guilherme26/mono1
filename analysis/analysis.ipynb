{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../pipeline/\")\n",
    "import preprocessing\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from gcn import GCNModel\n",
    "from torch_geometric.data import Data\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "profiles = pd.read_csv(\"../data/new_profiles_200t.csv\")\n",
    "comments = pd.read_csv(\"../data/new_comments_200t.csv\")\n",
    "\n",
    "comments = comments.drop_duplicates()\n",
    "profiles = preprocessing.categorical_to_numerical(profiles, col=\"category_1\")\n",
    "all_users = set(profiles.profile_username.values)\n",
    "\n",
    "data = preprocessing.scale(profiles.drop(columns=[\"category_1\", \"profile_username\"]).values)\n",
    "name_to_record = {name: record for name, record in zip(all_users, data)}\n",
    "\n",
    "input_dim, output_dim = data.shape[1], len(profiles.category_1.unique()) + 1\n",
    "user_to_label = {user: category for user, category in profiles[[\"profile_username\", \"category_1\"]].values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> (1/100) Running loss: 1.6746314764022827\n",
      "---> (2/100) Running loss: 1.4904470443725586\n",
      "---> (3/100) Running loss: 1.4438430070877075\n",
      "---> (4/100) Running loss: 1.4253123998641968\n",
      "---> (5/100) Running loss: 1.4152958393096924\n",
      "---> (6/100) Running loss: 1.4089057445526123\n",
      "---> (7/100) Running loss: 1.4018458127975464\n",
      "---> (8/100) Running loss: 1.3929702043533325\n",
      "---> (9/100) Running loss: 1.381588339805603\n",
      "---> (10/100) Running loss: 1.3684884309768677\n",
      "---> (11/100) Running loss: 1.3540241718292236\n",
      "---> (12/100) Running loss: 1.338917851448059\n",
      "---> (13/100) Running loss: 1.324125051498413\n",
      "---> (14/100) Running loss: 1.3099943399429321\n",
      "---> (15/100) Running loss: 1.296974539756775\n",
      "---> (16/100) Running loss: 1.285631537437439\n",
      "---> (17/100) Running loss: 1.274461030960083\n",
      "---> (18/100) Running loss: 1.2626692056655884\n",
      "---> (19/100) Running loss: 1.2503187656402588\n",
      "---> (20/100) Running loss: 1.2377372980117798\n",
      "---> (21/100) Running loss: 1.2246167659759521\n",
      "---> (22/100) Running loss: 1.2104144096374512\n",
      "---> (23/100) Running loss: 1.1947287321090698\n",
      "---> (24/100) Running loss: 1.177992820739746\n",
      "---> (25/100) Running loss: 1.161249041557312\n",
      "---> (26/100) Running loss: 1.1448578834533691\n",
      "---> (27/100) Running loss: 1.1289830207824707\n",
      "---> (28/100) Running loss: 1.113405704498291\n",
      "---> (29/100) Running loss: 1.097830891609192\n",
      "---> (30/100) Running loss: 1.0822662115097046\n",
      "---> (31/100) Running loss: 1.066758394241333\n",
      "---> (32/100) Running loss: 1.0511252880096436\n",
      "---> (33/100) Running loss: 1.0354461669921875\n",
      "---> (34/100) Running loss: 1.0200320482254028\n",
      "---> (35/100) Running loss: 1.0049699544906616\n",
      "---> (36/100) Running loss: 0.990063488483429\n",
      "---> (37/100) Running loss: 0.9757292866706848\n",
      "---> (38/100) Running loss: 0.9622835516929626\n",
      "---> (39/100) Running loss: 0.9494967460632324\n",
      "---> (40/100) Running loss: 0.9374125003814697\n",
      "---> (41/100) Running loss: 0.9260597825050354\n",
      "---> (42/100) Running loss: 0.9153527021408081\n",
      "---> (43/100) Running loss: 0.9051386713981628\n",
      "---> (44/100) Running loss: 0.8950703740119934\n",
      "---> (45/100) Running loss: 0.8852865099906921\n",
      "---> (46/100) Running loss: 0.8757703900337219\n",
      "---> (47/100) Running loss: 0.8663043975830078\n",
      "---> (48/100) Running loss: 0.856827437877655\n",
      "---> (49/100) Running loss: 0.8475958108901978\n",
      "---> (50/100) Running loss: 0.8385930061340332\n",
      "---> (51/100) Running loss: 0.8296734690666199\n",
      "---> (52/100) Running loss: 0.8210291266441345\n",
      "---> (53/100) Running loss: 0.8126463890075684\n",
      "---> (54/100) Running loss: 0.8043143153190613\n",
      "---> (55/100) Running loss: 0.7960091233253479\n",
      "---> (56/100) Running loss: 0.7880391478538513\n",
      "---> (57/100) Running loss: 0.7805771231651306\n",
      "---> (58/100) Running loss: 0.7740136981010437\n",
      "---> (59/100) Running loss: 0.7671290636062622\n",
      "---> (60/100) Running loss: 0.7599688768386841\n",
      "---> (61/100) Running loss: 0.7534485459327698\n",
      "---> (62/100) Running loss: 0.747521698474884\n",
      "---> (63/100) Running loss: 0.7418946623802185\n",
      "---> (64/100) Running loss: 0.7361719012260437\n",
      "---> (65/100) Running loss: 0.7303574085235596\n",
      "---> (66/100) Running loss: 0.7247006893157959\n",
      "---> (67/100) Running loss: 0.719375729560852\n",
      "---> (68/100) Running loss: 0.7143235206604004\n",
      "---> (69/100) Running loss: 0.7093650102615356\n",
      "---> (70/100) Running loss: 0.7043068408966064\n",
      "---> (71/100) Running loss: 0.6991901397705078\n",
      "---> (72/100) Running loss: 0.694249153137207\n",
      "---> (73/100) Running loss: 0.6894903182983398\n",
      "---> (74/100) Running loss: 0.6847886443138123\n",
      "---> (75/100) Running loss: 0.6800124049186707\n",
      "---> (76/100) Running loss: 0.6752735376358032\n",
      "---> (77/100) Running loss: 0.670677125453949\n",
      "---> (78/100) Running loss: 0.6660906672477722\n",
      "---> (79/100) Running loss: 0.6614606380462646\n",
      "---> (80/100) Running loss: 0.6568304300308228\n",
      "---> (81/100) Running loss: 0.6522619724273682\n",
      "---> (82/100) Running loss: 0.6477441787719727\n",
      "---> (83/100) Running loss: 0.6432048678398132\n",
      "---> (84/100) Running loss: 0.6386501789093018\n",
      "---> (85/100) Running loss: 0.6344282627105713\n",
      "---> (86/100) Running loss: 0.6303970217704773\n",
      "---> (87/100) Running loss: 0.6263034343719482\n",
      "---> (88/100) Running loss: 0.6215643286705017\n",
      "---> (89/100) Running loss: 0.6172078251838684\n",
      "---> (90/100) Running loss: 0.6131997108459473\n",
      "---> (91/100) Running loss: 0.6094356179237366\n",
      "---> (92/100) Running loss: 0.6057930588722229\n",
      "---> (93/100) Running loss: 0.6019324064254761\n",
      "---> (94/100) Running loss: 0.5975629091262817\n",
      "---> (95/100) Running loss: 0.5941044688224792\n",
      "---> (96/100) Running loss: 0.5913922786712646\n",
      "---> (97/100) Running loss: 0.586746871471405\n",
      "---> (98/100) Running loss: 0.5831248760223389\n",
      "---> (99/100) Running loss: 0.5803748965263367\n",
      "---> (100/100) Running loss: 0.5768125057220459\n"
     ]
    }
   ],
   "source": [
    "authors = profiles.profile_username.values\n",
    "\n",
    "username_to_index = utils.get_users_indices(authors)\n",
    "interactions = utils.get_interactions(comments[comments.media_author.isin(authors) & comments.commenter.isin(authors)], username_to_index)\n",
    "x, y = utils.get_x(authors, name_to_record, input_dim=input_dim), utils.get_y(user_to_label, authors)\n",
    "\n",
    "edge_index = utils.get_edge_index(interactions)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = Data(x=x, y=y, edge_index=edge_index).to(device)\n",
    "\n",
    "gcn = GCNModel(x.shape[1], 64, 5, lr=0.005, n_hidden_layers=2)\n",
    "history = gcn.fit(data, epochs=100)\n",
    "\n",
    "embeddings = gcn.forward(data.x, data.edge_index, apply_activation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "category1 = profiles[profiles.category_1 == 1].index.values\n",
    "category2 = profiles[profiles.category_1 == 2].index.values\n",
    "category3 = profiles[profiles.category_1 == 3].index.values\n",
    "category4 = profiles[profiles.category_1 == 4].index.values\n",
    "\n",
    "group_indices = [category1, category2, category3, category4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data(data, n_dim=2):\n",
    "    return PCA(n_components=n_dim, random_state=0).fit_transform(data)\n",
    "\n",
    "\n",
    "def plot_2d(data, group_indices, legends):\n",
    "    for group in group_indices:\n",
    "        plt.scatter(data[group][:, :1], data[group][:, 1:])\n",
    "    plt.legend(legends)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_3d(data, group_indices, legends):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    for group in group_indices:\n",
    "        ax.scatter(data[group][:, :1], data[group][:, 1:2], data[group][:, 2:])\n",
    "    plt.legend(legends)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = reduce_data(embeddings.detach().numpy(), n_dim=2)\n",
    "plot_2d(data, group_indices, [\"Politics\", \"Sport\", \"Music\", \"Show\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = reduce_data(embeddings.detach().numpy(), n_dim=3)\n",
    "plot_3d(data, group_indices, [\"Politics\", \"Sport\", \"Music\", \"Show\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = preprocessing.scale(profiles.drop(columns=[\"category_1\", \"profile_username\"]).values)\n",
    "data = reduce_data(data, n_dim=2)\n",
    "plot_2d(data, group_indices, [\"Politics\", \"Sport\", \"Music\", \"Show\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = preprocessing.scale(profiles.drop(columns=[\"category_1\", \"profile_username\"]).values)\n",
    "data = reduce_data(data, n_dim=3)\n",
    "plot_3d(data, group_indices, [\"Politics\", \"Sport\", \"Music\", \"Show\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rcParams[\"figure.figsize\"] = (12, 6)\n",
    "category_to_numerical = {\"Unknown Users\": 0, \"Politics\": 1, \"Sport\": 2, \"Music\": 3, \"Show\": 4}\n",
    "\n",
    "df_values = profiles.drop([\"profile_username\"], axis=1)\n",
    "df_category = df_values[df_values.category_1 == category_to_numerical[\"Unknown Users\"]]\n",
    "df_category = pd.DataFrame(preprocessing.scale(df_category.values), columns=df_category.columns)\\\n",
    "                    .drop([\"category_1\", \"is_tracked\"], axis=1)\n",
    "\n",
    "x = df_category.columns\n",
    "y = []\n",
    "for col in x:\n",
    "    y.append(np.mean(df_category[col].values))\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.xticks(rotation=90, fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.title(\"Unknown Users\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzUAAAIrCAYAAADWR7ezAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xm4JVV57/Hvz24GFXCiNcrUqDgQZxtwylWjJiAGkqgJXDXihEMw5ur12sZcBzQRNXGIkijOwSjBKbaCQeOAA4KMMoo2CAJGRUTxOjG994+qA5vD6T4bTtXep3Z/P8/TT5+qXafeVd377Pe8tVatlapCkiRJkobqFtNugCRJkiQthUWNJEmSpEGzqJEkSZI0aBY1kiRJkgbNokaSJEnSoFnUSJIkSRo0ixppEUleneRD7dc7Jvl/SVZs5Pj/l+Suk2uhJEnSps2iRpuUJBck+XVbePwoyQeSbDXu91fV96tqq6q6pj3fl5M8e94xW1XV+V23XZK0aUryiCTHJfl5kp8m+XqS3XqIc91NPGloLGq0KfqjqtoKeBCwBvjbKbdHkqQFJdkG+AzwduD2wHbAa4DfdhxnZZfnkybNokabrKq6BPgscJ8kd0myrr0Dtj7Jcxb6niSrk1SSlUn+Dvg94B1tz8872mMqyd3br2+Z5B+TXNjeYftau2/LJB9KclmSnyU5McmdJnXtkqTBuAdAVX2kqq6pql9X1eeq6vQkB7S9Nu9oc8y3kzxm7hs3ltvaXpmPtbnoCuB5wN8Af97mtG9N/EqlJbAq1yYryQ7A44FPAEcAZwJ3Ae4FfD7JeVX1xQ19f1W9IsnDgQ9V1Xs2cNg/AL8LPAz4IbAHcC1wAHAbYAeau20PAH7dwWVJkmbLd4BrknyQJlcdX1WXj7y+B/AxYFvgT4FPJNm5qn7K4rltX+DJwF8AW7TnuHtVPXUC1yV1yp4abYr+I8nPgK8BxwKHAQ8HXlZVv6mq04D30HzI32xJbgE8E3hRVV3S3mE7rqp+C1wF3IEmeVxTVSdX1RVLiSdJmj1tbngEUMC7gUvb3pe53v0fA2+tqquq6t+Bc4G92xt3i+W2b1TVf1TVtVXljTUNmkWNNkV/XFW3raqdquoFNHewflpVvxg55kKacctLsS2wJXDeAq8dDhwDHJHkB0nemGSzJcaTJM2gqjqnqg6oqu2B+9Dkrbe2L19SVTVy+IXt6+Pktot6bLY0URY1EvwAuH2SrUf27QhcMsb31kZe+wnwG+BuN/qm5o7aa6pqV5qhaU9giT1DkqTZV1XfBj5AU9wAbJckI4fsSJPXxslt83PYxnKatKxZ1GiTV1UXAccBr28f4L8f8CxgnGktfwQsuCZNVV0LvA94c/uw5ookD02yRZJHJ7lvu97NFTTD0a7t5IIkSTMjyb2SvCTJ9u32DsD+wPHtIXcE/irJZkmeDNwbOPpm5rYfAavb4dPSoPimlRr7A6tp7mx9EnhVVf3XGN/3NuBJSS5P8k8LvP6/gTOAE4GfAm+g+bn7HZoHO68AzqF5tufwJV6DJGn2/IJmMoATkvySppg5E3hJ+/oJwC40owP+DnhSVV3WvnZTc9tH278vS3JKlxch9S03HIYpSZKkIUhyAPDsqnrEtNsiTZs9NZIkSZIGzaJGkiRJ0qA5/EySJEnSoNlTI0mSJGnQLGokSZIkDdrKaQXedttta/Xq1dMKL0kCTj755J9U1appt2M5Mk9J0vSNm6emVtSsXr2ak046aVrhJUlAkgun3YblyjwlSdM3bp5y+JkkSZKkQbOokSRJkjRoFjWSJEmSBs2iRpIkSdKgWdRIkiRJGjSLGkmSJEmDZlEjSZIkadAsaiRJkiQNmkWNJEmSpEEbq6hJsmeSc5OsT7J2gdd3TPKlJKcmOT3J47tvqiRJkiTd2KJFTZIVwKHAXsCuwP5Jdp132N8CR1bVA4H9gH/uuqGSJEmStJBxemp2B9ZX1flVdSVwBLDvvGMK2Kb9+jbAD7proiRJkiRt2MoxjtkOuGhk+2Jgj3nHvBr4XJIXArcGHttJ6yRJkiRpEV1NFLA/8IGq2h54PHB4khudO8mBSU5KctKll17aUWhJkrphnpKkYRqnqLkE2GFke/t236hnAUcCVNU3gC2BbeefqKoOq6o1VbVm1apVN6/FkiT1xDwlScM0TlFzIrBLkp2TbE4zEcC6ecd8H3gMQJJ70xQ13uKSJEmS1LtFi5qquho4CDgGOIdmlrOzkhycZJ/2sJcAz0nyLeAjwAFVVX01WpIkSZLmjDNRAFV1NHD0vH2vHPn6bODh3TZNkiRJkhbX1UQBkiRJkjQVFjWSJEmSBs2iRpIkSdKgWdRIkiRJGjSLGkmSJEmDZlEjSZIkadAsaiRJkiQNmkWNJEmSpEGzqJEkSZI0aBY1kiRJkgbNokaSJEnSoFnUSJIkSRo0ixpJkiRJg2ZRI0mSJGnQLGokSZIkDZpFjSRJkqRBs6iRJEmSNGgWNZIkSZIGzaJGkiRJ0qBZ1EiSJEkaNIsaSZIkSYNmUSNJkiRp0CxqJEmSJA2aRY0kSZKkQbOokSRJkjRoFjWSJEmSBs2iRpIkSdKgWdRIkiRJGjSLGkmSJEmDZlEjSZIkadAsaiRJkiQN2lhFTZI9k5ybZH2StRs45s+SnJ3krCQf7raZkiRJkrSwlYsdkGQFcCjwOOBi4MQk66rq7JFjdgFeDjy8qi5Pcse+GixJkiRJo8bpqdkdWF9V51fVlcARwL7zjnkOcGhVXQ5QVT/utpmSJEmStLBxiprtgItGti9u9426B3CPJF9PcnySPRc6UZIDk5yU5KRLL7305rVYkqSemKckaZi6mihgJbAL8Chgf+DdSW47/6CqOqyq1lTVmlWrVnUUWpKkbpinJGmYxilqLgF2GNnevt036mJgXVVdVVXfA75DU+RIkiRJUq/GKWpOBHZJsnOSzYH9gHXzjvkPml4akmxLMxzt/A7bKUmSJEkLWrSoqaqrgYOAY4BzgCOr6qwkByfZpz3sGOCyJGcDXwJeWlWX9dVoSZIkSZqz6JTOAFV1NHD0vH2vHPm6gBe3fyRJkiRpYrqaKECSJEmSpsKiRpIkSdKgWdRIkiRJGjSLGkmSJEmDZlEjSZIkadAsaiRJkiQNmkWNJEmSpEGzqJEkSZI0aBY1kiRJkgbNokaSJEnSoFnUSJIkSRo0ixpJkiRJg2ZRI0mSJGnQLGokSZIkDZpFjSRJkqRBs6iRJEmSNGgWNZIkSZIGzaJGkiRJ0qBZ1EiSJEkatJXTbsDNtXrtUb2c94JD9u7lvJIkSZsKf0/TpNlTI0mSJGnQLGokSZIkDZpFjSRJkqRBs6iRJEmSNGgWNZIkSZIGzaJGkiRJ0qBZ1EiSJEkaNIsaSZIkSYNmUSNJkiRp0CxqJEmSJA3aWEVNkj2TnJtkfZK1GznuiUkqyZrumihJkiRJG7ZoUZNkBXAosBewK7B/kl0XOG5r4EXACV03UpIkSZI2ZJyemt2B9VV1flVdCRwB7LvAca8F3gD8psP2SZIkSdJGrRzjmO2Ai0a2Lwb2GD0gyYOAHarqqCQv7bB9kiQN0uq1R/Vy3gsO2buX80rSkC15ooAktwDeDLxkjGMPTHJSkpMuvfTSpYaWJKlT5ilJGqZxippLgB1Gtrdv983ZGrgP8OUkFwAPAdYtNFlAVR1WVWuqas2qVatufqslSeqBeUqShmmcouZEYJckOyfZHNgPWDf3YlX9vKq2rarVVbUaOB7Yp6pO6qXFkiRJkjRi0aKmqq4GDgKOAc4Bjqyqs5IcnGSfvhsoSZIkSRszzkQBVNXRwNHz9r1yA8c+aunN0qT4IOsw+f8mSZJ0vSVPFCBJkiRJ02RRI0mSJGnQLGokSZIkDZpFjSRJkqRBs6iRJEmSNGhjzX4mSVoaZ6yTJKk/9tRIkiRJGjSLGkmSJEmDZlEjSZIkadB8pkaSJEkaUx/PSPp85NJZ1EiSpGXLSTYkjcPhZ5IkSZIGzZ4aSZJmgD0akjZl9tRIkiRJGjSLGkmSJEmDZlEjSZIkadAsaiRJkiQNmkWNJEmSpEGzqJEkSZI0aE7pLEmSJG3ihj4tvD01kiRJkgbNokaSJEnSoFnUSJIkSRo0ixpJkiRJg2ZRI0mSJGnQLGokSZIkDZpFjSRJkqRBs6iRJEmSNGgWNZIkSZIGzaJGkiRJ0qBZ1EiSJEkatLGKmiR7Jjk3yfokaxd4/cVJzk5yepIvJNmp+6ZKkiRJ0o0tWtQkWQEcCuwF7Arsn2TXeYedCqypqvsBHwPe2HVDJUmSJGkh4/TU7A6sr6rzq+pK4Ahg39EDqupLVfWrdvN4YPtumylJkiRJCxunqNkOuGhk++J234Y8C/jsQi8kOTDJSUlOuvTSS8dvpSRJE2CekqRh6nSigCRPBdYAb1ro9ao6rKrWVNWaVatWdRlakqQlM09J0jCtHOOYS4AdRra3b/fdQJLHAq8AHllVv+2meZIkSZK0ceP01JwI7JJk5ySbA/sB60YPSPJA4F3APlX14+6bKUmSJEkLW7SnpqquTnIQcAywAnhfVZ2V5GDgpKpaRzPcbCvgo0kAvl9V+/TYbkmSJGmmrV57VC/nveCQvXs57zSNM/yMqjoaOHrevleOfP3YjtslSZIkSWPpdKIASZIkSZo0ixpJkiRJg2ZRI0mSJGnQLGokSZIkDZpFjSRJkqRBs6iRJEmSNGgWNZIkSZIGzaJGkiRJ0qBZ1EiSJEkatJXTboAkzVm99qheznvBIXv3cl5JkrQ82FMjSZIkadAsaiRJkiQNmkWNJEmSpEGzqJEkSZI0aBY1kiRJkgbN2c8kSZJazsIoDZNFjSRJ0oyzWNOss6iROjDLyWKWr02SNHzmKYHP1EiSJEkaOIsaSZIkSYNmUSNJkiRp0CxqJEmSJA2aEwWMqY+H0HwATZIkSVo6e2okSZIkDZpFjSRJkqRBc/iZpE2WaxtImiY/g6TuWNQsQ37ISZIkSeOzqJEkSTeJN98kLTcWNZpZJl1JkqRNgxMFSJIkSRq0sXpqkuwJvA1YAbynqg6Z9/oWwL8CDwYuA/68qi7otqmaBfaeSJIkqWuL9tQkWQEcCuwF7Arsn2TXeYc9C7i8qu4OvAV4Q9cNlSRJkqSFjDP8bHdgfVWdX1VXAkcA+847Zl/gg+3XHwMekyTdNVOSJEmSFjZOUbMdcNHI9sXtvgWPqaqrgZ8Dd+iigZIkSZK0MamqjR+QPAnYs6qe3W4/Ddijqg4aOebM9piL2+3z2mN+Mu9cBwIHtpv3BM7t6kIkSTfLTlW1atqNWC7MU5K07IyVp8aZKOASYIeR7e3bfQsdc3GSlcBtaCYMuIGqOgw4bIyYkiRNnHlKkoZpnOFnJwK7JNk5yebAfsC6ecesA57efv0k4Iu1WBeQJEmSJHVg0Z6aqro6yUHAMTRTOr+vqs5KcjBwUlWtA94LHJ5kPfBTmsJHkiRJknq36DM1kiRJkrScjTP8TJIkSZKWLYsaSZIkSYNmUSNJkiRp0CxqJEmSJA2aRY0kSZKkQbOokSRJkjRoFjWSJEmSBs2iRpIkSdKgWdRIkiRJGjSLGkmSJEmDZlEjSZIkadAsaiRJkiQNmkWNJEmSpEGzqJEkSZI0aBY1Uk+SvDPJ/512OyRJsynJAUm+Nu12SMuBRY02aUkuSHJlkm3n7T81SSVZfXPPXVXPq6rXLrWNkqRNW5JHJDkuyc+T/DTJ15PsNu12ScuJRY0E3wP2n9tIcl/gVtNrjiRJjSTbAJ8B3g7cHtgOeA3w22m2S1puLGokOBz4i5HtpwP/OreR5MtJnj2yfV13fxpvSfLjJFckOSPJfdrXPpDkdSPft2+S09rjzkuyZ+9XJkkaunsAVNVHquqaqvp1VX2uqk6fOyDJPyS5PMn3kuw1sv8uSda1vTvrkzyn3b9lkl/PjVJI8ookV7cFFElem+Stk71MaWksaiQ4Htgmyb2TrAD2Az405vf+AfA/aJLObYA/Ay6bf1CS3WkKpZcCt22/54Ilt1ySNOu+A1yT5INJ9kpyu3mv7wGcC2wLvBF4b5K0rx0BXAzcBXgS8PdJfr+qfgOcCDyyPe6RwIXAw0e2j+3rgqQ+WNRIjbnemscB5wCXjPl9VwFbA/cCUlXnVNV/L3Dcs4D3VdXnq+raqrqkqr7dRcMlSbOrqq4AHgEU8G7g0rb35U7tIRdW1bur6hrgg8CdgTsl2YGmSHlZVf2mqk4D3sP1IxOOBR6ZZCVwP+Cf2u0tgd2Ar0zoEqVOWNRIjcOB/wkcwMjQs8VU1ReBdwCHAj9Octhc9/08OwDnddBOSdImpr1hdkBVbQ/ch6bnZW542A9HjvtV++VW7TE/rapfjJzqQppncqApah4FPAg4A/g8TQ/NQ4D1VXWjUQfScmZRIwFVdSHNhAGPBz4x7+VfcsOJA35n3vf+U1U9GNiVZhjaSxcIcRFwt84aLEnaJLW9/B+gKW425gfA7ZNsPbJvR64fiXAccE/gT4Bjq+rs9vXH49AzDZBFjXS9ZwG/X1W/nLf/NOBPk9wqyd3b4wBIsluSPZJsRlP8/Aa4doFzvxd4RpLHJLlFku2S3Kun65AkzYgk90rykiTbt9s70MzYefzGvq+qLqIpXF7fTgxwP5r89aH29V8BJwN/yfVFzHHA87Co0QBZ1Eitqjqvqk5a4KW3AFcCP6IZr/xvI69tQzPG+XKabv3LgDctcO5vAs9oz/VzmoSxU5ftlyTNpF/QTAZwQpJf0hQzZwIvGeN79wdW0/TafBJ4VVX918jrxwKbAd8c2d4an6fRAKWqpt0GSZIkSbrZ7KmRJEmSNGgWNZIkSZIGzaJGkiRJ0qBZ1EiSJEkatJXTCrztttvW6tWrpxVekgScfPLJP6mqVdNux3JknpKk6Rs3T02tqFm9ejUnnbTQ7LmSpElJcuG027BcmackafrGzVMOP5MkSZI0aBY1kiRJkgbNokaSJEnSoFnUSJIkSRo0ixpJkiRJg2ZRI0mSJGnQpjalsyRJs2z12qN6Oe8Fh+zdy3klacjsqZEkSZI0aBY1kiRJkgbNokaSJEnSoI1V1CTZM8m5SdYnWbvA6zsm+VKSU5OcnuTx3TdVkiRJkm5s0aImyQrgUGAvYFdg/yS7zjvsb4Ejq+qBwH7AP3fdUEmSJElayDg9NbsD66vq/Kq6EjgC2HfeMQVs0359G+AH3TVRkiRJkjZsnKJmO+Cike2L232jXg08NcnFwNHACxc6UZIDk5yU5KRLL730ZjRXkqT+mKckaZi6mihgf+ADVbU98Hjg8CQ3OndVHVZVa6pqzapVqzoKLUlSN8xTkjRM4xQ1lwA7jGxv3+4b9SzgSICq+gawJbBtFw2UJEmSpI0Zp6g5Edglyc5JNqeZCGDdvGO+DzwGIMm9aYoa++0lSZIk9W7RoqaqrgYOAo4BzqGZ5eysJAcn2ac97CXAc5J8C/gIcEBVVV+NliRJkqQ5K8c5qKqOppkAYHTfK0e+Pht4eLdNkyRJkqTFdTVRgCRJkiRNhUWNJEmSpEGzqJEkSZI0aBY1kiRJkgbNokaSJEnSoFnUSJIkSRo0ixpJkiRJg2ZRI0mSJGnQLGokSZIkDZpFjSRJkqRBs6iRJEmSNGgWNZIkSZIGzaJGkiRJ0qBZ1EiSJEkaNIsaSZIkSYO2ctoNuLlWrz2ql/NecMjevZxXkiRJUj/sqZEkSZI0aBY1kiRJkgZtsMPPpE2Zwy8lSZKuZ0+NJEmSpEGzqJEkSZI0aBY1kiRJkgbNokaSJEnSoFnUSJIkSRo0ixpJkiRJg2ZRI0mSJGnQLGokSZIkDZpFjSRJkqRBs6iRJEmSNGgWNZIkSZIGbayiJsmeSc5Nsj7J2g0c82dJzk5yVpIPd9tMSZIkSVrYysUOSLICOBR4HHAxcGKSdVV19sgxuwAvBx5eVZcnuWNfDZYkSZKkUeP01OwOrK+q86vqSuAIYN95xzwHOLSqLgeoqh9320xJkiRJWtg4Rc12wEUj2xe3+0bdA7hHkq8nOT7Jnl01UJIkSZI2ZtHhZzfhPLsAjwK2B76S5L5V9bPRg5IcCBwIsOOOO3YUWpKkbpinJGmYxumpuQTYYWR7+3bfqIuBdVV1VVV9D/gOTZFzA1V1WFWtqao1q1aturltliSpF+YpSRqmcYqaE4FdkuycZHNgP2DdvGP+g6aXhiTb0gxHO7/DdkqSJEnSghYtaqrqauAg4BjgHODIqjorycFJ9mkPOwa4LMnZwJeAl1bVZX01WpIkSZLmjPVMTVUdDRw9b98rR74u4MXtH0mSJEmamK4mCpAkSZIAWL32qF7Oe8Ehe/dyXg3fOM/USJIkSdKyZVEjSZIkadAsaiRJkiQNmkWNJEmSpEGzqJEkSZI0aBY1kiRJkgbNokaSJEnSoFnUSJIkSRo0ixpJkiRJg2ZRI0mSJGnQVk67AZIkSZKma/Xao3o57wWH7N3Leeezp0aSJEnSoFnUSJIkSRo0ixpJkiRJg2ZRI0mSJGnQLGokSZIkDZpFjSRJkqRBs6iRJEmSNGgWNZIkSZIGzaJGkiRJ0qBZ1EiSJEkaNIsaSZIkSYNmUSNJkiRp0CxqJEmSJA2aRY0kSZKkQbOokSRJkjRoFjWSJEmSBm3ltBsgSZK0IavXHtXLeS84ZO9ezitpOuypkSRJkjRoFjWSJEmSBm2s4WdJ9gTeBqwA3lNVh2zguCcCHwN2q6qTOmulJA2cQ2gkSerPoj01SVYAhwJ7AbsC+yfZdYHjtgZeBJzQdSMlSZIkaUPGGX62O7C+qs6vqiuBI4B9FzjutcAbgN902D5JkiRJ2qhxhp9tB1w0sn0xsMfoAUkeBOxQVUcleemGTpTkQOBAgB133PGmt1adc0iMJF3PPCVJw7TkiQKS3AJ4M/CSxY6tqsOqak1VrVm1atVSQ0uS1CnzlCQN0zhFzSXADiPb27f75mwN3Af4cpILgIcA65Ks6aqRkiRJkrQh4xQ1JwK7JNk5yebAfsC6uRer6udVtW1Vra6q1cDxwD7OfiZJkiRpEhYtaqrqauAg4BjgHODIqjorycFJ9um7gZIkSZK0MWOtU1NVRwNHz9v3yg0c+6ilN0uSJEmSxrPkiQIkSZIkaZosaiRJkiQNmkWNJEmSpEGzqJEkSZI0aBY1kiRJkgbNokaSJEnSoFnUSJIkSRo0ixpJkiRJg2ZRI0mSJGnQVk67AZIkaelWrz2ql/NecMjevZxXkrpkT40kSZKkQbOokSRJkjRoFjWSJEmSBs2iRpIkSdKgWdRIkiRJGjSLGkmSJEmDZlEjSZIkadAsaiRJkiQNmkWNJEmSpEGzqJEkSZI0aBY1kiRJkgbNokaSJEnSoFnUSJIkSRq0ldNugCRJkqQbW732qF7Oe8Ehe/dy3mmyp0aSJEnSoNlTI0mSpMGyN0NgT40kSZKkgbOokSRJkjRoDj8bUx9dm3ZrSpKkSXCIlmadPTWSJEmSBm2soibJnknOTbI+ydoFXn9xkrOTnJ7kC0l26r6pkiRJknRjixY1SVYAhwJ7AbsC+yfZdd5hpwJrqup+wMeAN3bdUEmSJElayDg9NbsD66vq/Kq6EjgC2Hf0gKr6UlX9qt08Hti+22ZKkiRJ0sLGmShgO+Cike2LgT02cvyzgM8upVHS0PgApiRJ0vR0OvtZkqcCa4BHbuD1A4EDAXbccccuQ0uStGTmKUmLcUbc5Wmc4WeXADuMbG/f7ruBJI8FXgHsU1W/XehEVXVYVa2pqjWrVq26Oe2VJKk35ilJGqZxipoTgV2S7Jxkc2A/YN3oAUkeCLyLpqD5cffNlCRJkqSFLVrUVNXVwEHAMcA5wJFVdVaSg5Ps0x72JmAr4KNJTkuybgOnkyRJkqROjfVMTVUdDRw9b98rR75+bMftkiRJkqSxjLX4piRJkiQtVxY1kiRJkgat0ymdJUmShsx1x6RhsqdGkiRJ0qBZ1EiSJEkaNIsaSZIkSYNmUSNJkiRp0CxqJEmSJA2aRY0kSZKkQXNKZ0kb5fSmkiRpubOnRpIkSdKg2VMjaZNlL5QkSbPBokYT5S+RkiRJ6prDzyRJkiQNmkWNJEmSpEFz+JmkZcPhiZIk6eawp0aSJEnSoFnUSJIkSRo0ixpJkiRJg2ZRI0mSJGnQLGokSZIkDZpFjSRJkqRBs6iRJEmSNGgWNZIkSZIGzcU3JUmSpsAFh6XuWNRIkqSbxF/GJS03FjWaWSZdSZKkTYNFzTLkL+OSJEnS+JwoQJIkSdKgWdRIkiRJGjSLGkmSJEmDNlZRk2TPJOcmWZ9k7QKvb5Hk39vXT0iyuuuGSpIkSdJCFi1qkqwADgX2AnYF9k+y67zDngVcXlV3B94CvKHrhkqSJEnSQsbpqdkdWF9V51fVlcARwL7zjtkX+GD79ceAxyRJd82UJEmSpIWlqjZ+QPIkYM+qena7/TRgj6o6aOSYM9tjLm63z2uP+cm8cx0IHNhu3hM4t6sLWcS2wE8WPWqY8WY11qTjzWqsSceb1ViTjjfJWDtV1aoJxVr2zFPGWsbxZjXWpOPNaqxJx1t2eWqi69RU1WHAYZOMCZDkpKpaM4vxZjXWpOPNaqxJx5vVWJOON+lr0/XMU8ZarvFmNdak481qrEnHW455apzhZ5cAO4xsb9/uW/CYJCuB2wCXddFASZIkSdqYcYqaE4FdkuycZHNgP2DdvGPWAU9vv34S8MVabFybJEmSJHVg0eFnVXV1koOAY4AVwPuq6qwkBwMnVdU64L3A4UnWAz+lKXyWk0kPJZhkvFmNNel4sxpr0vFmNdak4018+JOmzvfz8GJNOt6sxpp0vFmNNel4yy5PLTpRgCRJkiQtZ2MtvilJkiRJy5VFjSRJkqRBs6iRJEmSNGgWNQOWZJskW0+7HX2Y5WuTFpLkDtNug9S1Wf4sn+Vrkxay3PPUzE4UkOTJwH9W1S+S/C3wIOB1VXVKjzFXAHdiZFa5qvp+D3F2A94HbA0E+BnwzKo6uetYbbwHAY8ACvh6z/+Gs3xtvwPs3sY6sap+2FOcFcB/VdWj+zj/BmJO8t/xPOB44KvAV6vqrB5j/SPtjI99xRiJ9V3gNOD9wGedFn/2madO7lbOAAAgAElEQVQ6jWee6iaWeaqbWOapKZjlnpr/2yaKRwCPpZl2+l/6CpbkhcCPgM8DR7V/PtNTuPcCL6iq1VW1E/CXNG+wziV5JfBB4A7AtsD72+Tbl5m8tiTPBr4J/CnNWk7HJ3lmH7Gq6hrg2iS36eP8803hPbIr8K423puSnJfkkz3FOgc4LMkJSZ7X87/pPWimyHwa8N0kf5/kHj3G0/SZpzpgnuoslnmqO+apKZjlnppTq+qBSV4PnFFVH57b11O89cAeVXVZH+efF+tG15HklKp6UA+xzgXuX1W/abdvCZxWVffsOlZ7/pm8tjbWw+beH20X7nE9/jt+CnggzS8vv5zbX1V/1UOsSb9HVgK7AY+kuet2B+D0qnpuH/HamPcEngHsD3wdeHdVfanHeI8GPgTcGvgWsLaqvtFXPE2HeaqzWOap7mKZp7qJZ56agkUX3xywS5K8C3gc8IYkW9Bvz9RFwM97PP9c1ynAse21fYSmG/XPgS/3FPYHwJbAb9rtLYBLug4yy9fWugz4xcj2L9p9fflE+2cSJvnvCHAFcAbwZpoP7V5/QWuHSdyr/fMTmg/vFyd5blV1ttBw+wvEU2nugP0IeCGwDngA8FFg565iadkwT3XDPNUN81R3zFNTMMs9NbcC9qS5+/XdJHcG7ltVn+sp3nuBe9J05/92bn9VvbnDGBuruKuqfr/DWG+n+bDekeZuw+fb7ccB36yqP+0qVhtvJq8tyYvbLx8A3Bf4VBtrX5q7Ngd0FWvSJv0eGYm7L82dr92BK4HjgK9U1Rd6iPUW4I+ALwDvrapvjrx2bpd3+ZJ8BzgceH9VXTzvtZdV1Ru6iqXlwTy15FjmqW5imae6j2uemoKZLGraivWsqrrXBGO+aqH9VfWaSbWhS0mevrHXq+qDk2pL1yZ5bRt6X4zE6uX9keR7NB/c8+PdtcMYU32PJLkXsBfw18Adq+qWPcR4BnBkVf1ygdduU1Wd3fVOkuX20KX6Y55auml/BvXJPNVZDPPUJpSnZrKogevGar6wepjVZZG4t6qqX/UcYwvgicBqbjiDzcF9xp2EWb62ScoNp13cEngycPuqeuWUmtSZJB8H7g+cRzuzDHDC3FjpjmMF+BOunzHna1XVy8OeSVYB/wf4XZr/MwC6vPur5cU8NUyzfG2TZJ7qLJZ5qjXLz9TcDjgryTe54QNo+/QRLMlDaWZE2QrYMcn9gedW1Qt6CPcpmnHRJzMyhKAPSR4OvBrYieb9Eppu9s7upMwzk9eWZnaQ/82Nk2AvHwQLjN99a5KTgc6TxRTeI68HTq1m9py+HQrcnWbsPMBzkzy2qv6yh1j/Bvw78ATgecDTgUt7iKPlwzzVAfNUZ7HMU90xT03BLPfUPHKh/VV1bE/xTqCZAnHd3KwoSc6sqvv0EKuX824g1reB/0Xz4X3dD2dfD73N6rUl+RbwzgVi9bmuwZxbAGuA51fV/XuINen3yGbA84H/0e46FnhnVV3VQ6xvA/ee625PcguaIUP37iHWyVX14CSnV9X92n0nVtVuXcfS8mCe6iyWeaqbWOap7uKZp6ZgZntqqurYJDsBu1TVf7UPZK7oOeZFTS/gdfqq0I9Lct+qOqOn84/6eVV9dgJx5szqtV1dVb2tP7GAfxyNDVwA/FlPsSb9HvkXYDPgn9vtp7X7nt1DrPU0D5he2G7v0O7rw1yy++8ke9PM1nP7nmJpGTBPdcY81Q3zVHfMU1Mws0VNkucAB9L8Y98N2I7mDsRjegp5UZKHAdVW6C+iWRCpD48ADmgfsvst13ej3q+HWF9K8iaaaRdHZ8vpayXeWb22Tyd5AfDJebF+2kMsaoKrNDP598hu8+7kfbG9w9iZJJ+mGZu8NXBOOzyogD1oFqfrw+vSLJr2EuDtwDY0dxY1o8xTnTFPdcM81R3z1BTM8vCz02im0jthpJv9jKq6b0/xtgXeRrMqdIDPAS/qqYt4p4X2V9WF7eu3q6rLO4q10BSW1dcY21m9tjb5LRSrl/G87YfOq7hh1/fB1eEsKCOxJv0eOQV4clWd127fFfhYdbjw3YaGBc3pa3iQNi3mqeF9lrfxZvLazFOdxjNPTcEsFzUnVNUeuX7F5pXAKT3dSVlW0tPKxhuI9fSa4LSZs3ptSR5XVZ/v8HwfB84E5tr/NJrVlHuZk3+RtnT675jkMcD7gfNpfjHbCXhG9bhy8kba8o2qeugSzzG3jsKCqofVtbU8mKdm77O8jTeT12aeuknnM09NwcwOP6NZ8fdvgFsmeRzwAuDTXQdZpv/RWfyQzryI6z+QJmFWr+0NNIuCdeVuVfXEke3XtHeFp6HTf8eq+kKSXWgWEQQ4t6p6nYFoI7Zc/JBFndT+/XBgV5qZZaCZ3vTsDs6v5cs8NRnmqW6Yp8ZknpqOWS5q1gLPAs4AngscDbynhzgnLX7IxE2y+22SH94wu9fWdaxfJ3lEVX0NrpvO8tcdxxhXJ9eWZEN37+6ehKr6RBdxbqIlvx/n7g4meT7wiKq6ut1+J83aBppd5qnJME8tz1jmqcnYZPLUzBY1VXUt8G7g3UluD2xfPYy1G7e7Msnbq+qFXcdfBmZz/GJjktfWdaznAx9sxywDXE4zn/w0dHVtf7RIjGkkiy7djuahy7mHcrdq92lGmacmxjy1PGOZp4ZnWeepmS1qknwZ2IfmGk8GfpzkuKqa1iwND59grCHfuVlO8SZ9bV06B3gjzYxKt6VZKO6PgdOn0JZO/h2r6hldnKdjXb5HDgFObR9oDc3Ds6/p8PxaZsxTMxlr0vHMU90wT41nWeepmS1qgNtU1RVJng38a1W9Ksk0flA6l+RuwMVV9dskjwLuR3ONP2sP6Ws60IV8vcuTzdq1JXlyVX00yc5VtdDMMnMuWGqseT4F/Aw4Bbik43PfVJ28R5K8eGOvV9Wbu4hzEz2tqxNV1fuTfJZmOk6Al1XVD7s6v5Yl89RkmKc2wjwFmKfGstzz1C2m3YAerUxyZ5qFnD4z7cZ07OPANUnuDhxGs9DSh+de7HJO+SQvSrJNGu9NckqSPxiJdVBXsVqzdm0vb//++MYO6mG2l+2rar+qemNV/ePcn45jABN9j2y9yJ/OJPlFkis29GfuuKo6s8OYB1fVD6vqU1X1KZq79v/W1fm1LJmnOmCeWjLzlHlq3JjLOk/Nck/NwcAxwNeq6sQ0c4R/d4rt6bL779qqujrJnwBvr6q3Jzm1w/OPemZVvS3JH9KMm3wacDjN+gZ9mLVruyzJ54Cdk6yb/2JV7dNhrFGTXPF6Iu+RqppYF3dVbQ2Q5LXAf9NcT4CnAHfuKewOSV5eVa9PsgVwJNDXe1/Lg3mqG+appTFPdcQ8NV0zW9RU1UeBj45snw88ccPf0bu3dXiuq5LsT/NA3dxDaZt1eP5Rc0nu8cDhVXVWkj7H8M7ate0NPIjmw6aXO1CjkpxB8zDiSuAZSc6n/xWvJ/oeSbI9zUrGc+P/v0qzgODFPYTbp264KvS/pFkV+pU9xHom8G9JXg48GvhsVb2lhzhaJsxTnTFPLY15qutg5qmpmNmiJsn7WWA2i6p6Zk/xPr1AvJ/TTKX5rqr6QIfhngE8D/i7qvpekp1pPoz6cPLcHRzg5Um2Bq7tKRbM2LVV1ZXA8UkeVlWXdnnuDXjCBGLMN+n3yPtphno8ud1+arvvcT3E+mWSpwBH0Px87w/8sssASUYX6Xsb8C6a8d3HJnlQVZ3SZTwtH+apzpinlsA81Qvz1BSkh9kjl4Uko3e7tgT+BPhB9bTIWJK3AauAj7S7/hy4guYNtk1Vdfag1iQluQXwAOD8qvpZkjsA21VV5w+zJllB87DlU7o+9wbi9X5tG/gl4jo9dutPzCTfI22806rqAYvt6yjWapoP8IfT/D9+HfjrqrqgwxgbW2G6qur3u4ql5cU81Q3z1JJjmKe6j2eemoKZ7ampqhs88JbkI8DXegz5sKrabWT700lOrKrdkpzVZaCRrttRc3fbXldVl3UYrmhWj30CzfjvW9PN6rQ3DlR1TZKdkmze3jnq2+er6rpZaqrqsiRH0u3MNf/Q4bmWpaq6Nsn3gHsk6eW9Mc9lSZ7K9b+Y7Q90+Z6/TpsU9u3j3CMxHt3n+bV8mac6Y55aGvNU98xTUzCzRc0CdgHu2OP5t0qyY1V9HyDJjjSLEgF0/cH3WeAarp9tZT/gVsAPgQ+w8cWfbqp/pumi/X2aZPELmhlSdtvYNy3B+cDX24cVr+s+rQ6nQWw/0G4FbJvkdlw/1nYbYLuu4gBU1bEjcW8J7FhV53YZY9rSTEf7ImB74DTgIcA3aN4zfXgmzVjlt9D8MnMczXCQziW5B/AvwJ2q6j5J7kczfvl1PcS6E/D3wF2qaq8kuwIPrar3dh1Ly5Z56uYxTy2BeaoX5qkpmNmiJskvuOFdoh8CL+sx5EuAryU5j+bDZ2fgBUluDYy1mvNN8NiqGh3feEaSU6rqQe2dgS7t0Z73VICqujzJ5h3HGHVe++cWdDz94YjnAn8N3IVmwbu5ZHEF8I4+Aib5I5q7YZvTzDDzAODgWejWp0kUuwHHV9Wjk9yL5kOvU0neUFUvA3af4L/bu4GX0owfpqpOT/JhoPNkQfOL3vuBV7Tb3wH+HVgWyULdM091xjzVAfPU0pmnpmtmi5q5qe42JMnvVlVn3e1VdXSSXYB7tbvOrarftF+/tas4rRVJdq+qbwIk2Q1Y0b52dcexrmrHEFcbaxU9PlxX7XSISW5VVb/qKcbbgLcleWFVvb2PGAt4NbA78OW2Dae1D5fOgt9U1W+SkGSLqvp2knv2EOfxSdbSrKnw0cUO7sitquqbueEkOV3/jM3ZtqqOTDOrDNVMGXtNT7G0DJinOmOe6sarMU8tlXlqima2qBnD4TRTGHYiyZOB/6yqbyX5W+BVSV7X04wQzwbel2Qrmrs3VwDPbu+2vb7jWP8EfBK4Y5K/A54E/G3HMa6T5KE0Ff9WwI5J7g88t6pe0HWsatYWeBiwmpGfhar6165jAVdV1c/nfejMyiwdFye5LfAfwOeTXA5c2EOc/wQupxlCcwXt9J9zf1fVNj3E/Ema1cPnfll6Es16AH34Zfvw6lysh9A8g6BNl3lqPOapbpinls48NUUzO/vZYpKcWlUP7PB8p1fV/ZI8AngtTRfuK6tqj65iLBDzNgBV1esbqu2mfQzND+UXquqcHmOdQJOQ1s39/yQ5s6ru00Osw4G70YyvnbvTUH3MPJTkvcAXgLU061D8FbBZVT2v61jTlOSRwG1ofnHq5SHaJJ+qql4fihyJdVeaFcMfRpOovgc8pao6T4Zppsx8O3Af4EyaWaqe1NfsPFr+zFM3KY55aumxzFPdxTBPTcGmXNScMm+871LPd2pVPTDJ64EzqurDXSekefH2Bn6XkRlequrgDs+/TVVdkeT2C71eVT/tKta8uCdU1R6j/3ZJvlU3XFiqq1jnALvWBH4IktyKZgzqH9Ak3WOA144M/Ricab1HFpPkG1X10I7PeWvgFlX1iy7Pu0CclcA9ad4j51bVVX3G0/Jmnlr0/OapbmOZpybEPNWPTXn4WdcuSfIumoWV3pBkC5qHCDuX5J00s6I8GngPzR2jb3Yc5sM002OezA27n+e6Uu/acbw5F7Vd7ZVkM5qH+/q643Ym8Dv01017nXbc9SuAV7Rjv2895ETRmv8eGR2z0Od7ZDGdTdeZ5oHq42lWg/4q0Om0t/Ni3Qp4MbBTVT0nyS5J7llVn+krpjY55qlumKeGwzzVoeWepzblnprjq+ohHZ7vVsCeNHe/vpvkzsB9q+pzXcUYiTU3hGDu762Az1bV73UcJ8AO1U7/OQlJtqVZROqxNB8+nwP+qo+7KWkWk3oATaL97dz+6mHGkjQzkTyPZvjAiTTTcr6tqt7UdaxNXZd3t9tf+vYAfo9mYbN7AqdX1Z90cf55sf6dJvH+RTXTct4KOK56WKxNw2CeGiuOeaq7WOapCTFP9WPmemra8X4bNPdAZJeJoj3fr9oPnx1G2vCTLmOM+HX796+S3IVmQac7dx2kqirJUcB9uz73Rtyz5q3UnOThNCvkdu3VPZxzQ3Ztu8CfQrN+w1qaD4bBJotxf9YG7hrgqvbva4Eft3/6cLeq+vMk+8N1nylZ7Js0POap7pinOmWeGibzVGvmihrgH9u/twTWAN+iuZNyP5qVjDsdwzgnyWuBA2jmrp/r/ir6WdjpM2lm8XgTcEob5z09xAE4JcluVXViT+ef7+3ceLafhfYtWVUdm2QnYJeq+q/2jsOKxb7vZtqsHabwx8A7quqqJEPvJp3Kz9oYuvyAvQI4A3gz8O7qdhX0+a5Ms/Dd3Kwyd2PkzqxminmqW+apbpinJsc81YOZK2qq6tEAST4BPKiqzmi370O/dzz+jKaC7WUmjXneWFW/BT6e5DM0P6x9jXvdA3hKkgtpVk6em5bwfl0GSTNF5sOAVUlePPLSNvT0AZ7kOcCBwO1pZpfZDngnzQw6XXsXcAHNB+pX2iR1RQ9xJmaKP2vMS/K3BFaOPBz5tA5D7Q88AngBzXS0xwFfqaovdBhjzqtopgPdIcm/0QwjOKCHOJoy81TnzFPdME91yDw1eTP7TE2Ss6rqdxfb12G8jwPPr6q+uvxGY91oLGaX4zPnnXenhfZXx1MFppli8VE043nfOfLSL4BPV9V3u4zXxjyNZqGxE0ZmsDmjqnofxtB2166oqqvb7adXVdcrek/EFH7WrkvyVXW3NIsJvrOq+kjyczHvBexFs8L3Havqlj3E+BBwOs2wnfNp3pd9DQ3SMmCe6iyWeaoH5qklxTNPTcHM9dSMOD3Je4APtdtPofmP6MvrgVOTnElPD/Ml+R2auzS3TPJAru++3IZmlpnOVdWFaRYWm3u486tV9a0e4hwLHJvkA10noo34bVVdOTccNM00hROp8qu5mzC64u+LgEEmCyb/s/aXtEkeoH3g+Y59BGp/Cbw/zXCdrwJ/MRe3B++l+Tl7HM0d2VOTfKWalcU1m8xTHTBP9cM8tSTmqSmY5aLmGcDzaX4IAb4C/EuP8T4IvIFmXOO1PcX4Q5puvu1pxonOJYsrgL/pI2CSFwHPAT7R7vpQksOq6u19xAO2SHIYN149uY8x38cm+Rua5Ps4mq7bT/cQZxzL5kG7m2HSP2uTTPKvB06tqmsWPXKJqupLSb4C7EYzDe7zaNb4WBbJQr0wT3XAPDUx5qnxmaemYGaHnwG0Yxh3rKpzJxDrxKrare84bawnVtXHN/J6Z13ESU4HHlpVv2y3bw18o+uxyiPxvkXTrX8y16+eTFWd3EOsWwDP4oYLjb2npvBD0dewjEmZ8M/aG4Gf0dyNeiFNkj+7ql7RQ6zNaBLh/2h3HUszhKDzxcaSfAG4NfANmrttX5vEMCFNl3mqk1jmqQkwT92kWOapKZjZoibJPjSzrmxeVTsneQBwcJfd7PPivZmmO38dN+zWn/h0gV1+8CQ5A9it2gW4kmwJnNjXeN4kJ1fVg/s493KWHlf17tsUftYmluTb4Qqbcf2Qi6cB11TVs3uI9RbgwTSfH1+nuZP4jar69Ua/UYNlnjJPDYl56ibFM09NwSwPP3sVzXjGLwNU1WlJdu4x3twP+ui6An1NlbmYLruI3w+ckOST7fYf04yp7Munk7wA+CQ3TLp9LGr2BOC1wE40PwtzM+Zs03WsMfSxvsGkTPRnraquBd7d/unbblV1/5HtL7Z3aTtXVf8LIMnWNMN33k+zkvgWfcTTsmCe6oZ5ajLMU2MyT03HLBc1V1XVz3PDNYF665aqdtrADZnwrCGdXWdVvTnJl2mmCwR4RlWd2tX5F/D09u+XjjYDuGsPsd4K/CnN6tq9dlm2Y77fTzNLzntofrlYW+1K3lV1UJ/xezaRn7X2buwGz9vTUJNrktytqs5r23BXRoabdCnJQTQPYD6YZlrV99F072t2mae6OJF5qhPmqaUzT03XLBc1ZyX5n8CKNFPp/RVw3BTbM8lZQzq7A5bk9jRv3AtG9m3Wx1hNgKrq8y7lfBcBZ05obPIzq+ptSf4QuB1N9/DhwOcmELtvk/pZe0IP51zMS4EvJTmf5udqJ5oHTvuwJc3iaSdXO4WqZp55qosTmae6Yp5aOvPUFM1yUfNC4BU0XcMfphnP+NoptmeSs4Z02UV8CrADcDnNNdwW+GGSHwHP6frByDSrJb+Y5mG+A9sPn3tW1We6jNP6P8DRSY7lhkMI3txDrLn//8cDh1fVWZl3y2jAJvKzVpObQnU05hfm3oPtrnP/f3v3HmRZVV9x/LsGCgFxBBSwAAVBxBrJAArhGalISZQgRhQkChKxDERFiUU0MalCpcgDBYtHWaYUCVgYZVQUSnyAKKCMD8ABhkFKwCBEjDg8BkoElJU/zrn27Z4n9D737Ht7faq6Zs7pmfv7dU93rzln77O3mw0Fu6j1sS5eN6qWnCojOVVGcmqWklP9muSFAg63vWht50bYT8mHIt+3itMP0Vw5LylRY6jWp4Av2v5me3wQ8AaaIeozbe9VuN4XaFaUeavtXdrwuNb2biXrtLW+BTzCjOVNbX+4g1rn0ezd8EKa9eTXA747CQ+bStqDJiy2Z+pGiUsPs0v6nu39JT3M9OH94nPMJR22pvfb/vKa3h+xLpJTZSSnitVKTs2+TnKqR5N8UTOy3YzXsZ9iq4ZI+hywB1Nr1R9Cs4nU9sAi26eVqNPWWmnnYkk32V4oaUnpH+KSrrO9x/DnS9KNMx6CK1Vrqe1dSr/uamrNA3YD7rT9oKTnANvY7nLzr5GQdBtwErCU6aFbejfvHWzfWfI111DrvDW827aPHUUfMdmSU2Ukp4rVSk7Nvk5yqkcTN/1M0mtohk63kXTW0LvmM31n3FErOdS+LfAy248ASDoZ+BrNGuXXA8XCArhX0geAz7fHbwL+T9J6dLN52+Nq1pI3gKQdGRpyL+wySQcNHoLs2OW2Dxwc2F4u6SLgwDX8nXFxn+1RbAa3CHi5pG8Pfy67YLur+cgRyank1FORnCojOTUHTNxFDfBL4DrgUJofnAMPA3/fVdG1DbUXXjVkS6b/AH0C2Mr2o5JK/2B9M81SiF+h+QH+/fbcesARhWvR1voG8HxJFwL70Swb2IW/A05qP2dP0M3w8IbAxsBzJW3G1Jzl+TTD/JPgZDXr5H+b6XO+Sw99z1Ozs/aLV/X9VnKO+Wq+nzupFXNScqqs5NQsJKeKSk71aOIuamzfCNwo6cIRr8ywB6seaj9eUtGhduBCmjX5v9oevxb4nJpdlJcVrIPt3wAnSHqm292ah9xeslZb73JJN9DsoyDgvW0Pxdl+VhevO8NxwInA1jT/eRmExQrgnBHUH4W3AS+h2fxrcFfUQOmwOJJm/4n1ga7/7UbxtRFzVHIqOfUUaiWnykhOzQET90yNpItsH6HVrBVe+qGwobpXAwcPDbVvQjPU/mqau2ALCtfbg+buEMD3bV9X8vWH6uxLs179JrZfIGlX4Djb7+yiXltzIdMf5uvsgbdR1ZJ0gu2zS79uDSTdZnvntf/JYvVeY/vro6oXUVpyqqzkVLE6yaly9ZJTPZi4kRqadfZh9GuFj2yovZ2D/XnbZ5Z83dX4OPAXwCXQ3GGU9Iquikn6DLAQuIVu76aMtJbts9vg3Z7pwXRB6Vo9uFbSAttF776upd4ZNHPzAa4CPmL7odKFJG0LnM3Uf8yuobkre0/pWjGnJKfKSk4VkJwqXi85NWITd1Fj+972t/OAe23/DqB9qG+rDkuPbKidZnj4XyTtDFxMExyd3AEDsH23pi9V38lOta29S98trKGWpM8COwJLmPr8GZiEsNgbWCLp5zT/YRrM+e7kbjPNDsZLmZorfzTN0q1rXN7yaTqPZk+Dw9vjo9pzr+qgVswRyanyklOzl5wqKjnVg4mbfjYg6TpgX9uPt8cb0Ax/79lhzZEMtQ/V25xmLf4jaTYB26mDGl+k2T32HGAvmjuMe9g+snSttt65wOmjuJsy4lq3Ags8gd9wkrZb1fnSS2UO1VtpidYulm0dda2Ye5JTxWokp8rUSk6Vq5ec6sHEjdQMWX8QFAC2H28DoxMjHmofeBHNg2/bAbd2VON44EyaFVD+F/gW0Nk8ZZo7Qosl/Yru76aMstZS4HnAvWv7g+Omq1BYg0cl7W/7ewCS9gMe7ajWcklHAf/dHv81sLyjWjH3JKfKSE6VkZwqJznVg0m+qLlP0qG2LwGQ9Dqgk9VJWiMbapd0GvB64A6adflPsf1gF7WAnW2/ZUb9/Si7n8Gwc2mGaaftnjwBtZ4LLJP0I6YvJ3lox3Un0fHABZKe3R4/ABzTUa1jaeYqf5xmGsa1NKvoRJSQnCojOVVGcqqc5FQPJnn62Y4084e3bk/dAxxt+46O645iqP04mocEdwCeMThv++oOao10x2tJi23v08Vr91zrgFWdt33VKOpPCjU7Xr/R9kWS5gPYXtFBnf+w/QFJh9teVPr1IyA5VbBWcqpMreRUAcmp/kzsSE0bCnu3S1YyWMJyQNIxts/voPQohtqfBK6k2bF5Cc0DcIuBV5YqIGkfYF9gC03f4Gk+zYZmXfmJpM/R7KPQ5QZZI61l+6p2Tu9Otq+QtDHdfh4nku0nJb0fuKiLkBhysKR/BP6JZofoiOKSU7OTnCorOVVGcqo/E3tRMzAzJIa8FygWFiMean8PsCfwA9t/LuklwL8WrrEBsAkrbyC1Anhj4VrDNqL5wX3Q0LlOlq8cZS1J7wD+FticZnWZbYBPAgeWrjUHXCHpJOALwB832rN9f8Ea36CZLrCJpBW089gHv7rgbt4RyamnLTlVUHKqqORUDyZ2+tnaSPqJ7d0Lvt4oh9p/bHtPSUuAvWw/JukW2y/toNZ2PTxgN3Haf6s/BX44+LqTdLPtP+m3s/HTLsm5qg0Ld85R+68AAA2jSURBVOig1ldtv67060asi+TUOtdKThWQnConOdWPiR+pWYPSV3OdD7UPuUfSpsBXgMslPQB09QP9t5I+CrwU2HBw0nYXHxeSXgicwMqbfxV/UFGj3bDqsXZlo0Ht9Sn/NThXLKBZ2Wh/ms/hNTR3E4tbW1CMcr57zEnJqXWTnCojOVVOcqoHc/miRmv/I0/JKIbaAbD9+va3H5L0HeDZNMOQXbiQZvj0EJrVPI4B7uuoFjQBeC7N/OGuV3oZ5YZVV0n6ILCRpFfR/LC7tIM6c8H5NNNLzmqP39yeO2K1f6M7G679j0Q8bcmpdZOcKiM5VU5yqgdzefrZObbfXfD1RjbUPkqSrrf9ckk3DdbFH3ysHdX7oe29unjtVdQa5eZY84C308yLFvBN4NOeq9+AsyBpmWfssL2qcyPqpbMVliKSU+smOVWsVnKqkORUPyZ2pGbGSigDDwHX215SMihaoxxqH6Un2l/vlfSXwC9pHiLsypmSTqbZPG14pZcbOqg1sg2rbD8JfKp9i9m5QdLetn8AIGkvoNNd0SO6kJwqJjlVQHKqqORUDyZ2pKZdAnEPpoZODwFuopkDu8j2aR3WPoB2qN1Du0WPI0mH0MwFfT7NvN75wIdsdzIkLenfaDYau4OpYX13MTe6XbrybGAfpjasOsH23R3UOgQ4hWYJ1fWZw6uTzJakW4GdgV+0p14A3Ab8nu522l5dL0Uf5I65JTlVRnKqWK3kVCHJqX5M8kXN1cDBg6Uy1ewD8DXg1TR3wUY+BDiOJJ1P81Dig+3x5sDHbB/bUb3bgQWjCNn2YzvR9gPtcWcfW/txHQbcnKH82WlDfrVKr4Kk6fs2bASsb/vh9n272F5asl7MHcmpMpJTxWolpwpJTvVjYqefAVsyNCxMMzy9le1HJT22mr8TK1vooX0MbN8vqcsr/qXApsCvO6wxsHAQFND5x3Y3sDRBMXujXLpVK+/bsC1D+zbMlaCIziSnykhOlZGcKiQ51Y9Jvqi5EPihpK+2x68FPifpmcCy/toaO/MkbTbjLlGXXzebAj+V9GOmz1UuvlQmo/3Y3g9cJukqpn9cZ3RUL8p4F+2+DQC2fyZpy35bigmSnCojOVVGcmo8JadaE3tRY/sUSV9nam33420PHtJ6S09tjaPTgcWSFrXHhwOndljv5A5fe6ZRfmynAo/QLK24QUc1orzs2xCdSU4Vk5wqIzk1npJTrUl+puYs4PO2r+27l3EnaQFTm7NdabvTO4iStqLZSwHgR7Y7G+If1ccmaantXbp47eiOpNOAB4G30my2905gme1/7rWxmAjJqXKSU0XqJKfGUHJqyiRf1BwDvIlm9YmLaYIjy+lVTtIRwEeB79KsvPJnwD/Y/mKffc1W+0PnCtvf6ruXWHfZtyG6lJwaT8mpqElyasrEXtQMtPNP3wAcCbzA9k49txRrIOlG4FWDu16StqD5Ibtrv53NjqSHgWfSzFN+giyVGRGt5NR4SU5F1Glin6kZ8iLgJTTrrt/acy+xdvNmDOMvB+b11Uwptp/Vdw+x7iTdzBrmJI9yj4GYE5JT4yU5Fb1LTq1sYi9q2mHU19NsjvV54JThJR+jWt+Q9E2mdk9+E/D1HvspRtJCmk31/vh9Z/vLvTUUa3JI3w3E5EtOja3kVNQgOTXDxE4/k3Qc8GVgB+AZg/O2r+6tqVgnkg4D9m8Pr7F9cZ/9lCDpM8BC4Bam70DdyeZwEVG/5NT4Sk5F1GdiR2poviGvpNmEaAmwN7CYqRVEokKSXghcNrgzJGkjSdvb/p9+O5u1vbM7+PiQ9D3b+7dzzIfv/GSOeZSUnBpDyamoQXJqZWM/B3QN3kOz3OJdtv8c2J1mybuo2yKm7hAB/KE9N+4Wt8tyxnh4KzRzzG3PH3p71lwMiuhMcmo8JaeiBsmpGSb5ouZ3tn8HIOkZtn9Ks2xm1G19248PDtrfT8ImYBfQBMZtkm6SdLOkm/puKlZrEYCkb/fdSEy05NR4Sk5FDZJTM0zy9LN7JG0KfAW4XNIDwF099xRrd5+kQ21fAiDpdcBveu6phHOBo4GbmX6HL+o0T9IHgRdLet/Md9o+o4eeYvIkp8ZTcipqkJyaYWIXChgm6QDg2cA3hu+uRH0k7QhcCGzdnroHONr2Hf11NXuSFtvep+8+Yt1I2hn4K+BE4JMz32/7wyNvKiZacmp8JKeiBsmplc2Ji5oYP5I2AbD9yIzzx9g+v5+unj5JnwA2BS6l2dgMyFKZtZP0GtsTsVRrRJSVnIoaJKem5KImxoqkG2y/rO8+nipJ563idJbKrJykZwMnA69oT10FfMT2Q/11FRE1S07FKCWnpuSiJsaKpJ/Y3r3vPmJukPQlYCkwuOt6NLCr7cP66yoiapacilFKTk2Z5NXPYjKN5VW4pG0lXSzp1+3blyRt23dfsVY72j7Z9p3t24dpNkqMiFid5FSMUnKqlYuaGDfqu4Gn6TzgEpoHS7emmbO8qqH+qMujkga7hiNpP+DRHvuJiPolp2KUklOtSV7SOSbT9/tu4GnawvZwOPyXpBN76ybW1fHABe2cZYAHgGN67Cci6pecilFKTrUyUhNVkfReSfPVOFfSDZIOGrzf9rv77G8Wlks6StJ67dtRwPK+m4rVkzQP2Nn2rsBCYKHt3W1nM7qIOSw5FbVITk2Xi5qozbG2VwAHAZvRPPD27/22VMSxwBHAr4B7gTcCf9NnQ7Fmtp8E3t/+fkX7dRkRkZyKKiSnpstFTdRmMBf5YOCztm9hfOcnD/sIcIztLWxvSRMec25jrDF0haSTJD1f0uaDt76bioheJaeiJsmpVpZ0jqq06+RvA7wQ2BVYD/iu7Zf32tgsrWqJzyz7WT9JP2cVKxnZnpMry0REcirqkpyakoUCojZvB3YD7rT9W0nPAd7Wc08lzJO0me0HANq7KPn+q98C4J3A/jShcQ3wyV47ioi+JaeiJsmpVr5YozaX2z5wcGB7uaSLgAPX8HfGwenAYkmL2uPDgVN77CfWzfnACuCs9vjN7bkjeusoIvqWnIqaJKdauaiJKkjaENgYeK6kzZianzyfZph/rNm+QNJ1wCvbU4fZXtZnT7FOdrG9YOj4O5Ly7xYxByWnolLJqVYuaqIWxwEn0mz4dT1TYbECOKevpkpqw2FO/qAZYzdI2tv2DwAk7QVc13NPEdGP5FTUKDnVykIBURVJJ9g+u+8+IgAk3QrsDPyiPfUC4Dbg94BtL+yrt4joR3IqapKcmpKLmqiOpH2B7RkaSbR9QW8NxZwlabs1vd/2XaPqJSLqkZyKWiSnpuSiJqoi6bPAjsAS4A/tadt+T39dRURENJJTEXXKRU1UpR1GXeB8YUZERIWSUxF1mtd3AxEzLAWe13cTERERq5GciqhQVj+L2jwXWCbpR8Bjg5O2D+2vpYiIiD9KTkVUKBc1UZsP9d1ARETEGnyo7wYiYmV5piaq067ksZPtKyRtDKxn++G++4qIiIDkVESN8kxNVEXSO4AvAv/ZntoG+Ep/HUVERExJTkXUKRc1UZt3AfvR7NCM7Z8BW/baUURExJTkVESFclETtXnM9uODA0nrA5kjGRERtUhORVQoFzVRm6skfRDYSNKrgEXApT33FBERMZCciqhQFgqIqkiaB7wdOAgQ8E3g09nkLCIiapCciqhTLmoiIiIiImKsZfpZVEXSIZJ+Iul+SSskPSxpRd99RUREQHIqolYZqYmqSLodOAy4OUP5ERFRm+RURJ0yUhO1uRtYmqCIiIhKJaciKpSRmqiKpD2BU4CrgMcG522f0VtTERERreRURJ3W77uBiBlOBR4BNgQ26LmXiIiImZJTERXKRU3UZmvbu/TdRERExGokpyIqlGdqojaXSTqo7yYiIiJWIzkVUaE8UxNVkfQw8EyaecpP0GxsZtvze20sIiKC5FRErXJRExERERERYy3P1ER1JC0Etmfo69P2l3trKCIiYkhyKqI+uaiJqkj6DLAQuAV4sj1tIGERERG9S05F1CnTz6IqkpbZXtB3HxEREauSnIqoU1Y/i9oslpSwiIiIWiWnIiqUkZqoiqQDgEuAX9GsLDNYVWZhr41FRESQnIqoVS5qoiqSbgfeB9zM1FxlbN/VW1MRERGt5FREnbJQQNTmPtuX9N1ERETEaiSnIiqUkZqoiqRPAJsCl9IM6wNZKjMiIuqQnIqoU0ZqojYb0YTEQUPnslRmRETUIjkVUaGM1ERERERExFjLks5RFUnbSrpY0q/bty9J2rbvviIiIiA5FVGrXNREbc6jWSpz6/bt0vZcREREDZJTERXK9LOoiqQltndb27mIiIg+JKci6pSRmqjNcklHSVqvfTsKWN53UxEREa3kVESFMlITVZG0HXA2sA/NajLXAifYvrvXxiIiIkhORdQqFzVRFUnnAyfafqA93hz4mO1j++0sIiIiORVRq0w/i9osHAQFgO37gd177CciImJYciqiQrmoidrMk7TZ4KC9A5ZNYiMiohbJqYgK5ZswanM6sFjSovb4cODUHvuJiIgYlpyKqFCeqYnqSFoAvLI9vNL2sj77iYiIGJaciqhPLmoiIiIiImKs5ZmaiIiIiIgYa7moiYiIiIiIsZaLmoiIiIiIGGu5qImIiIiIiLGWi5qIiIiIiBhr/w9et8RqH3orhgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1008x504 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_values = profiles.drop([\"profile_username\"], axis=1)\n",
    "fig, axes = plt.subplots(2, 2, sharey=True, figsize=(14, 7))\n",
    "for i, category in enumerate([\"Politics\", \"Sport\", \"Music\", \"Show\"]):    \n",
    "    df_category = df_values[df_values.category_1 == category_to_numerical[category]]\n",
    "    df_category = pd.DataFrame(preprocessing.scale(df_category.values), columns=df_category.columns)\\\n",
    "                        .drop([\"category_1\", \"is_tracked\"], axis=1)\n",
    "\n",
    "    x = df_category.columns\n",
    "    y = []\n",
    "    for col in x:\n",
    "        y.append(np.mean(df_category[col].values))\n",
    "    \n",
    "    axes[i//2, i%2].bar(x, y)\n",
    "    axes[i//2, i%2].set_title(category)\n",
    "    \n",
    "    if i//2 != 1:\n",
    "        axes[i//2, i%2].set_xticks([])\n",
    "    for tick in axes[i//2, i%2].get_xticklabels():\n",
    "        tick.set_rotation(90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "legends = [\"Unknown Users\", \"Politics\", \"Sport\", \"Music\", \"Show\"]\n",
    "values = profiles[[\"category_1\", \"profile_username\"]].groupby(\"category_1\").count().values\n",
    "\n",
    "plt.pie(values, labels=legends, autopct=\"%.2f\", startangle=45, explode=[0, 0.1, 0.15, 0.2, 0.25])\n",
    "plt.title(\"Percentage per Category\", fontsize=16)\n",
    "rcParams[\"font.size\"] = 14\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_trace = json.load(open(\"../data/results/histories/models_histories_100e_2l_64u.json\"))[\"GCNModel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(gcn_trace)), gcn_trace)\n",
    "plt.title(\"Best Model Training Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = profiles.category_1.values\n",
    "x = profiles.drop([\"category_1\", \"profile_username\"], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.696721002954871 0.036289000810455926\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "f1s = []\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_idx, test_idx in skf.split(x, y):\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(x[train_idx], y[train_idx])\n",
    "    y_pred = xgb.predict(x[test_idx])\n",
    "    \n",
    "    f1s.append(f1_score(y[test_idx], y_pred, average=\"macro\"))\n",
    "\n",
    "print(np.mean(f1s), np.std(f1s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usar treino no teste para achar o erro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import CrossEntropyLoss, NLLLoss\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, lr=0.0001):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.m = nn.Sequential(*[nn.Linear(in_dim, 100),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(100, 300),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(300, 100),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(100, 100),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(100, out_dim),\n",
    "                                nn.Softmax(dim=1)])\n",
    "        \n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.optimizer = Adam(self.parameters(), lr=lr)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.m(x)\n",
    "    \n",
    "    def fit(self, x, y_true, n_epochs=10):\n",
    "        for epoch in range(n_epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            y_preds = self.forward(x)\n",
    "            \n",
    "            print(y_preds[:10], y_true[:10])\n",
    "            loss = self.criterion(y_preds, y_true)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            print(\"{}/{} Running Loss: {}\".format(epoch+1, n_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../data/64e.emb\").read().split('\\n')[1:-1]\n",
    "data = [(int(row.split()[0]), row.split()[1:]) for row in data]\n",
    "\n",
    "indices = [index for index, _ in data]\n",
    "\n",
    "# y = torch.Tensor(profiles.iloc[indices].category_1.values).long()\n",
    "# x = preprocessing.scale(profiles.iloc[indices].drop([\"profile_username\", \"category_1\"], axis=1).values)\n",
    "y = torch.Tensor(profiles[profiles.is_tracked == 1].category_1.values).long()\n",
    "x = preprocessing.scale(profiles[profiles.is_tracked == 1].drop([\"profile_username\"], axis=1).values)\n",
    "x = torch.Tensor(x)\n",
    "\n",
    "# data = torch.Tensor([list(map(float, embeds)) for index, embeds in sorted(data)])\n",
    "\n",
    "# x = torch.cat((data, x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1848, 0.1974, 0.1895, 0.2011, 0.2272],\n",
      "        [0.1854, 0.1971, 0.1896, 0.2001, 0.2279],\n",
      "        [0.1863, 0.1973, 0.1900, 0.1992, 0.2272],\n",
      "        [0.1856, 0.1975, 0.1896, 0.2001, 0.2271],\n",
      "        [0.1859, 0.1976, 0.1899, 0.1992, 0.2274],\n",
      "        [0.1850, 0.1971, 0.1894, 0.2004, 0.2280],\n",
      "        [0.1854, 0.1974, 0.1898, 0.1999, 0.2275],\n",
      "        [0.1855, 0.1974, 0.1898, 0.1999, 0.2274],\n",
      "        [0.1854, 0.1973, 0.1900, 0.1999, 0.2274],\n",
      "        [0.1864, 0.1974, 0.1901, 0.1990, 0.2272]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "1/100 Running Loss: 1.6099988222122192\n",
      "tensor([[0.1845, 0.1976, 0.1896, 0.2013, 0.2271],\n",
      "        [0.1850, 0.1974, 0.1896, 0.2003, 0.2277],\n",
      "        [0.1859, 0.1976, 0.1901, 0.1994, 0.2271],\n",
      "        [0.1852, 0.1978, 0.1897, 0.2003, 0.2270],\n",
      "        [0.1855, 0.1979, 0.1900, 0.1994, 0.2272],\n",
      "        [0.1846, 0.1974, 0.1895, 0.2007, 0.2278],\n",
      "        [0.1851, 0.1978, 0.1898, 0.2000, 0.2273],\n",
      "        [0.1851, 0.1977, 0.1899, 0.2001, 0.2272],\n",
      "        [0.1850, 0.1975, 0.1900, 0.2002, 0.2273],\n",
      "        [0.1860, 0.1978, 0.1901, 0.1991, 0.2270]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "2/100 Running Loss: 1.609816312789917\n",
      "tensor([[0.1841, 0.1978, 0.1897, 0.2015, 0.2270],\n",
      "        [0.1845, 0.1977, 0.1897, 0.2006, 0.2275],\n",
      "        [0.1855, 0.1979, 0.1902, 0.1996, 0.2269],\n",
      "        [0.1848, 0.1980, 0.1898, 0.2005, 0.2268],\n",
      "        [0.1851, 0.1983, 0.1901, 0.1995, 0.2270],\n",
      "        [0.1841, 0.1976, 0.1896, 0.2010, 0.2277],\n",
      "        [0.1848, 0.1980, 0.1899, 0.2002, 0.2271],\n",
      "        [0.1848, 0.1979, 0.1900, 0.2002, 0.2270],\n",
      "        [0.1847, 0.1977, 0.1901, 0.2004, 0.2272],\n",
      "        [0.1856, 0.1981, 0.1902, 0.1993, 0.2267]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "3/100 Running Loss: 1.609635353088379\n",
      "tensor([[0.1837, 0.1980, 0.1898, 0.2016, 0.2269],\n",
      "        [0.1841, 0.1979, 0.1898, 0.2009, 0.2274],\n",
      "        [0.1851, 0.1982, 0.1902, 0.1998, 0.2267],\n",
      "        [0.1845, 0.1982, 0.1899, 0.2007, 0.2267],\n",
      "        [0.1847, 0.1985, 0.1902, 0.1997, 0.2268],\n",
      "        [0.1837, 0.1978, 0.1896, 0.2013, 0.2276],\n",
      "        [0.1844, 0.1982, 0.1900, 0.2003, 0.2270],\n",
      "        [0.1845, 0.1982, 0.1900, 0.2004, 0.2269],\n",
      "        [0.1843, 0.1979, 0.1901, 0.2006, 0.2270],\n",
      "        [0.1853, 0.1984, 0.1903, 0.1995, 0.2265]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "4/100 Running Loss: 1.609458088874817\n",
      "tensor([[0.1834, 0.1982, 0.1899, 0.2018, 0.2268],\n",
      "        [0.1837, 0.1981, 0.1898, 0.2011, 0.2273],\n",
      "        [0.1847, 0.1985, 0.1903, 0.2000, 0.2265],\n",
      "        [0.1841, 0.1984, 0.1899, 0.2010, 0.2266],\n",
      "        [0.1844, 0.1988, 0.1902, 0.1999, 0.2267],\n",
      "        [0.1833, 0.1980, 0.1897, 0.2016, 0.2275],\n",
      "        [0.1841, 0.1985, 0.1901, 0.2005, 0.2268],\n",
      "        [0.1841, 0.1984, 0.1901, 0.2006, 0.2267],\n",
      "        [0.1839, 0.1982, 0.1902, 0.2008, 0.2269],\n",
      "        [0.1849, 0.1988, 0.1903, 0.1996, 0.2263]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "5/100 Running Loss: 1.6092827320098877\n",
      "tensor([[0.1830, 0.1984, 0.1900, 0.2020, 0.2267],\n",
      "        [0.1832, 0.1983, 0.1899, 0.2014, 0.2272],\n",
      "        [0.1843, 0.1987, 0.1903, 0.2002, 0.2264],\n",
      "        [0.1837, 0.1986, 0.1900, 0.2012, 0.2265],\n",
      "        [0.1840, 0.1990, 0.1903, 0.2001, 0.2266],\n",
      "        [0.1828, 0.1982, 0.1898, 0.2018, 0.2274],\n",
      "        [0.1838, 0.1987, 0.1901, 0.2007, 0.2267],\n",
      "        [0.1838, 0.1987, 0.1902, 0.2008, 0.2266],\n",
      "        [0.1836, 0.1984, 0.1902, 0.2011, 0.2268],\n",
      "        [0.1846, 0.1991, 0.1904, 0.1998, 0.2262]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "6/100 Running Loss: 1.6091101169586182\n",
      "tensor([[0.1826, 0.1986, 0.1901, 0.2022, 0.2266],\n",
      "        [0.1828, 0.1985, 0.1899, 0.2017, 0.2271],\n",
      "        [0.1840, 0.1990, 0.1904, 0.2004, 0.2263],\n",
      "        [0.1832, 0.1988, 0.1901, 0.2015, 0.2264],\n",
      "        [0.1837, 0.1992, 0.1904, 0.2002, 0.2265],\n",
      "        [0.1824, 0.1984, 0.1898, 0.2021, 0.2273],\n",
      "        [0.1834, 0.1989, 0.1902, 0.2008, 0.2266],\n",
      "        [0.1834, 0.1989, 0.1903, 0.2009, 0.2264],\n",
      "        [0.1832, 0.1986, 0.1903, 0.2013, 0.2266],\n",
      "        [0.1842, 0.1994, 0.1904, 0.2000, 0.2260]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "7/100 Running Loss: 1.6089366674423218\n",
      "tensor([[0.1822, 0.1988, 0.1902, 0.2024, 0.2265],\n",
      "        [0.1824, 0.1987, 0.1900, 0.2019, 0.2270],\n",
      "        [0.1836, 0.1992, 0.1904, 0.2006, 0.2262],\n",
      "        [0.1828, 0.1990, 0.1901, 0.2017, 0.2263],\n",
      "        [0.1833, 0.1995, 0.1905, 0.2004, 0.2264],\n",
      "        [0.1820, 0.1987, 0.1899, 0.2023, 0.2272],\n",
      "        [0.1831, 0.1992, 0.1903, 0.2010, 0.2264],\n",
      "        [0.1831, 0.1992, 0.1904, 0.2011, 0.2263],\n",
      "        [0.1828, 0.1988, 0.1904, 0.2015, 0.2265],\n",
      "        [0.1839, 0.1997, 0.1904, 0.2001, 0.2259]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "8/100 Running Loss: 1.6087654829025269\n",
      "tensor([[0.1818, 0.1990, 0.1903, 0.2026, 0.2264],\n",
      "        [0.1819, 0.1990, 0.1900, 0.2021, 0.2269],\n",
      "        [0.1832, 0.1995, 0.1905, 0.2007, 0.2261],\n",
      "        [0.1824, 0.1993, 0.1902, 0.2019, 0.2262],\n",
      "        [0.1829, 0.1997, 0.1905, 0.2006, 0.2262],\n",
      "        [0.1815, 0.1989, 0.1899, 0.2025, 0.2271],\n",
      "        [0.1827, 0.1994, 0.1904, 0.2012, 0.2263],\n",
      "        [0.1827, 0.1994, 0.1904, 0.2013, 0.2261],\n",
      "        [0.1824, 0.1990, 0.1904, 0.2017, 0.2264],\n",
      "        [0.1835, 0.2000, 0.1905, 0.2003, 0.2258]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "9/100 Running Loss: 1.6085929870605469\n",
      "tensor([[0.1814, 0.1992, 0.1904, 0.2027, 0.2263],\n",
      "        [0.1815, 0.1992, 0.1901, 0.2024, 0.2268],\n",
      "        [0.1828, 0.1998, 0.1905, 0.2009, 0.2260],\n",
      "        [0.1820, 0.1995, 0.1903, 0.2021, 0.2261],\n",
      "        [0.1825, 0.1999, 0.1906, 0.2008, 0.2261],\n",
      "        [0.1811, 0.1992, 0.1900, 0.2027, 0.2271],\n",
      "        [0.1824, 0.1996, 0.1905, 0.2014, 0.2261],\n",
      "        [0.1823, 0.1997, 0.1905, 0.2015, 0.2260],\n",
      "        [0.1820, 0.1992, 0.1905, 0.2019, 0.2263],\n",
      "        [0.1831, 0.2002, 0.1905, 0.2005, 0.2256]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "10/100 Running Loss: 1.608420729637146\n",
      "tensor([[0.1810, 0.1994, 0.1905, 0.2029, 0.2262],\n",
      "        [0.1810, 0.1995, 0.1901, 0.2026, 0.2268],\n",
      "        [0.1824, 0.2001, 0.1905, 0.2011, 0.2259],\n",
      "        [0.1816, 0.1997, 0.1904, 0.2024, 0.2260],\n",
      "        [0.1821, 0.2002, 0.1907, 0.2010, 0.2260],\n",
      "        [0.1806, 0.1994, 0.1900, 0.2029, 0.2270],\n",
      "        [0.1820, 0.1998, 0.1906, 0.2015, 0.2260],\n",
      "        [0.1820, 0.1999, 0.1906, 0.2017, 0.2258],\n",
      "        [0.1816, 0.1995, 0.1906, 0.2021, 0.2262],\n",
      "        [0.1828, 0.2005, 0.1906, 0.2006, 0.2255]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "11/100 Running Loss: 1.608247995376587\n",
      "tensor([[0.1806, 0.1997, 0.1905, 0.2031, 0.2261],\n",
      "        [0.1805, 0.1998, 0.1901, 0.2028, 0.2267],\n",
      "        [0.1820, 0.2004, 0.1906, 0.2013, 0.2258],\n",
      "        [0.1811, 0.2000, 0.1904, 0.2026, 0.2259],\n",
      "        [0.1817, 0.2004, 0.1908, 0.2012, 0.2258],\n",
      "        [0.1802, 0.1997, 0.1900, 0.2032, 0.2269],\n",
      "        [0.1816, 0.2001, 0.1907, 0.2017, 0.2259],\n",
      "        [0.1816, 0.2002, 0.1907, 0.2019, 0.2257],\n",
      "        [0.1813, 0.1997, 0.1907, 0.2023, 0.2261],\n",
      "        [0.1824, 0.2009, 0.1907, 0.2008, 0.2253]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "12/100 Running Loss: 1.6080741882324219\n",
      "tensor([[0.1801, 0.1999, 0.1906, 0.2033, 0.2260],\n",
      "        [0.1801, 0.2000, 0.1902, 0.2030, 0.2267],\n",
      "        [0.1816, 0.2007, 0.1906, 0.2014, 0.2257],\n",
      "        [0.1807, 0.2002, 0.1905, 0.2028, 0.2258],\n",
      "        [0.1814, 0.2007, 0.1909, 0.2014, 0.2257],\n",
      "        [0.1797, 0.2000, 0.1901, 0.2034, 0.2269],\n",
      "        [0.1813, 0.2004, 0.1907, 0.2019, 0.2258],\n",
      "        [0.1812, 0.2005, 0.1907, 0.2020, 0.2256],\n",
      "        [0.1809, 0.1999, 0.1907, 0.2025, 0.2260],\n",
      "        [0.1820, 0.2012, 0.1907, 0.2009, 0.2252]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "13/100 Running Loss: 1.6078972816467285\n",
      "tensor([[0.1797, 0.2002, 0.1907, 0.2035, 0.2260],\n",
      "        [0.1796, 0.2003, 0.1902, 0.2033, 0.2266],\n",
      "        [0.1812, 0.2010, 0.1907, 0.2016, 0.2256],\n",
      "        [0.1802, 0.2005, 0.1905, 0.2030, 0.2258],\n",
      "        [0.1810, 0.2010, 0.1910, 0.2015, 0.2255],\n",
      "        [0.1792, 0.2002, 0.1901, 0.2036, 0.2268],\n",
      "        [0.1809, 0.2007, 0.1908, 0.2020, 0.2257],\n",
      "        [0.1808, 0.2008, 0.1908, 0.2022, 0.2255],\n",
      "        [0.1805, 0.2002, 0.1908, 0.2027, 0.2259],\n",
      "        [0.1816, 0.2015, 0.1907, 0.2011, 0.2250]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "14/100 Running Loss: 1.6077194213867188\n",
      "tensor([[0.1792, 0.2004, 0.1907, 0.2037, 0.2259],\n",
      "        [0.1791, 0.2006, 0.1903, 0.2035, 0.2266],\n",
      "        [0.1808, 0.2013, 0.1907, 0.2018, 0.2254],\n",
      "        [0.1797, 0.2008, 0.1906, 0.2032, 0.2257],\n",
      "        [0.1806, 0.2012, 0.1911, 0.2017, 0.2254],\n",
      "        [0.1787, 0.2005, 0.1902, 0.2038, 0.2267],\n",
      "        [0.1805, 0.2009, 0.1908, 0.2022, 0.2255],\n",
      "        [0.1804, 0.2011, 0.1909, 0.2024, 0.2254],\n",
      "        [0.1801, 0.2004, 0.1908, 0.2029, 0.2258],\n",
      "        [0.1812, 0.2019, 0.1907, 0.2013, 0.2249]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "15/100 Running Loss: 1.6075382232666016\n",
      "tensor([[0.1787, 0.2007, 0.1908, 0.2039, 0.2258],\n",
      "        [0.1787, 0.2008, 0.1903, 0.2037, 0.2265],\n",
      "        [0.1803, 0.2016, 0.1907, 0.2020, 0.2253],\n",
      "        [0.1793, 0.2011, 0.1906, 0.2034, 0.2256],\n",
      "        [0.1802, 0.2015, 0.1911, 0.2018, 0.2253],\n",
      "        [0.1783, 0.2008, 0.1902, 0.2041, 0.2267],\n",
      "        [0.1801, 0.2012, 0.1909, 0.2024, 0.2254],\n",
      "        [0.1800, 0.2014, 0.1909, 0.2025, 0.2253],\n",
      "        [0.1797, 0.2007, 0.1909, 0.2031, 0.2257],\n",
      "        [0.1808, 0.2023, 0.1908, 0.2014, 0.2248]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "16/100 Running Loss: 1.6073555946350098\n",
      "tensor([[0.1783, 0.2010, 0.1909, 0.2041, 0.2258],\n",
      "        [0.1782, 0.2011, 0.1904, 0.2039, 0.2264],\n",
      "        [0.1799, 0.2020, 0.1908, 0.2021, 0.2252],\n",
      "        [0.1788, 0.2014, 0.1907, 0.2036, 0.2255],\n",
      "        [0.1798, 0.2018, 0.1912, 0.2020, 0.2252],\n",
      "        [0.1778, 0.2010, 0.1903, 0.2043, 0.2266],\n",
      "        [0.1797, 0.2015, 0.1910, 0.2026, 0.2253],\n",
      "        [0.1795, 0.2017, 0.1910, 0.2027, 0.2251],\n",
      "        [0.1792, 0.2010, 0.1909, 0.2032, 0.2256],\n",
      "        [0.1804, 0.2026, 0.1908, 0.2015, 0.2246]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "17/100 Running Loss: 1.607168436050415\n",
      "tensor([[0.1778, 0.2012, 0.1909, 0.2043, 0.2257],\n",
      "        [0.1777, 0.2014, 0.1904, 0.2042, 0.2263],\n",
      "        [0.1795, 0.2023, 0.1908, 0.2023, 0.2251],\n",
      "        [0.1783, 0.2017, 0.1907, 0.2038, 0.2254],\n",
      "        [0.1794, 0.2022, 0.1913, 0.2021, 0.2250],\n",
      "        [0.1773, 0.2013, 0.1903, 0.2045, 0.2266],\n",
      "        [0.1792, 0.2018, 0.1910, 0.2027, 0.2252],\n",
      "        [0.1791, 0.2020, 0.1911, 0.2028, 0.2250],\n",
      "        [0.1788, 0.2012, 0.1910, 0.2034, 0.2255],\n",
      "        [0.1800, 0.2030, 0.1909, 0.2017, 0.2244]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "18/100 Running Loss: 1.6069766283035278\n",
      "tensor([[0.1773, 0.2015, 0.1910, 0.2045, 0.2256],\n",
      "        [0.1772, 0.2017, 0.1905, 0.2044, 0.2263],\n",
      "        [0.1790, 0.2027, 0.1909, 0.2025, 0.2249],\n",
      "        [0.1778, 0.2020, 0.1908, 0.2040, 0.2253],\n",
      "        [0.1790, 0.2025, 0.1913, 0.2023, 0.2249],\n",
      "        [0.1768, 0.2016, 0.1904, 0.2048, 0.2265],\n",
      "        [0.1788, 0.2022, 0.1911, 0.2029, 0.2250],\n",
      "        [0.1787, 0.2023, 0.1911, 0.2030, 0.2249],\n",
      "        [0.1784, 0.2015, 0.1910, 0.2036, 0.2254],\n",
      "        [0.1796, 0.2034, 0.1909, 0.2018, 0.2243]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "19/100 Running Loss: 1.6067790985107422\n",
      "tensor([[0.1769, 0.2018, 0.1911, 0.2047, 0.2256],\n",
      "        [0.1767, 0.2020, 0.1905, 0.2046, 0.2262],\n",
      "        [0.1786, 0.2031, 0.1909, 0.2027, 0.2248],\n",
      "        [0.1773, 0.2023, 0.1909, 0.2042, 0.2253],\n",
      "        [0.1785, 0.2029, 0.1914, 0.2025, 0.2248],\n",
      "        [0.1763, 0.2019, 0.1904, 0.2050, 0.2264],\n",
      "        [0.1784, 0.2025, 0.1911, 0.2031, 0.2249],\n",
      "        [0.1783, 0.2026, 0.1912, 0.2032, 0.2248],\n",
      "        [0.1779, 0.2018, 0.1911, 0.2038, 0.2253],\n",
      "        [0.1791, 0.2039, 0.1909, 0.2020, 0.2241]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "20/100 Running Loss: 1.6065757274627686\n",
      "tensor([[0.1764, 0.2021, 0.1912, 0.2049, 0.2255],\n",
      "        [0.1761, 0.2023, 0.1906, 0.2048, 0.2261],\n",
      "        [0.1781, 0.2034, 0.1910, 0.2029, 0.2247],\n",
      "        [0.1768, 0.2026, 0.1909, 0.2044, 0.2252],\n",
      "        [0.1781, 0.2032, 0.1914, 0.2026, 0.2246],\n",
      "        [0.1758, 0.2022, 0.1905, 0.2052, 0.2264],\n",
      "        [0.1780, 0.2028, 0.1912, 0.2032, 0.2248],\n",
      "        [0.1778, 0.2029, 0.1913, 0.2033, 0.2246],\n",
      "        [0.1775, 0.2021, 0.1911, 0.2040, 0.2252],\n",
      "        [0.1787, 0.2043, 0.1910, 0.2021, 0.2239]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "21/100 Running Loss: 1.606368064880371\n",
      "tensor([[0.1759, 0.2024, 0.1912, 0.2051, 0.2254],\n",
      "        [0.1756, 0.2026, 0.1907, 0.2050, 0.2261],\n",
      "        [0.1776, 0.2038, 0.1910, 0.2030, 0.2245],\n",
      "        [0.1763, 0.2029, 0.1910, 0.2046, 0.2251],\n",
      "        [0.1776, 0.2036, 0.1915, 0.2028, 0.2245],\n",
      "        [0.1752, 0.2025, 0.1905, 0.2054, 0.2263],\n",
      "        [0.1775, 0.2032, 0.1913, 0.2034, 0.2246],\n",
      "        [0.1774, 0.2033, 0.1914, 0.2035, 0.2245],\n",
      "        [0.1770, 0.2024, 0.1912, 0.2042, 0.2251],\n",
      "        [0.1782, 0.2047, 0.1910, 0.2023, 0.2237]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "22/100 Running Loss: 1.606153964996338\n",
      "tensor([[0.1754, 0.2027, 0.1913, 0.2053, 0.2253],\n",
      "        [0.1751, 0.2030, 0.1907, 0.2052, 0.2260],\n",
      "        [0.1772, 0.2042, 0.1910, 0.2032, 0.2244],\n",
      "        [0.1758, 0.2033, 0.1911, 0.2048, 0.2250],\n",
      "        [0.1772, 0.2040, 0.1915, 0.2030, 0.2243],\n",
      "        [0.1747, 0.2028, 0.1906, 0.2057, 0.2262],\n",
      "        [0.1771, 0.2035, 0.1913, 0.2035, 0.2245],\n",
      "        [0.1769, 0.2037, 0.1914, 0.2036, 0.2244],\n",
      "        [0.1766, 0.2028, 0.1912, 0.2044, 0.2250],\n",
      "        [0.1778, 0.2052, 0.1911, 0.2024, 0.2235]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "23/100 Running Loss: 1.6059348583221436\n",
      "tensor([[0.1748, 0.2030, 0.1914, 0.2055, 0.2252],\n",
      "        [0.1746, 0.2033, 0.1908, 0.2055, 0.2259],\n",
      "        [0.1767, 0.2047, 0.1911, 0.2034, 0.2242],\n",
      "        [0.1753, 0.2036, 0.1911, 0.2050, 0.2249],\n",
      "        [0.1767, 0.2044, 0.1916, 0.2031, 0.2242],\n",
      "        [0.1741, 0.2032, 0.1906, 0.2059, 0.2262],\n",
      "        [0.1766, 0.2039, 0.1914, 0.2037, 0.2244],\n",
      "        [0.1764, 0.2041, 0.1915, 0.2038, 0.2242],\n",
      "        [0.1761, 0.2031, 0.1913, 0.2046, 0.2248],\n",
      "        [0.1773, 0.2056, 0.1911, 0.2026, 0.2233]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "24/100 Running Loss: 1.6057095527648926\n",
      "tensor([[0.1743, 0.2033, 0.1915, 0.2058, 0.2251],\n",
      "        [0.1740, 0.2037, 0.1908, 0.2057, 0.2258],\n",
      "        [0.1762, 0.2051, 0.1911, 0.2036, 0.2240],\n",
      "        [0.1748, 0.2040, 0.1912, 0.2052, 0.2248],\n",
      "        [0.1762, 0.2049, 0.1916, 0.2033, 0.2240],\n",
      "        [0.1736, 0.2035, 0.1907, 0.2061, 0.2261],\n",
      "        [0.1762, 0.2042, 0.1915, 0.2039, 0.2242],\n",
      "        [0.1760, 0.2045, 0.1916, 0.2039, 0.2241],\n",
      "        [0.1756, 0.2035, 0.1913, 0.2048, 0.2247],\n",
      "        [0.1768, 0.2061, 0.1912, 0.2027, 0.2231]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "25/100 Running Loss: 1.6054764986038208\n",
      "tensor([[0.1738, 0.2037, 0.1915, 0.2060, 0.2251],\n",
      "        [0.1734, 0.2040, 0.1909, 0.2059, 0.2257],\n",
      "        [0.1757, 0.2056, 0.1911, 0.2038, 0.2239],\n",
      "        [0.1742, 0.2043, 0.1913, 0.2054, 0.2248],\n",
      "        [0.1757, 0.2053, 0.1916, 0.2035, 0.2238],\n",
      "        [0.1730, 0.2039, 0.1907, 0.2064, 0.2260],\n",
      "        [0.1757, 0.2047, 0.1915, 0.2040, 0.2241],\n",
      "        [0.1755, 0.2049, 0.1917, 0.2041, 0.2239],\n",
      "        [0.1752, 0.2038, 0.1914, 0.2051, 0.2246],\n",
      "        [0.1764, 0.2066, 0.1912, 0.2029, 0.2229]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "26/100 Running Loss: 1.605236530303955\n",
      "tensor([[0.1732, 0.2040, 0.1916, 0.2062, 0.2250],\n",
      "        [0.1729, 0.2044, 0.1909, 0.2061, 0.2257],\n",
      "        [0.1751, 0.2060, 0.1912, 0.2040, 0.2237],\n",
      "        [0.1737, 0.2047, 0.1913, 0.2056, 0.2247],\n",
      "        [0.1752, 0.2058, 0.1917, 0.2037, 0.2237],\n",
      "        [0.1724, 0.2042, 0.1908, 0.2066, 0.2260],\n",
      "        [0.1753, 0.2051, 0.1916, 0.2042, 0.2239],\n",
      "        [0.1750, 0.2053, 0.1918, 0.2042, 0.2237],\n",
      "        [0.1747, 0.2042, 0.1914, 0.2053, 0.2245],\n",
      "        [0.1759, 0.2071, 0.1912, 0.2031, 0.2227]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "27/100 Running Loss: 1.604988694190979\n",
      "tensor([[0.1727, 0.2044, 0.1917, 0.2063, 0.2249],\n",
      "        [0.1723, 0.2048, 0.1910, 0.2063, 0.2256],\n",
      "        [0.1746, 0.2065, 0.1912, 0.2042, 0.2235],\n",
      "        [0.1731, 0.2051, 0.1914, 0.2058, 0.2246],\n",
      "        [0.1747, 0.2062, 0.1917, 0.2039, 0.2235],\n",
      "        [0.1718, 0.2046, 0.1908, 0.2069, 0.2259],\n",
      "        [0.1748, 0.2055, 0.1917, 0.2043, 0.2237],\n",
      "        [0.1745, 0.2057, 0.1918, 0.2044, 0.2236],\n",
      "        [0.1741, 0.2046, 0.1915, 0.2055, 0.2244],\n",
      "        [0.1754, 0.2076, 0.1913, 0.2032, 0.2225]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "28/100 Running Loss: 1.6047329902648926\n",
      "tensor([[0.1721, 0.2048, 0.1918, 0.2065, 0.2248],\n",
      "        [0.1717, 0.2052, 0.1910, 0.2066, 0.2256],\n",
      "        [0.1740, 0.2070, 0.1912, 0.2044, 0.2234],\n",
      "        [0.1725, 0.2055, 0.1914, 0.2060, 0.2245],\n",
      "        [0.1742, 0.2067, 0.1918, 0.2041, 0.2233],\n",
      "        [0.1712, 0.2049, 0.1909, 0.2071, 0.2259],\n",
      "        [0.1743, 0.2060, 0.1918, 0.2044, 0.2235],\n",
      "        [0.1740, 0.2062, 0.1919, 0.2045, 0.2234],\n",
      "        [0.1736, 0.2050, 0.1915, 0.2057, 0.2242],\n",
      "        [0.1748, 0.2082, 0.1913, 0.2034, 0.2223]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "29/100 Running Loss: 1.6044703722000122\n",
      "tensor([[0.1715, 0.2052, 0.1918, 0.2067, 0.2247],\n",
      "        [0.1711, 0.2056, 0.1911, 0.2068, 0.2255],\n",
      "        [0.1734, 0.2074, 0.1912, 0.2046, 0.2233],\n",
      "        [0.1720, 0.2059, 0.1915, 0.2062, 0.2244],\n",
      "        [0.1736, 0.2072, 0.1918, 0.2042, 0.2232],\n",
      "        [0.1706, 0.2054, 0.1909, 0.2073, 0.2258],\n",
      "        [0.1738, 0.2064, 0.1918, 0.2046, 0.2234],\n",
      "        [0.1735, 0.2066, 0.1920, 0.2046, 0.2232],\n",
      "        [0.1731, 0.2054, 0.1915, 0.2059, 0.2242],\n",
      "        [0.1743, 0.2088, 0.1913, 0.2036, 0.2221]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "30/100 Running Loss: 1.6042004823684692\n",
      "tensor([[0.1709, 0.2057, 0.1919, 0.2069, 0.2246],\n",
      "        [0.1704, 0.2061, 0.1911, 0.2070, 0.2254],\n",
      "        [0.1729, 0.2079, 0.1912, 0.2048, 0.2231],\n",
      "        [0.1714, 0.2064, 0.1915, 0.2064, 0.2243],\n",
      "        [0.1731, 0.2077, 0.1919, 0.2043, 0.2230],\n",
      "        [0.1700, 0.2058, 0.1910, 0.2075, 0.2258],\n",
      "        [0.1733, 0.2069, 0.1919, 0.2047, 0.2232],\n",
      "        [0.1730, 0.2071, 0.1921, 0.2048, 0.2230],\n",
      "        [0.1725, 0.2058, 0.1915, 0.2061, 0.2241],\n",
      "        [0.1737, 0.2094, 0.1914, 0.2037, 0.2219]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "31/100 Running Loss: 1.6039239168167114\n",
      "tensor([[0.1703, 0.2061, 0.1919, 0.2071, 0.2245],\n",
      "        [0.1698, 0.2065, 0.1912, 0.2072, 0.2253],\n",
      "        [0.1723, 0.2084, 0.1913, 0.2050, 0.2230],\n",
      "        [0.1708, 0.2068, 0.1916, 0.2066, 0.2242],\n",
      "        [0.1725, 0.2082, 0.1920, 0.2045, 0.2228],\n",
      "        [0.1693, 0.2062, 0.1910, 0.2077, 0.2257],\n",
      "        [0.1728, 0.2074, 0.1920, 0.2048, 0.2231],\n",
      "        [0.1725, 0.2076, 0.1921, 0.2049, 0.2229],\n",
      "        [0.1720, 0.2062, 0.1916, 0.2063, 0.2240],\n",
      "        [0.1732, 0.2100, 0.1914, 0.2038, 0.2217]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "32/100 Running Loss: 1.6036410331726074\n",
      "tensor([[0.1697, 0.2066, 0.1920, 0.2073, 0.2244],\n",
      "        [0.1691, 0.2070, 0.1912, 0.2074, 0.2253],\n",
      "        [0.1717, 0.2090, 0.1913, 0.2052, 0.2229],\n",
      "        [0.1702, 0.2073, 0.1916, 0.2068, 0.2241],\n",
      "        [0.1719, 0.2087, 0.1920, 0.2046, 0.2227],\n",
      "        [0.1687, 0.2067, 0.1911, 0.2080, 0.2256],\n",
      "        [0.1723, 0.2078, 0.1920, 0.2049, 0.2229],\n",
      "        [0.1719, 0.2081, 0.1922, 0.2050, 0.2227],\n",
      "        [0.1714, 0.2066, 0.1916, 0.2065, 0.2239],\n",
      "        [0.1726, 0.2106, 0.1915, 0.2038, 0.2215]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "33/100 Running Loss: 1.603350043296814\n",
      "tensor([[0.1691, 0.2070, 0.1920, 0.2076, 0.2243],\n",
      "        [0.1685, 0.2074, 0.1912, 0.2077, 0.2252],\n",
      "        [0.1711, 0.2095, 0.1913, 0.2054, 0.2227],\n",
      "        [0.1696, 0.2078, 0.1917, 0.2070, 0.2240],\n",
      "        [0.1713, 0.2093, 0.1921, 0.2047, 0.2226],\n",
      "        [0.1680, 0.2071, 0.1911, 0.2082, 0.2256],\n",
      "        [0.1717, 0.2083, 0.1921, 0.2051, 0.2228],\n",
      "        [0.1714, 0.2086, 0.1923, 0.2052, 0.2226],\n",
      "        [0.1708, 0.2071, 0.1916, 0.2067, 0.2238],\n",
      "        [0.1720, 0.2113, 0.1915, 0.2039, 0.2213]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "34/100 Running Loss: 1.6030547618865967\n",
      "tensor([[0.1684, 0.2075, 0.1920, 0.2078, 0.2242],\n",
      "        [0.1678, 0.2079, 0.1913, 0.2079, 0.2251],\n",
      "        [0.1704, 0.2101, 0.1914, 0.2055, 0.2225],\n",
      "        [0.1689, 0.2083, 0.1917, 0.2072, 0.2239],\n",
      "        [0.1707, 0.2099, 0.1922, 0.2048, 0.2224],\n",
      "        [0.1673, 0.2076, 0.1911, 0.2085, 0.2255],\n",
      "        [0.1712, 0.2089, 0.1921, 0.2052, 0.2226],\n",
      "        [0.1708, 0.2092, 0.1924, 0.2052, 0.2224],\n",
      "        [0.1702, 0.2076, 0.1916, 0.2069, 0.2236],\n",
      "        [0.1714, 0.2120, 0.1916, 0.2040, 0.2211]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "35/100 Running Loss: 1.6027501821517944\n",
      "tensor([[0.1678, 0.2080, 0.1921, 0.2080, 0.2241],\n",
      "        [0.1671, 0.2085, 0.1913, 0.2082, 0.2250],\n",
      "        [0.1698, 0.2108, 0.1914, 0.2057, 0.2224],\n",
      "        [0.1683, 0.2088, 0.1918, 0.2074, 0.2238],\n",
      "        [0.1701, 0.2105, 0.1922, 0.2050, 0.2222],\n",
      "        [0.1666, 0.2081, 0.1911, 0.2088, 0.2254],\n",
      "        [0.1706, 0.2094, 0.1922, 0.2053, 0.2224],\n",
      "        [0.1702, 0.2098, 0.1925, 0.2053, 0.2222],\n",
      "        [0.1696, 0.2081, 0.1917, 0.2071, 0.2235],\n",
      "        [0.1708, 0.2127, 0.1917, 0.2040, 0.2208]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "36/100 Running Loss: 1.6024421453475952\n",
      "tensor([[0.1671, 0.2085, 0.1921, 0.2082, 0.2240],\n",
      "        [0.1663, 0.2090, 0.1913, 0.2084, 0.2249],\n",
      "        [0.1691, 0.2114, 0.1915, 0.2058, 0.2222],\n",
      "        [0.1676, 0.2094, 0.1918, 0.2076, 0.2237],\n",
      "        [0.1694, 0.2111, 0.1923, 0.2051, 0.2221],\n",
      "        [0.1659, 0.2086, 0.1911, 0.2090, 0.2254],\n",
      "        [0.1700, 0.2100, 0.1923, 0.2055, 0.2222],\n",
      "        [0.1696, 0.2104, 0.1926, 0.2054, 0.2220],\n",
      "        [0.1690, 0.2086, 0.1917, 0.2073, 0.2234],\n",
      "        [0.1701, 0.2134, 0.1918, 0.2041, 0.2206]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "37/100 Running Loss: 1.6021287441253662\n",
      "tensor([[0.1664, 0.2091, 0.1922, 0.2084, 0.2239],\n",
      "        [0.1656, 0.2095, 0.1913, 0.2087, 0.2248],\n",
      "        [0.1684, 0.2121, 0.1915, 0.2059, 0.2220],\n",
      "        [0.1669, 0.2100, 0.1919, 0.2078, 0.2235],\n",
      "        [0.1688, 0.2118, 0.1924, 0.2052, 0.2219],\n",
      "        [0.1651, 0.2091, 0.1912, 0.2093, 0.2253],\n",
      "        [0.1694, 0.2107, 0.1924, 0.2056, 0.2220],\n",
      "        [0.1690, 0.2110, 0.1927, 0.2055, 0.2218],\n",
      "        [0.1683, 0.2092, 0.1918, 0.2075, 0.2232],\n",
      "        [0.1695, 0.2142, 0.1918, 0.2042, 0.2204]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "38/100 Running Loss: 1.601806640625\n",
      "tensor([[0.1657, 0.2096, 0.1922, 0.2086, 0.2238],\n",
      "        [0.1648, 0.2101, 0.1913, 0.2090, 0.2247],\n",
      "        [0.1677, 0.2128, 0.1916, 0.2061, 0.2218],\n",
      "        [0.1661, 0.2106, 0.1919, 0.2080, 0.2234],\n",
      "        [0.1681, 0.2125, 0.1925, 0.2053, 0.2217],\n",
      "        [0.1644, 0.2097, 0.1912, 0.2095, 0.2252],\n",
      "        [0.1688, 0.2113, 0.1924, 0.2057, 0.2218],\n",
      "        [0.1684, 0.2117, 0.1927, 0.2057, 0.2215],\n",
      "        [0.1677, 0.2097, 0.1918, 0.2077, 0.2231],\n",
      "        [0.1688, 0.2149, 0.1919, 0.2043, 0.2201]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "39/100 Running Loss: 1.6014782190322876\n",
      "tensor([[0.1650, 0.2102, 0.1923, 0.2088, 0.2237],\n",
      "        [0.1641, 0.2108, 0.1914, 0.2092, 0.2246],\n",
      "        [0.1670, 0.2136, 0.1916, 0.2062, 0.2216],\n",
      "        [0.1654, 0.2112, 0.1920, 0.2082, 0.2232],\n",
      "        [0.1674, 0.2132, 0.1925, 0.2054, 0.2215],\n",
      "        [0.1636, 0.2103, 0.1913, 0.2097, 0.2251],\n",
      "        [0.1681, 0.2120, 0.1925, 0.2058, 0.2216],\n",
      "        [0.1677, 0.2124, 0.1928, 0.2058, 0.2213],\n",
      "        [0.1670, 0.2103, 0.1919, 0.2079, 0.2229],\n",
      "        [0.1681, 0.2157, 0.1919, 0.2043, 0.2199]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "40/100 Running Loss: 1.601143479347229\n",
      "tensor([[0.1642, 0.2108, 0.1923, 0.2090, 0.2236],\n",
      "        [0.1633, 0.2114, 0.1914, 0.2094, 0.2245],\n",
      "        [0.1662, 0.2143, 0.1917, 0.2064, 0.2214],\n",
      "        [0.1646, 0.2119, 0.1920, 0.2084, 0.2231],\n",
      "        [0.1667, 0.2139, 0.1926, 0.2055, 0.2213],\n",
      "        [0.1628, 0.2109, 0.1913, 0.2100, 0.2250],\n",
      "        [0.1675, 0.2126, 0.1926, 0.2060, 0.2214],\n",
      "        [0.1670, 0.2131, 0.1929, 0.2059, 0.2211],\n",
      "        [0.1663, 0.2109, 0.1919, 0.2081, 0.2228],\n",
      "        [0.1674, 0.2166, 0.1920, 0.2044, 0.2196]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "41/100 Running Loss: 1.600803256034851\n",
      "tensor([[0.1634, 0.2114, 0.1924, 0.2093, 0.2235],\n",
      "        [0.1624, 0.2121, 0.1915, 0.2096, 0.2244],\n",
      "        [0.1655, 0.2151, 0.1917, 0.2065, 0.2212],\n",
      "        [0.1638, 0.2126, 0.1921, 0.2086, 0.2230],\n",
      "        [0.1660, 0.2146, 0.1927, 0.2056, 0.2211],\n",
      "        [0.1620, 0.2115, 0.1914, 0.2102, 0.2249],\n",
      "        [0.1668, 0.2133, 0.1927, 0.2061, 0.2211],\n",
      "        [0.1663, 0.2138, 0.1930, 0.2061, 0.2209],\n",
      "        [0.1656, 0.2115, 0.1920, 0.2083, 0.2226],\n",
      "        [0.1667, 0.2174, 0.1920, 0.2045, 0.2193]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/100 Running Loss: 1.600455403327942\n",
      "tensor([[0.1626, 0.2121, 0.1924, 0.2095, 0.2234],\n",
      "        [0.1616, 0.2128, 0.1915, 0.2099, 0.2243],\n",
      "        [0.1647, 0.2159, 0.1918, 0.2067, 0.2210],\n",
      "        [0.1630, 0.2132, 0.1921, 0.2088, 0.2228],\n",
      "        [0.1652, 0.2154, 0.1927, 0.2057, 0.2209],\n",
      "        [0.1612, 0.2122, 0.1914, 0.2105, 0.2248],\n",
      "        [0.1661, 0.2140, 0.1927, 0.2063, 0.2209],\n",
      "        [0.1656, 0.2145, 0.1930, 0.2062, 0.2206],\n",
      "        [0.1649, 0.2122, 0.1920, 0.2085, 0.2225],\n",
      "        [0.1659, 0.2183, 0.1921, 0.2046, 0.2191]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "43/100 Running Loss: 1.6001019477844238\n",
      "tensor([[0.1618, 0.2127, 0.1925, 0.2098, 0.2233],\n",
      "        [0.1608, 0.2135, 0.1915, 0.2101, 0.2241],\n",
      "        [0.1639, 0.2167, 0.1918, 0.2068, 0.2207],\n",
      "        [0.1622, 0.2139, 0.1922, 0.2090, 0.2227],\n",
      "        [0.1644, 0.2162, 0.1928, 0.2059, 0.2207],\n",
      "        [0.1603, 0.2128, 0.1914, 0.2107, 0.2247],\n",
      "        [0.1654, 0.2147, 0.1928, 0.2064, 0.2207],\n",
      "        [0.1648, 0.2153, 0.1931, 0.2064, 0.2204],\n",
      "        [0.1641, 0.2128, 0.1921, 0.2087, 0.2223],\n",
      "        [0.1652, 0.2192, 0.1922, 0.2047, 0.2188]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "44/100 Running Loss: 1.5997400283813477\n",
      "tensor([[0.1609, 0.2134, 0.1925, 0.2100, 0.2232],\n",
      "        [0.1599, 0.2142, 0.1915, 0.2104, 0.2240],\n",
      "        [0.1631, 0.2175, 0.1919, 0.2070, 0.2205],\n",
      "        [0.1614, 0.2146, 0.1922, 0.2093, 0.2225],\n",
      "        [0.1636, 0.2170, 0.1929, 0.2060, 0.2205],\n",
      "        [0.1594, 0.2136, 0.1915, 0.2110, 0.2246],\n",
      "        [0.1647, 0.2155, 0.1929, 0.2065, 0.2205],\n",
      "        [0.1641, 0.2161, 0.1932, 0.2065, 0.2202],\n",
      "        [0.1634, 0.2135, 0.1921, 0.2089, 0.2222],\n",
      "        [0.1644, 0.2201, 0.1922, 0.2047, 0.2185]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "45/100 Running Loss: 1.5993703603744507\n",
      "tensor([[0.1601, 0.2140, 0.1926, 0.2102, 0.2231],\n",
      "        [0.1590, 0.2149, 0.1916, 0.2106, 0.2239],\n",
      "        [0.1623, 0.2184, 0.1920, 0.2071, 0.2203],\n",
      "        [0.1605, 0.2153, 0.1923, 0.2095, 0.2224],\n",
      "        [0.1628, 0.2178, 0.1930, 0.2061, 0.2203],\n",
      "        [0.1585, 0.2143, 0.1915, 0.2112, 0.2245],\n",
      "        [0.1639, 0.2162, 0.1930, 0.2067, 0.2202],\n",
      "        [0.1633, 0.2168, 0.1933, 0.2066, 0.2200],\n",
      "        [0.1626, 0.2141, 0.1922, 0.2091, 0.2220],\n",
      "        [0.1636, 0.2210, 0.1923, 0.2048, 0.2183]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "46/100 Running Loss: 1.598994255065918\n",
      "tensor([[0.1592, 0.2148, 0.1926, 0.2103, 0.2231],\n",
      "        [0.1581, 0.2157, 0.1916, 0.2108, 0.2237],\n",
      "        [0.1614, 0.2192, 0.1920, 0.2073, 0.2201],\n",
      "        [0.1597, 0.2161, 0.1923, 0.2097, 0.2223],\n",
      "        [0.1620, 0.2186, 0.1931, 0.2062, 0.2201],\n",
      "        [0.1576, 0.2150, 0.1915, 0.2115, 0.2244],\n",
      "        [0.1632, 0.2170, 0.1930, 0.2068, 0.2200],\n",
      "        [0.1625, 0.2176, 0.1934, 0.2068, 0.2197],\n",
      "        [0.1618, 0.2148, 0.1922, 0.2093, 0.2218],\n",
      "        [0.1628, 0.2219, 0.1924, 0.2049, 0.2180]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "47/100 Running Loss: 1.598610758781433\n",
      "tensor([[0.1584, 0.2155, 0.1926, 0.2105, 0.2230],\n",
      "        [0.1572, 0.2165, 0.1917, 0.2110, 0.2237],\n",
      "        [0.1605, 0.2201, 0.1921, 0.2075, 0.2199],\n",
      "        [0.1588, 0.2169, 0.1923, 0.2098, 0.2222],\n",
      "        [0.1611, 0.2195, 0.1932, 0.2064, 0.2199],\n",
      "        [0.1567, 0.2158, 0.1916, 0.2117, 0.2243],\n",
      "        [0.1624, 0.2178, 0.1931, 0.2070, 0.2198],\n",
      "        [0.1617, 0.2184, 0.1934, 0.2069, 0.2195],\n",
      "        [0.1610, 0.2155, 0.1923, 0.2095, 0.2216],\n",
      "        [0.1620, 0.2228, 0.1925, 0.2049, 0.2178]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "48/100 Running Loss: 1.5982205867767334\n",
      "tensor([[0.1575, 0.2162, 0.1927, 0.2106, 0.2230],\n",
      "        [0.1562, 0.2173, 0.1917, 0.2112, 0.2236],\n",
      "        [0.1596, 0.2210, 0.1921, 0.2076, 0.2196],\n",
      "        [0.1579, 0.2176, 0.1924, 0.2100, 0.2221],\n",
      "        [0.1602, 0.2204, 0.1932, 0.2065, 0.2197],\n",
      "        [0.1558, 0.2165, 0.1916, 0.2119, 0.2243],\n",
      "        [0.1616, 0.2186, 0.1932, 0.2071, 0.2196],\n",
      "        [0.1609, 0.2192, 0.1935, 0.2070, 0.2193],\n",
      "        [0.1602, 0.2162, 0.1923, 0.2097, 0.2215],\n",
      "        [0.1611, 0.2238, 0.1926, 0.2050, 0.2175]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "49/100 Running Loss: 1.5978213548660278\n",
      "tensor([[0.1566, 0.2170, 0.1927, 0.2108, 0.2230],\n",
      "        [0.1553, 0.2181, 0.1917, 0.2114, 0.2235],\n",
      "        [0.1587, 0.2219, 0.1922, 0.2077, 0.2195],\n",
      "        [0.1570, 0.2184, 0.1924, 0.2101, 0.2221],\n",
      "        [0.1594, 0.2213, 0.1933, 0.2066, 0.2194],\n",
      "        [0.1548, 0.2173, 0.1916, 0.2120, 0.2242],\n",
      "        [0.1608, 0.2194, 0.1933, 0.2072, 0.2194],\n",
      "        [0.1600, 0.2201, 0.1936, 0.2072, 0.2191],\n",
      "        [0.1593, 0.2170, 0.1924, 0.2099, 0.2214],\n",
      "        [0.1602, 0.2248, 0.1927, 0.2050, 0.2172]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "50/100 Running Loss: 1.5974105596542358\n",
      "tensor([[0.1557, 0.2177, 0.1928, 0.2109, 0.2230],\n",
      "        [0.1543, 0.2189, 0.1917, 0.2116, 0.2235],\n",
      "        [0.1578, 0.2229, 0.1922, 0.2078, 0.2193],\n",
      "        [0.1560, 0.2193, 0.1925, 0.2103, 0.2220],\n",
      "        [0.1584, 0.2222, 0.1934, 0.2067, 0.2192],\n",
      "        [0.1539, 0.2181, 0.1916, 0.2122, 0.2242],\n",
      "        [0.1600, 0.2202, 0.1933, 0.2073, 0.2192],\n",
      "        [0.1592, 0.2210, 0.1937, 0.2072, 0.2189],\n",
      "        [0.1584, 0.2178, 0.1924, 0.2101, 0.2213],\n",
      "        [0.1593, 0.2259, 0.1928, 0.2051, 0.2169]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "51/100 Running Loss: 1.5969889163970947\n",
      "tensor([[0.1547, 0.2185, 0.1928, 0.2110, 0.2229],\n",
      "        [0.1533, 0.2198, 0.1917, 0.2118, 0.2234],\n",
      "        [0.1569, 0.2239, 0.1922, 0.2079, 0.2191],\n",
      "        [0.1551, 0.2201, 0.1925, 0.2104, 0.2219],\n",
      "        [0.1575, 0.2232, 0.1934, 0.2068, 0.2191],\n",
      "        [0.1529, 0.2190, 0.1916, 0.2124, 0.2242],\n",
      "        [0.1591, 0.2211, 0.1934, 0.2075, 0.2189],\n",
      "        [0.1583, 0.2219, 0.1938, 0.2073, 0.2187],\n",
      "        [0.1576, 0.2185, 0.1924, 0.2102, 0.2213],\n",
      "        [0.1584, 0.2270, 0.1929, 0.2050, 0.2167]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "52/100 Running Loss: 1.5965558290481567\n",
      "tensor([[0.1538, 0.2193, 0.1928, 0.2112, 0.2229],\n",
      "        [0.1523, 0.2206, 0.1917, 0.2119, 0.2234],\n",
      "        [0.1559, 0.2249, 0.1923, 0.2080, 0.2189],\n",
      "        [0.1541, 0.2210, 0.1926, 0.2106, 0.2218],\n",
      "        [0.1566, 0.2242, 0.1935, 0.2068, 0.2189],\n",
      "        [0.1518, 0.2198, 0.1916, 0.2126, 0.2242],\n",
      "        [0.1583, 0.2220, 0.1935, 0.2076, 0.2187],\n",
      "        [0.1574, 0.2228, 0.1939, 0.2074, 0.2185],\n",
      "        [0.1567, 0.2193, 0.1924, 0.2104, 0.2212],\n",
      "        [0.1574, 0.2282, 0.1930, 0.2050, 0.2164]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "53/100 Running Loss: 1.596110224723816\n",
      "tensor([[0.1528, 0.2201, 0.1929, 0.2113, 0.2229],\n",
      "        [0.1513, 0.2215, 0.1917, 0.2121, 0.2234],\n",
      "        [0.1549, 0.2259, 0.1923, 0.2081, 0.2187],\n",
      "        [0.1531, 0.2218, 0.1926, 0.2107, 0.2217],\n",
      "        [0.1556, 0.2253, 0.1935, 0.2069, 0.2187],\n",
      "        [0.1508, 0.2207, 0.1916, 0.2128, 0.2241],\n",
      "        [0.1574, 0.2229, 0.1936, 0.2077, 0.2185],\n",
      "        [0.1565, 0.2237, 0.1941, 0.2074, 0.2183],\n",
      "        [0.1558, 0.2201, 0.1925, 0.2106, 0.2211],\n",
      "        [0.1564, 0.2294, 0.1931, 0.2049, 0.2162]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "54/100 Running Loss: 1.595649242401123\n",
      "tensor([[0.1518, 0.2210, 0.1929, 0.2115, 0.2228],\n",
      "        [0.1502, 0.2224, 0.1918, 0.2123, 0.2233],\n",
      "        [0.1539, 0.2270, 0.1924, 0.2081, 0.2185],\n",
      "        [0.1521, 0.2227, 0.1927, 0.2108, 0.2217],\n",
      "        [0.1547, 0.2263, 0.1936, 0.2069, 0.2185],\n",
      "        [0.1497, 0.2216, 0.1916, 0.2130, 0.2241],\n",
      "        [0.1565, 0.2238, 0.1937, 0.2077, 0.2183],\n",
      "        [0.1556, 0.2247, 0.1942, 0.2075, 0.2181],\n",
      "        [0.1548, 0.2209, 0.1925, 0.2107, 0.2211],\n",
      "        [0.1554, 0.2306, 0.1931, 0.2049, 0.2159]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "55/100 Running Loss: 1.5951738357543945\n",
      "tensor([[0.1508, 0.2219, 0.1930, 0.2116, 0.2228],\n",
      "        [0.1491, 0.2234, 0.1918, 0.2124, 0.2232],\n",
      "        [0.1528, 0.2281, 0.1925, 0.2082, 0.2183],\n",
      "        [0.1510, 0.2237, 0.1928, 0.2109, 0.2216],\n",
      "        [0.1536, 0.2275, 0.1937, 0.2069, 0.2183],\n",
      "        [0.1486, 0.2225, 0.1916, 0.2131, 0.2241],\n",
      "        [0.1556, 0.2248, 0.1938, 0.2077, 0.2181],\n",
      "        [0.1547, 0.2257, 0.1943, 0.2075, 0.2179],\n",
      "        [0.1539, 0.2217, 0.1925, 0.2108, 0.2210],\n",
      "        [0.1544, 0.2319, 0.1932, 0.2049, 0.2157]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "56/100 Running Loss: 1.5946799516677856\n",
      "tensor([[0.1497, 0.2227, 0.1931, 0.2117, 0.2227],\n",
      "        [0.1480, 0.2244, 0.1919, 0.2126, 0.2232],\n",
      "        [0.1517, 0.2294, 0.1926, 0.2082, 0.2181],\n",
      "        [0.1500, 0.2246, 0.1929, 0.2110, 0.2215],\n",
      "        [0.1526, 0.2286, 0.1938, 0.2069, 0.2181],\n",
      "        [0.1475, 0.2235, 0.1917, 0.2133, 0.2240],\n",
      "        [0.1547, 0.2258, 0.1939, 0.2078, 0.2179],\n",
      "        [0.1537, 0.2268, 0.1945, 0.2075, 0.2176],\n",
      "        [0.1529, 0.2226, 0.1926, 0.2110, 0.2209],\n",
      "        [0.1533, 0.2332, 0.1933, 0.2048, 0.2154]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "57/100 Running Loss: 1.5941721200942993\n",
      "tensor([[0.1486, 0.2236, 0.1932, 0.2119, 0.2227],\n",
      "        [0.1469, 0.2254, 0.1920, 0.2127, 0.2231],\n",
      "        [0.1506, 0.2306, 0.1927, 0.2082, 0.2178],\n",
      "        [0.1489, 0.2257, 0.1930, 0.2111, 0.2214],\n",
      "        [0.1515, 0.2299, 0.1939, 0.2069, 0.2179],\n",
      "        [0.1464, 0.2245, 0.1917, 0.2134, 0.2240],\n",
      "        [0.1537, 0.2268, 0.1941, 0.2077, 0.2177],\n",
      "        [0.1526, 0.2279, 0.1947, 0.2075, 0.2174],\n",
      "        [0.1519, 0.2235, 0.1926, 0.2111, 0.2209],\n",
      "        [0.1522, 0.2346, 0.1934, 0.2047, 0.2151]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "58/100 Running Loss: 1.5936501026153564\n",
      "tensor([[0.1475, 0.2246, 0.1933, 0.2120, 0.2227],\n",
      "        [0.1457, 0.2264, 0.1920, 0.2128, 0.2230],\n",
      "        [0.1494, 0.2319, 0.1929, 0.2082, 0.2176],\n",
      "        [0.1477, 0.2267, 0.1931, 0.2112, 0.2213],\n",
      "        [0.1503, 0.2311, 0.1940, 0.2068, 0.2177],\n",
      "        [0.1452, 0.2255, 0.1918, 0.2136, 0.2239],\n",
      "        [0.1527, 0.2279, 0.1942, 0.2077, 0.2175],\n",
      "        [0.1516, 0.2290, 0.1948, 0.2074, 0.2171],\n",
      "        [0.1509, 0.2244, 0.1927, 0.2112, 0.2208],\n",
      "        [0.1511, 0.2360, 0.1935, 0.2046, 0.2148]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "59/100 Running Loss: 1.593111515045166\n",
      "tensor([[0.1464, 0.2255, 0.1933, 0.2121, 0.2226],\n",
      "        [0.1445, 0.2275, 0.1921, 0.2129, 0.2230],\n",
      "        [0.1482, 0.2333, 0.1930, 0.2082, 0.2173],\n",
      "        [0.1465, 0.2278, 0.1932, 0.2113, 0.2212],\n",
      "        [0.1491, 0.2324, 0.1942, 0.2068, 0.2175],\n",
      "        [0.1441, 0.2265, 0.1918, 0.2137, 0.2239],\n",
      "        [0.1517, 0.2291, 0.1944, 0.2077, 0.2172],\n",
      "        [0.1505, 0.2303, 0.1950, 0.2074, 0.2169],\n",
      "        [0.1499, 0.2253, 0.1928, 0.2113, 0.2208],\n",
      "        [0.1499, 0.2375, 0.1937, 0.2045, 0.2145]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "60/100 Running Loss: 1.592559576034546\n",
      "tensor([[0.1452, 0.2265, 0.1935, 0.2122, 0.2226],\n",
      "        [0.1433, 0.2287, 0.1922, 0.2130, 0.2229],\n",
      "        [0.1470, 0.2347, 0.1932, 0.2081, 0.2171],\n",
      "        [0.1453, 0.2290, 0.1933, 0.2114, 0.2210],\n",
      "        [0.1480, 0.2337, 0.1943, 0.2067, 0.2172],\n",
      "        [0.1428, 0.2276, 0.1919, 0.2138, 0.2238],\n",
      "        [0.1506, 0.2303, 0.1945, 0.2076, 0.2170],\n",
      "        [0.1494, 0.2315, 0.1952, 0.2073, 0.2166],\n",
      "        [0.1488, 0.2263, 0.1928, 0.2114, 0.2207],\n",
      "        [0.1487, 0.2390, 0.1938, 0.2044, 0.2141]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "61/100 Running Loss: 1.5919922590255737\n",
      "tensor([[0.1440, 0.2276, 0.1935, 0.2123, 0.2225],\n",
      "        [0.1420, 0.2298, 0.1924, 0.2130, 0.2228],\n",
      "        [0.1457, 0.2361, 0.1933, 0.2080, 0.2168],\n",
      "        [0.1440, 0.2302, 0.1934, 0.2115, 0.2209],\n",
      "        [0.1467, 0.2351, 0.1945, 0.2067, 0.2170],\n",
      "        [0.1416, 0.2287, 0.1920, 0.2139, 0.2238],\n",
      "        [0.1495, 0.2315, 0.1947, 0.2076, 0.2168],\n",
      "        [0.1482, 0.2328, 0.1954, 0.2072, 0.2164],\n",
      "        [0.1477, 0.2274, 0.1929, 0.2115, 0.2206],\n",
      "        [0.1475, 0.2406, 0.1939, 0.2042, 0.2138]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "62/100 Running Loss: 1.5914051532745361\n",
      "tensor([[0.1428, 0.2287, 0.1936, 0.2124, 0.2225],\n",
      "        [0.1406, 0.2311, 0.1925, 0.2131, 0.2227],\n",
      "        [0.1445, 0.2376, 0.1934, 0.2080, 0.2165],\n",
      "        [0.1428, 0.2314, 0.1935, 0.2115, 0.2207],\n",
      "        [0.1455, 0.2366, 0.1946, 0.2066, 0.2167],\n",
      "        [0.1403, 0.2299, 0.1921, 0.2140, 0.2237],\n",
      "        [0.1484, 0.2328, 0.1948, 0.2075, 0.2165],\n",
      "        [0.1471, 0.2341, 0.1955, 0.2072, 0.2161],\n",
      "        [0.1465, 0.2285, 0.1930, 0.2115, 0.2205],\n",
      "        [0.1462, 0.2423, 0.1941, 0.2040, 0.2134]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "63/100 Running Loss: 1.5907953977584839\n",
      "tensor([[0.1415, 0.2298, 0.1938, 0.2126, 0.2224],\n",
      "        [0.1393, 0.2324, 0.1925, 0.2133, 0.2225],\n",
      "        [0.1432, 0.2392, 0.1936, 0.2079, 0.2162],\n",
      "        [0.1415, 0.2327, 0.1937, 0.2116, 0.2206],\n",
      "        [0.1442, 0.2381, 0.1948, 0.2065, 0.2164],\n",
      "        [0.1390, 0.2311, 0.1922, 0.2141, 0.2235],\n",
      "        [0.1472, 0.2341, 0.1950, 0.2074, 0.2162],\n",
      "        [0.1459, 0.2355, 0.1957, 0.2071, 0.2158],\n",
      "        [0.1453, 0.2296, 0.1932, 0.2116, 0.2203],\n",
      "        [0.1450, 0.2440, 0.1942, 0.2038, 0.2130]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "64/100 Running Loss: 1.590164065361023\n",
      "tensor([[0.1402, 0.2310, 0.1938, 0.2127, 0.2223],\n",
      "        [0.1380, 0.2338, 0.1926, 0.2134, 0.2223],\n",
      "        [0.1419, 0.2407, 0.1937, 0.2078, 0.2159],\n",
      "        [0.1401, 0.2341, 0.1938, 0.2116, 0.2204],\n",
      "        [0.1429, 0.2397, 0.1949, 0.2064, 0.2160],\n",
      "        [0.1377, 0.2324, 0.1923, 0.2142, 0.2234],\n",
      "        [0.1460, 0.2355, 0.1952, 0.2073, 0.2159],\n",
      "        [0.1447, 0.2370, 0.1959, 0.2070, 0.2155],\n",
      "        [0.1441, 0.2308, 0.1933, 0.2116, 0.2202],\n",
      "        [0.1437, 0.2458, 0.1943, 0.2036, 0.2126]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "65/100 Running Loss: 1.5895134210586548\n",
      "tensor([[0.1389, 0.2322, 0.1939, 0.2128, 0.2221],\n",
      "        [0.1366, 0.2351, 0.1927, 0.2135, 0.2221],\n",
      "        [0.1405, 0.2424, 0.1939, 0.2077, 0.2156],\n",
      "        [0.1388, 0.2354, 0.1939, 0.2117, 0.2202],\n",
      "        [0.1415, 0.2413, 0.1951, 0.2063, 0.2157],\n",
      "        [0.1363, 0.2337, 0.1923, 0.2144, 0.2233],\n",
      "        [0.1448, 0.2369, 0.1954, 0.2072, 0.2156],\n",
      "        [0.1434, 0.2384, 0.1961, 0.2069, 0.2152],\n",
      "        [0.1428, 0.2320, 0.1934, 0.2117, 0.2201],\n",
      "        [0.1424, 0.2476, 0.1944, 0.2034, 0.2122]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "66/100 Running Loss: 1.5888407230377197\n",
      "tensor([[0.1376, 0.2334, 0.1940, 0.2130, 0.2220],\n",
      "        [0.1352, 0.2365, 0.1927, 0.2136, 0.2219],\n",
      "        [0.1392, 0.2441, 0.1940, 0.2076, 0.2152],\n",
      "        [0.1375, 0.2368, 0.1940, 0.2118, 0.2200],\n",
      "        [0.1402, 0.2430, 0.1953, 0.2062, 0.2153],\n",
      "        [0.1350, 0.2350, 0.1923, 0.2145, 0.2232],\n",
      "        [0.1436, 0.2384, 0.1956, 0.2071, 0.2153],\n",
      "        [0.1422, 0.2400, 0.1963, 0.2068, 0.2148],\n",
      "        [0.1416, 0.2332, 0.1935, 0.2117, 0.2200],\n",
      "        [0.1410, 0.2495, 0.1946, 0.2031, 0.2118]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "67/100 Running Loss: 1.588146686553955\n",
      "tensor([[0.1363, 0.2347, 0.1941, 0.2131, 0.2218],\n",
      "        [0.1338, 0.2379, 0.1928, 0.2137, 0.2218],\n",
      "        [0.1378, 0.2458, 0.1941, 0.2075, 0.2148],\n",
      "        [0.1361, 0.2382, 0.1941, 0.2118, 0.2197],\n",
      "        [0.1388, 0.2448, 0.1954, 0.2061, 0.2149],\n",
      "        [0.1336, 0.2364, 0.1924, 0.2147, 0.2230],\n",
      "        [0.1424, 0.2399, 0.1958, 0.2069, 0.2150],\n",
      "        [0.1409, 0.2416, 0.1964, 0.2067, 0.2144],\n",
      "        [0.1403, 0.2344, 0.1936, 0.2118, 0.2199],\n",
      "        [0.1396, 0.2515, 0.1947, 0.2028, 0.2113]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "68/100 Running Loss: 1.5874313116073608\n",
      "tensor([[0.1350, 0.2360, 0.1942, 0.2132, 0.2217],\n",
      "        [0.1324, 0.2394, 0.1929, 0.2138, 0.2216],\n",
      "        [0.1364, 0.2476, 0.1943, 0.2074, 0.2143],\n",
      "        [0.1348, 0.2397, 0.1942, 0.2119, 0.2195],\n",
      "        [0.1374, 0.2466, 0.1956, 0.2060, 0.2144],\n",
      "        [0.1321, 0.2378, 0.1924, 0.2148, 0.2229],\n",
      "        [0.1411, 0.2415, 0.1960, 0.2068, 0.2146],\n",
      "        [0.1397, 0.2432, 0.1966, 0.2066, 0.2140],\n",
      "        [0.1390, 0.2357, 0.1936, 0.2119, 0.2197],\n",
      "        [0.1383, 0.2536, 0.1948, 0.2026, 0.2107]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "69/100 Running Loss: 1.5866930484771729\n",
      "tensor([[0.1336, 0.2374, 0.1943, 0.2133, 0.2215],\n",
      "        [0.1309, 0.2409, 0.1929, 0.2139, 0.2214],\n",
      "        [0.1350, 0.2495, 0.1944, 0.2073, 0.2139],\n",
      "        [0.1333, 0.2411, 0.1943, 0.2120, 0.2193],\n",
      "        [0.1360, 0.2485, 0.1957, 0.2058, 0.2139],\n",
      "        [0.1307, 0.2392, 0.1925, 0.2149, 0.2227],\n",
      "        [0.1399, 0.2431, 0.1962, 0.2067, 0.2141],\n",
      "        [0.1384, 0.2448, 0.1968, 0.2064, 0.2136],\n",
      "        [0.1377, 0.2370, 0.1937, 0.2120, 0.2196],\n",
      "        [0.1368, 0.2558, 0.1950, 0.2023, 0.2101]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "70/100 Running Loss: 1.5859346389770508\n",
      "tensor([[0.1322, 0.2388, 0.1944, 0.2134, 0.2213],\n",
      "        [0.1294, 0.2424, 0.1930, 0.2140, 0.2212],\n",
      "        [0.1336, 0.2514, 0.1945, 0.2071, 0.2134],\n",
      "        [0.1319, 0.2427, 0.1944, 0.2120, 0.2190],\n",
      "        [0.1345, 0.2505, 0.1959, 0.2057, 0.2134],\n",
      "        [0.1292, 0.2407, 0.1926, 0.2150, 0.2225],\n",
      "        [0.1386, 0.2447, 0.1964, 0.2066, 0.2137],\n",
      "        [0.1370, 0.2465, 0.1970, 0.2063, 0.2131],\n",
      "        [0.1364, 0.2384, 0.1938, 0.2120, 0.2194],\n",
      "        [0.1354, 0.2580, 0.1951, 0.2020, 0.2095]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "71/100 Running Loss: 1.5851519107818604\n",
      "tensor([[0.1308, 0.2402, 0.1945, 0.2135, 0.2210],\n",
      "        [0.1279, 0.2440, 0.1931, 0.2141, 0.2209],\n",
      "        [0.1321, 0.2533, 0.1947, 0.2070, 0.2130],\n",
      "        [0.1305, 0.2443, 0.1945, 0.2120, 0.2187],\n",
      "        [0.1331, 0.2525, 0.1961, 0.2055, 0.2129],\n",
      "        [0.1277, 0.2422, 0.1926, 0.2152, 0.2223],\n",
      "        [0.1373, 0.2464, 0.1965, 0.2065, 0.2133],\n",
      "        [0.1357, 0.2483, 0.1972, 0.2062, 0.2127],\n",
      "        [0.1350, 0.2398, 0.1939, 0.2121, 0.2192],\n",
      "        [0.1339, 0.2603, 0.1953, 0.2017, 0.2089]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "72/100 Running Loss: 1.5843418836593628\n",
      "tensor([[0.1294, 0.2417, 0.1946, 0.2136, 0.2208],\n",
      "        [0.1264, 0.2457, 0.1932, 0.2141, 0.2207],\n",
      "        [0.1306, 0.2553, 0.1948, 0.2068, 0.2125],\n",
      "        [0.1290, 0.2459, 0.1946, 0.2121, 0.2185],\n",
      "        [0.1316, 0.2545, 0.1962, 0.2053, 0.2124],\n",
      "        [0.1262, 0.2438, 0.1927, 0.2153, 0.2220],\n",
      "        [0.1360, 0.2481, 0.1967, 0.2063, 0.2128],\n",
      "        [0.1343, 0.2501, 0.1974, 0.2060, 0.2122],\n",
      "        [0.1336, 0.2413, 0.1940, 0.2122, 0.2189],\n",
      "        [0.1323, 0.2627, 0.1955, 0.2013, 0.2082]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "73/100 Running Loss: 1.5835051536560059\n",
      "tensor([[0.1279, 0.2432, 0.1947, 0.2136, 0.2205],\n",
      "        [0.1248, 0.2474, 0.1933, 0.2141, 0.2204],\n",
      "        [0.1290, 0.2574, 0.1950, 0.2066, 0.2119],\n",
      "        [0.1275, 0.2476, 0.1947, 0.2121, 0.2181],\n",
      "        [0.1300, 0.2567, 0.1965, 0.2050, 0.2118],\n",
      "        [0.1247, 0.2454, 0.1928, 0.2153, 0.2218],\n",
      "        [0.1347, 0.2500, 0.1969, 0.2061, 0.2123],\n",
      "        [0.1329, 0.2520, 0.1976, 0.2058, 0.2116],\n",
      "        [0.1322, 0.2428, 0.1941, 0.2122, 0.2187],\n",
      "        [0.1307, 0.2652, 0.1957, 0.2009, 0.2074]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "74/100 Running Loss: 1.5826390981674194\n",
      "tensor([[0.1264, 0.2449, 0.1949, 0.2137, 0.2202],\n",
      "        [0.1232, 0.2492, 0.1934, 0.2141, 0.2200],\n",
      "        [0.1274, 0.2596, 0.1953, 0.2063, 0.2113],\n",
      "        [0.1260, 0.2493, 0.1949, 0.2121, 0.2178],\n",
      "        [0.1285, 0.2590, 0.1967, 0.2047, 0.2111],\n",
      "        [0.1231, 0.2471, 0.1929, 0.2154, 0.2215],\n",
      "        [0.1333, 0.2518, 0.1972, 0.2059, 0.2118],\n",
      "        [0.1315, 0.2540, 0.1979, 0.2055, 0.2110],\n",
      "        [0.1308, 0.2444, 0.1943, 0.2122, 0.2184],\n",
      "        [0.1291, 0.2678, 0.1960, 0.2004, 0.2066]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "75/100 Running Loss: 1.5817433595657349\n",
      "tensor([[0.1249, 0.2465, 0.1950, 0.2137, 0.2198],\n",
      "        [0.1216, 0.2511, 0.1935, 0.2141, 0.2196],\n",
      "        [0.1258, 0.2620, 0.1955, 0.2060, 0.2107],\n",
      "        [0.1244, 0.2512, 0.1950, 0.2120, 0.2174],\n",
      "        [0.1268, 0.2614, 0.1969, 0.2044, 0.2104],\n",
      "        [0.1216, 0.2489, 0.1930, 0.2154, 0.2211],\n",
      "        [0.1318, 0.2538, 0.1975, 0.2056, 0.2112],\n",
      "        [0.1300, 0.2561, 0.1982, 0.2052, 0.2104],\n",
      "        [0.1293, 0.2460, 0.1944, 0.2122, 0.2180],\n",
      "        [0.1275, 0.2705, 0.1962, 0.1999, 0.2058]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "76/100 Running Loss: 1.5808171033859253\n",
      "tensor([[0.1234, 0.2483, 0.1952, 0.2137, 0.2194],\n",
      "        [0.1200, 0.2531, 0.1937, 0.2140, 0.2192],\n",
      "        [0.1241, 0.2644, 0.1957, 0.2057, 0.2100],\n",
      "        [0.1228, 0.2531, 0.1952, 0.2119, 0.2170],\n",
      "        [0.1252, 0.2639, 0.1972, 0.2040, 0.2097],\n",
      "        [0.1199, 0.2508, 0.1931, 0.2154, 0.2208],\n",
      "        [0.1304, 0.2559, 0.1977, 0.2053, 0.2106],\n",
      "        [0.1285, 0.2583, 0.1985, 0.2049, 0.2098],\n",
      "        [0.1278, 0.2477, 0.1945, 0.2122, 0.2177],\n",
      "        [0.1258, 0.2734, 0.1965, 0.1994, 0.2049]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "77/100 Running Loss: 1.5798571109771729\n",
      "tensor([[0.1218, 0.2501, 0.1953, 0.2137, 0.2190],\n",
      "        [0.1183, 0.2551, 0.1939, 0.2139, 0.2187],\n",
      "        [0.1224, 0.2670, 0.1960, 0.2053, 0.2093],\n",
      "        [0.1212, 0.2551, 0.1954, 0.2118, 0.2165],\n",
      "        [0.1235, 0.2665, 0.1975, 0.2036, 0.2089],\n",
      "        [0.1183, 0.2527, 0.1933, 0.2154, 0.2204],\n",
      "        [0.1289, 0.2581, 0.1980, 0.2050, 0.2100],\n",
      "        [0.1270, 0.2606, 0.1988, 0.2045, 0.2091],\n",
      "        [0.1263, 0.2495, 0.1947, 0.2121, 0.2173],\n",
      "        [0.1241, 0.2763, 0.1968, 0.1987, 0.2040]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "78/100 Running Loss: 1.5788657665252686\n",
      "tensor([[0.1202, 0.2520, 0.1956, 0.2136, 0.2186],\n",
      "        [0.1166, 0.2573, 0.1941, 0.2137, 0.2182],\n",
      "        [0.1207, 0.2696, 0.1963, 0.2049, 0.2085],\n",
      "        [0.1195, 0.2572, 0.1957, 0.2116, 0.2160],\n",
      "        [0.1218, 0.2692, 0.1978, 0.2031, 0.2081],\n",
      "        [0.1166, 0.2548, 0.1934, 0.2153, 0.2199],\n",
      "        [0.1274, 0.2603, 0.1983, 0.2047, 0.2093],\n",
      "        [0.1254, 0.2630, 0.1991, 0.2041, 0.2084],\n",
      "        [0.1247, 0.2514, 0.1949, 0.2120, 0.2169],\n",
      "        [0.1224, 0.2795, 0.1971, 0.1981, 0.2030]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "79/100 Running Loss: 1.577839970588684\n",
      "tensor([[0.1186, 0.2539, 0.1958, 0.2135, 0.2182],\n",
      "        [0.1149, 0.2595, 0.1943, 0.2135, 0.2177],\n",
      "        [0.1190, 0.2724, 0.1966, 0.2044, 0.2077],\n",
      "        [0.1178, 0.2594, 0.1959, 0.2114, 0.2155],\n",
      "        [0.1201, 0.2721, 0.1981, 0.2026, 0.2072],\n",
      "        [0.1148, 0.2569, 0.1936, 0.2152, 0.2195],\n",
      "        [0.1258, 0.2627, 0.1987, 0.2042, 0.2086],\n",
      "        [0.1238, 0.2655, 0.1995, 0.2036, 0.2076],\n",
      "        [0.1231, 0.2533, 0.1951, 0.2119, 0.2165],\n",
      "        [0.1207, 0.2827, 0.1974, 0.1974, 0.2019]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "80/100 Running Loss: 1.5767791271209717\n",
      "tensor([[0.1169, 0.2560, 0.1960, 0.2134, 0.2177],\n",
      "        [0.1131, 0.2619, 0.1945, 0.2133, 0.2172],\n",
      "        [0.1172, 0.2753, 0.1969, 0.2038, 0.2068],\n",
      "        [0.1161, 0.2616, 0.1961, 0.2112, 0.2150],\n",
      "        [0.1183, 0.2750, 0.1984, 0.2020, 0.2063],\n",
      "        [0.1131, 0.2591, 0.1937, 0.2151, 0.2190],\n",
      "        [0.1242, 0.2652, 0.1990, 0.2038, 0.2078],\n",
      "        [0.1222, 0.2681, 0.1998, 0.2031, 0.2068],\n",
      "        [0.1215, 0.2554, 0.1953, 0.2118, 0.2161],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [0.1189, 0.2861, 0.1976, 0.1966, 0.2007]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "81/100 Running Loss: 1.575679063796997\n",
      "tensor([[0.1153, 0.2580, 0.1962, 0.2133, 0.2172],\n",
      "        [0.1114, 0.2643, 0.1947, 0.2130, 0.2166],\n",
      "        [0.1154, 0.2782, 0.1972, 0.2032, 0.2059],\n",
      "        [0.1144, 0.2640, 0.1964, 0.2109, 0.2144],\n",
      "        [0.1165, 0.2781, 0.1987, 0.2013, 0.2053],\n",
      "        [0.1113, 0.2614, 0.1939, 0.2150, 0.2184],\n",
      "        [0.1226, 0.2677, 0.1993, 0.2033, 0.2071],\n",
      "        [0.1205, 0.2708, 0.2002, 0.2026, 0.2059],\n",
      "        [0.1199, 0.2575, 0.1956, 0.2116, 0.2156],\n",
      "        [0.1171, 0.2897, 0.1979, 0.1958, 0.1995]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "82/100 Running Loss: 1.574539303779602\n",
      "tensor([[0.1136, 0.2602, 0.1964, 0.2132, 0.2166],\n",
      "        [0.1096, 0.2669, 0.1949, 0.2128, 0.2159],\n",
      "        [0.1136, 0.2814, 0.1975, 0.2027, 0.2049],\n",
      "        [0.1126, 0.2664, 0.1966, 0.2106, 0.2137],\n",
      "        [0.1147, 0.2814, 0.1990, 0.2007, 0.2042],\n",
      "        [0.1096, 0.2638, 0.1941, 0.2148, 0.2178],\n",
      "        [0.1210, 0.2704, 0.1997, 0.2028, 0.2062],\n",
      "        [0.1188, 0.2736, 0.2006, 0.2020, 0.2050],\n",
      "        [0.1182, 0.2596, 0.1958, 0.2113, 0.2150],\n",
      "        [0.1152, 0.2934, 0.1982, 0.1950, 0.1982]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "83/100 Running Loss: 1.573362946510315\n",
      "tensor([[0.1119, 0.2625, 0.1966, 0.2130, 0.2161],\n",
      "        [0.1077, 0.2695, 0.1951, 0.2125, 0.2152],\n",
      "        [0.1117, 0.2846, 0.1978, 0.2020, 0.2038],\n",
      "        [0.1108, 0.2689, 0.1968, 0.2104, 0.2130],\n",
      "        [0.1128, 0.2848, 0.1992, 0.2001, 0.2031],\n",
      "        [0.1078, 0.2662, 0.1942, 0.2146, 0.2172],\n",
      "        [0.1193, 0.2731, 0.2000, 0.2022, 0.2054],\n",
      "        [0.1171, 0.2765, 0.2009, 0.2015, 0.2040],\n",
      "        [0.1165, 0.2618, 0.1960, 0.2111, 0.2145],\n",
      "        [0.1133, 0.2973, 0.1984, 0.1941, 0.1969]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "84/100 Running Loss: 1.5721462965011597\n",
      "tensor([[0.1101, 0.2648, 0.1968, 0.2128, 0.2155],\n",
      "        [0.1059, 0.2722, 0.1953, 0.2122, 0.2144],\n",
      "        [0.1099, 0.2880, 0.1981, 0.2013, 0.2027],\n",
      "        [0.1090, 0.2715, 0.1971, 0.2101, 0.2123],\n",
      "        [0.1109, 0.2883, 0.1995, 0.1994, 0.2019],\n",
      "        [0.1059, 0.2688, 0.1944, 0.2144, 0.2165],\n",
      "        [0.1176, 0.2760, 0.2004, 0.2016, 0.2044],\n",
      "        [0.1154, 0.2795, 0.2013, 0.2009, 0.2030],\n",
      "        [0.1148, 0.2642, 0.1963, 0.2108, 0.2139],\n",
      "        [0.1114, 0.3013, 0.1987, 0.1931, 0.1954]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "85/100 Running Loss: 1.5708897113800049\n",
      "tensor([[0.1084, 0.2672, 0.1970, 0.2126, 0.2148],\n",
      "        [0.1040, 0.2750, 0.1955, 0.2119, 0.2136],\n",
      "        [0.1080, 0.2915, 0.1985, 0.2006, 0.2014],\n",
      "        [0.1072, 0.2743, 0.1973, 0.2097, 0.2115],\n",
      "        [0.1090, 0.2919, 0.1998, 0.1986, 0.2007],\n",
      "        [0.1041, 0.2714, 0.1946, 0.2141, 0.2158],\n",
      "        [0.1159, 0.2790, 0.2007, 0.2010, 0.2034],\n",
      "        [0.1136, 0.2827, 0.2016, 0.2002, 0.2019],\n",
      "        [0.1131, 0.2666, 0.1965, 0.2106, 0.2132],\n",
      "        [0.1095, 0.3055, 0.1990, 0.1920, 0.1939]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "86/100 Running Loss: 1.5695905685424805\n",
      "tensor([[0.1066, 0.2696, 0.1972, 0.2124, 0.2141],\n",
      "        [0.1021, 0.2779, 0.1957, 0.2115, 0.2128],\n",
      "        [0.1060, 0.2951, 0.1988, 0.1998, 0.2002],\n",
      "        [0.1054, 0.2770, 0.1976, 0.2094, 0.2107],\n",
      "        [0.1071, 0.2957, 0.2000, 0.1978, 0.1994],\n",
      "        [0.1022, 0.2740, 0.1947, 0.2139, 0.2151],\n",
      "        [0.1142, 0.2821, 0.2010, 0.2004, 0.2023],\n",
      "        [0.1118, 0.2860, 0.2020, 0.1995, 0.2007],\n",
      "        [0.1114, 0.2691, 0.1968, 0.2103, 0.2125],\n",
      "        [0.1075, 0.3099, 0.1993, 0.1908, 0.1924]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "87/100 Running Loss: 1.5682483911514282\n",
      "tensor([[0.1048, 0.2722, 0.1974, 0.2122, 0.2134],\n",
      "        [0.1003, 0.2808, 0.1960, 0.2111, 0.2118],\n",
      "        [0.1041, 0.2989, 0.1992, 0.1990, 0.1988],\n",
      "        [0.1035, 0.2799, 0.1979, 0.2090, 0.2098],\n",
      "        [0.1051, 0.2996, 0.2003, 0.1969, 0.1980],\n",
      "        [0.1004, 0.2768, 0.1949, 0.2136, 0.2143],\n",
      "        [0.1124, 0.2853, 0.2014, 0.1998, 0.2012],\n",
      "        [0.1100, 0.2894, 0.2023, 0.1988, 0.1995],\n",
      "        [0.1097, 0.2717, 0.1970, 0.2100, 0.2117],\n",
      "        [0.1056, 0.3145, 0.1996, 0.1896, 0.1908]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "88/100 Running Loss: 1.566860556602478\n",
      "tensor([[0.1030, 0.2748, 0.1977, 0.2119, 0.2127],\n",
      "        [0.0983, 0.2839, 0.1962, 0.2107, 0.2109],\n",
      "        [0.1021, 0.3029, 0.1995, 0.1980, 0.1974],\n",
      "        [0.1017, 0.2828, 0.1982, 0.2085, 0.2089],\n",
      "        [0.1031, 0.3037, 0.2005, 0.1960, 0.1966],\n",
      "        [0.0985, 0.2796, 0.1951, 0.2132, 0.2135],\n",
      "        [0.1106, 0.2886, 0.2017, 0.1990, 0.2001],\n",
      "        [0.1081, 0.2929, 0.2027, 0.1980, 0.1983],\n",
      "        [0.1079, 0.2744, 0.1972, 0.2096, 0.2109],\n",
      "        [0.1036, 0.3194, 0.1998, 0.1882, 0.1891]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "89/100 Running Loss: 1.5654295682907104\n",
      "tensor([[0.1011, 0.2774, 0.1979, 0.2116, 0.2119],\n",
      "        [0.0964, 0.2871, 0.1964, 0.2102, 0.2099],\n",
      "        [0.1001, 0.3071, 0.1998, 0.1970, 0.1960],\n",
      "        [0.0998, 0.2859, 0.1984, 0.2080, 0.2079],\n",
      "        [0.1011, 0.3080, 0.2008, 0.1949, 0.1952],\n",
      "        [0.0966, 0.2825, 0.1953, 0.2129, 0.2126],\n",
      "        [0.1088, 0.2920, 0.2020, 0.1982, 0.1989],\n",
      "        [0.1063, 0.2966, 0.2030, 0.1971, 0.1970],\n",
      "        [0.1061, 0.2771, 0.1975, 0.2093, 0.2101],\n",
      "        [0.1015, 0.3244, 0.2000, 0.1868, 0.1873]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "90/100 Running Loss: 1.5639512538909912\n",
      "tensor([[0.0993, 0.2802, 0.1981, 0.2113, 0.2111],\n",
      "        [0.0945, 0.2904, 0.1966, 0.2097, 0.2089],\n",
      "        [0.0981, 0.3114, 0.2001, 0.1959, 0.1945],\n",
      "        [0.0979, 0.2890, 0.1987, 0.2075, 0.2069],\n",
      "        [0.0991, 0.3123, 0.2010, 0.1939, 0.1937],\n",
      "        [0.0947, 0.2855, 0.1955, 0.2125, 0.2117],\n",
      "        [0.1070, 0.2956, 0.2023, 0.1974, 0.1977],\n",
      "        [0.1044, 0.3004, 0.2033, 0.1962, 0.1957],\n",
      "        [0.1043, 0.2800, 0.1977, 0.2089, 0.2092],\n",
      "        [0.0994, 0.3297, 0.2002, 0.1853, 0.1855]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "91/100 Running Loss: 1.5624268054962158\n",
      "tensor([[0.0975, 0.2830, 0.1983, 0.2109, 0.2103],\n",
      "        [0.0925, 0.2937, 0.1968, 0.2091, 0.2078],\n",
      "        [0.0961, 0.3160, 0.2003, 0.1947, 0.1929],\n",
      "        [0.0960, 0.2922, 0.1989, 0.2070, 0.2058],\n",
      "        [0.0970, 0.3169, 0.2012, 0.1927, 0.1922],\n",
      "        [0.0928, 0.2886, 0.1957, 0.2121, 0.2108],\n",
      "        [0.1051, 0.2994, 0.2026, 0.1965, 0.1964],\n",
      "        [0.1025, 0.3043, 0.2036, 0.1952, 0.1944],\n",
      "        [0.1024, 0.2829, 0.1980, 0.2084, 0.2083],\n",
      "        [0.0973, 0.3352, 0.2003, 0.1837, 0.1835]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "92/100 Running Loss: 1.5608506202697754\n",
      "tensor([[0.0956, 0.2859, 0.1985, 0.2105, 0.2094],\n",
      "        [0.0905, 0.2972, 0.1970, 0.2086, 0.2067],\n",
      "        [0.0940, 0.3207, 0.2005, 0.1934, 0.1913],\n",
      "        [0.0940, 0.2956, 0.1992, 0.2064, 0.2048],\n",
      "        [0.0949, 0.3217, 0.2015, 0.1914, 0.1905],\n",
      "        [0.0909, 0.2918, 0.1959, 0.2116, 0.2098],\n",
      "        [0.1032, 0.3032, 0.2029, 0.1955, 0.1951],\n",
      "        [0.1005, 0.3084, 0.2040, 0.1942, 0.1929],\n",
      "        [0.1006, 0.2860, 0.1982, 0.2080, 0.2073],\n",
      "        [0.0951, 0.3410, 0.2004, 0.1820, 0.1815]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "93/100 Running Loss: 1.5592234134674072\n",
      "tensor([[0.0937, 0.2889, 0.1987, 0.2101, 0.2085],\n",
      "        [0.0886, 0.3008, 0.1972, 0.2079, 0.2055],\n",
      "        [0.0920, 0.3256, 0.2008, 0.1921, 0.1896],\n",
      "        [0.0921, 0.2990, 0.1994, 0.2058, 0.2036],\n",
      "        [0.0928, 0.3267, 0.2017, 0.1901, 0.1888],\n",
      "        [0.0889, 0.2951, 0.1960, 0.2111, 0.2088],\n",
      "        [0.1013, 0.3073, 0.2032, 0.1946, 0.1937],\n",
      "        [0.0985, 0.3127, 0.2043, 0.1931, 0.1914],\n",
      "        [0.0987, 0.2891, 0.1984, 0.2075, 0.2063],\n",
      "        [0.0929, 0.3470, 0.2005, 0.1803, 0.1793]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "94/100 Running Loss: 1.5575426816940308\n",
      "tensor([[0.0919, 0.2920, 0.1990, 0.2096, 0.2075],\n",
      "        [0.0866, 0.3045, 0.1973, 0.2073, 0.2043],\n",
      "        [0.0899, 0.3307, 0.2009, 0.1907, 0.1878],\n",
      "        [0.0901, 0.3026, 0.1996, 0.2052, 0.2024],\n",
      "        [0.0907, 0.3318, 0.2018, 0.1887, 0.1869],\n",
      "        [0.0870, 0.2985, 0.1962, 0.2106, 0.2077],\n",
      "        [0.0993, 0.3114, 0.2035, 0.1935, 0.1922],\n",
      "        [0.0966, 0.3171, 0.2045, 0.1919, 0.1898],\n",
      "        [0.0968, 0.2924, 0.1987, 0.2069, 0.2052],\n",
      "        [0.0907, 0.3532, 0.2005, 0.1785, 0.1771]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "95/100 Running Loss: 1.55580735206604\n",
      "tensor([[0.0900, 0.2952, 0.1992, 0.2091, 0.2065],\n",
      "        [0.0846, 0.3083, 0.1975, 0.2066, 0.2030],\n",
      "        [0.0878, 0.3359, 0.2011, 0.1893, 0.1859],\n",
      "        [0.0882, 0.3064, 0.1998, 0.2045, 0.2012],\n",
      "        [0.0886, 0.3371, 0.2020, 0.1873, 0.1851],\n",
      "        [0.0850, 0.3019, 0.1963, 0.2101, 0.2066],\n",
      "        [0.0974, 0.3158, 0.2038, 0.1924, 0.1907],\n",
      "        [0.0946, 0.3217, 0.2048, 0.1907, 0.1882],\n",
      "        [0.0949, 0.2958, 0.1989, 0.2063, 0.2041],\n",
      "        [0.0884, 0.3597, 0.2005, 0.1765, 0.1748]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "96/100 Running Loss: 1.5540136098861694\n",
      "tensor([[0.0881, 0.2985, 0.1993, 0.2086, 0.2055],\n",
      "        [0.0826, 0.3122, 0.1976, 0.2059, 0.2017],\n",
      "        [0.0856, 0.3413, 0.2013, 0.1878, 0.1840],\n",
      "        [0.0862, 0.3102, 0.2000, 0.2037, 0.1999],\n",
      "        [0.0864, 0.3426, 0.2021, 0.1858, 0.1831],\n",
      "        [0.0831, 0.3054, 0.1964, 0.2095, 0.2055],\n",
      "        [0.0954, 0.3203, 0.2041, 0.1911, 0.1891],\n",
      "        [0.0926, 0.3264, 0.2051, 0.1894, 0.1865],\n",
      "        [0.0930, 0.2992, 0.1991, 0.2057, 0.2030],\n",
      "        [0.0861, 0.3665, 0.2005, 0.1745, 0.1724]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "97/100 Running Loss: 1.5521644353866577\n",
      "tensor([[0.0862, 0.3018, 0.1995, 0.2080, 0.2045],\n",
      "        [0.0806, 0.3161, 0.1976, 0.2053, 0.2004],\n",
      "        [0.0835, 0.3468, 0.2014, 0.1864, 0.1820],\n",
      "        [0.0843, 0.3141, 0.2002, 0.2030, 0.1985],\n",
      "        [0.0842, 0.3482, 0.2022, 0.1843, 0.1810],\n",
      "        [0.0811, 0.3090, 0.1965, 0.2090, 0.2043],\n",
      "        [0.0934, 0.3250, 0.2043, 0.1899, 0.1874],\n",
      "        [0.0906, 0.3314, 0.2054, 0.1880, 0.1847],\n",
      "        [0.0911, 0.3027, 0.1993, 0.2050, 0.2019],\n",
      "        [0.0838, 0.3734, 0.2004, 0.1724, 0.1700]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "98/100 Running Loss: 1.5502578020095825\n",
      "tensor([[0.0843, 0.3053, 0.1996, 0.2074, 0.2034],\n",
      "        [0.0786, 0.3201, 0.1977, 0.2046, 0.1990],\n",
      "        [0.0813, 0.3525, 0.2014, 0.1848, 0.1799],\n",
      "        [0.0823, 0.3180, 0.2003, 0.2022, 0.1972],\n",
      "        [0.0820, 0.3541, 0.2022, 0.1828, 0.1789],\n",
      "        [0.0792, 0.3127, 0.1966, 0.2085, 0.2031],\n",
      "        [0.0914, 0.3299, 0.2045, 0.1885, 0.1857],\n",
      "        [0.0885, 0.3365, 0.2055, 0.1865, 0.1829],\n",
      "        [0.0892, 0.3063, 0.1994, 0.2044, 0.2007],\n",
      "        [0.0814, 0.3806, 0.2003, 0.1702, 0.1674]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "99/100 Running Loss: 1.5482896566390991\n",
      "tensor([[0.0824, 0.3088, 0.1997, 0.2069, 0.2022],\n",
      "        [0.0766, 0.3243, 0.1977, 0.2038, 0.1976],\n",
      "        [0.0791, 0.3585, 0.2014, 0.1832, 0.1778],\n",
      "        [0.0803, 0.3221, 0.2004, 0.2015, 0.1958],\n",
      "        [0.0798, 0.3601, 0.2022, 0.1811, 0.1768],\n",
      "        [0.0772, 0.3165, 0.1965, 0.2078, 0.2018],\n",
      "        [0.0894, 0.3349, 0.2046, 0.1871, 0.1839],\n",
      "        [0.0865, 0.3418, 0.2057, 0.1850, 0.1810],\n",
      "        [0.0872, 0.3100, 0.1996, 0.2038, 0.1994],\n",
      "        [0.0791, 0.3881, 0.2000, 0.1680, 0.1648]], grad_fn=<SliceBackward>) tensor([3, 3, 2, 3, 2, 3, 2, 2, 3, 1])\n",
      "100/100 Running Loss: 1.546258568763733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.39      1.00      0.56        21\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.00      0.00      0.00        11\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "   micro avg       0.39      0.39      0.39        54\n",
      "   macro avg       0.10      0.25      0.14        54\n",
      "weighted avg       0.15      0.39      0.22        54\n",
      "\n",
      "0.14\n",
      "tensor([[0.1811, 0.2173, 0.2034, 0.2115, 0.1867],\n",
      "        [0.1828, 0.2171, 0.2037, 0.2112, 0.1854],\n",
      "        [0.1828, 0.2171, 0.2037, 0.2111, 0.1853],\n",
      "        [0.1823, 0.2170, 0.2024, 0.2126, 0.1856],\n",
      "        [0.1825, 0.2175, 0.2025, 0.2112, 0.1863],\n",
      "        [0.1818, 0.2174, 0.2032, 0.2109, 0.1866],\n",
      "        [0.1816, 0.2174, 0.2025, 0.2120, 0.1864],\n",
      "        [0.1811, 0.2177, 0.2034, 0.2110, 0.1868],\n",
      "        [0.1825, 0.2173, 0.2026, 0.2118, 0.1858],\n",
      "        [0.1829, 0.2169, 0.2025, 0.2125, 0.1851]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "1/100 Running Loss: 1.6015799045562744\n",
      "tensor([[0.1806, 0.2179, 0.2035, 0.2113, 0.1866],\n",
      "        [0.1822, 0.2176, 0.2038, 0.2111, 0.1853],\n",
      "        [0.1822, 0.2177, 0.2038, 0.2110, 0.1853],\n",
      "        [0.1818, 0.2176, 0.2026, 0.2125, 0.1855],\n",
      "        [0.1820, 0.2181, 0.2027, 0.2110, 0.1862],\n",
      "        [0.1813, 0.2181, 0.2034, 0.2107, 0.1865],\n",
      "        [0.1811, 0.2181, 0.2027, 0.2118, 0.1863],\n",
      "        [0.1806, 0.2184, 0.2035, 0.2108, 0.1867],\n",
      "        [0.1821, 0.2178, 0.2028, 0.2117, 0.1857],\n",
      "        [0.1824, 0.2174, 0.2028, 0.2124, 0.1851]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "2/100 Running Loss: 1.6013176441192627\n",
      "tensor([[0.1801, 0.2186, 0.2037, 0.2111, 0.1865],\n",
      "        [0.1817, 0.2181, 0.2040, 0.2110, 0.1852],\n",
      "        [0.1817, 0.2182, 0.2040, 0.2109, 0.1852],\n",
      "        [0.1813, 0.2182, 0.2028, 0.2123, 0.1855],\n",
      "        [0.1815, 0.2187, 0.2029, 0.2108, 0.1861],\n",
      "        [0.1808, 0.2188, 0.2035, 0.2106, 0.1863],\n",
      "        [0.1806, 0.2187, 0.2029, 0.2115, 0.1863],\n",
      "        [0.1801, 0.2191, 0.2036, 0.2106, 0.1866],\n",
      "        [0.1816, 0.2183, 0.2030, 0.2115, 0.1856],\n",
      "        [0.1819, 0.2179, 0.2031, 0.2122, 0.1850]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "3/100 Running Loss: 1.601056456565857\n",
      "tensor([[0.1796, 0.2192, 0.2039, 0.2109, 0.1864],\n",
      "        [0.1811, 0.2187, 0.2041, 0.2109, 0.1851],\n",
      "        [0.1812, 0.2188, 0.2042, 0.2108, 0.1851],\n",
      "        [0.1808, 0.2188, 0.2030, 0.2121, 0.1854],\n",
      "        [0.1810, 0.2194, 0.2031, 0.2106, 0.1860],\n",
      "        [0.1803, 0.2194, 0.2036, 0.2104, 0.1862],\n",
      "        [0.1801, 0.2194, 0.2031, 0.2113, 0.1862],\n",
      "        [0.1796, 0.2198, 0.2037, 0.2105, 0.1864],\n",
      "        [0.1811, 0.2188, 0.2031, 0.2114, 0.1855],\n",
      "        [0.1814, 0.2184, 0.2033, 0.2120, 0.1849]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "4/100 Running Loss: 1.6007962226867676\n",
      "tensor([[0.1791, 0.2199, 0.2040, 0.2107, 0.1863],\n",
      "        [0.1806, 0.2192, 0.2043, 0.2108, 0.1850],\n",
      "        [0.1806, 0.2193, 0.2044, 0.2107, 0.1850],\n",
      "        [0.1802, 0.2193, 0.2033, 0.2119, 0.1853],\n",
      "        [0.1805, 0.2200, 0.2033, 0.2104, 0.1858],\n",
      "        [0.1798, 0.2201, 0.2038, 0.2102, 0.1861],\n",
      "        [0.1796, 0.2200, 0.2032, 0.2111, 0.1861],\n",
      "        [0.1792, 0.2204, 0.2039, 0.2103, 0.1863],\n",
      "        [0.1807, 0.2194, 0.2033, 0.2112, 0.1854],\n",
      "        [0.1809, 0.2189, 0.2036, 0.2118, 0.1848]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "5/100 Running Loss: 1.600535273551941\n",
      "tensor([[0.1786, 0.2206, 0.2042, 0.2105, 0.1862],\n",
      "        [0.1801, 0.2198, 0.2045, 0.2107, 0.1849],\n",
      "        [0.1801, 0.2199, 0.2046, 0.2106, 0.1849],\n",
      "        [0.1797, 0.2199, 0.2035, 0.2117, 0.1852],\n",
      "        [0.1800, 0.2206, 0.2035, 0.2102, 0.1857],\n",
      "        [0.1793, 0.2208, 0.2039, 0.2100, 0.1859],\n",
      "        [0.1791, 0.2207, 0.2034, 0.2109, 0.1860],\n",
      "        [0.1787, 0.2210, 0.2040, 0.2101, 0.1861],\n",
      "        [0.1802, 0.2199, 0.2035, 0.2111, 0.1853],\n",
      "        [0.1804, 0.2193, 0.2039, 0.2116, 0.1848]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "6/100 Running Loss: 1.600273847579956\n",
      "tensor([[0.1781, 0.2212, 0.2043, 0.2103, 0.1861],\n",
      "        [0.1795, 0.2204, 0.2047, 0.2106, 0.1848],\n",
      "        [0.1795, 0.2204, 0.2048, 0.2105, 0.1848],\n",
      "        [0.1792, 0.2205, 0.2037, 0.2115, 0.1851],\n",
      "        [0.1796, 0.2212, 0.2037, 0.2100, 0.1855],\n",
      "        [0.1788, 0.2215, 0.2041, 0.2098, 0.1858],\n",
      "        [0.1786, 0.2213, 0.2036, 0.2106, 0.1859],\n",
      "        [0.1783, 0.2217, 0.2041, 0.2100, 0.1860],\n",
      "        [0.1797, 0.2204, 0.2037, 0.2109, 0.1852],\n",
      "        [0.1799, 0.2198, 0.2041, 0.2114, 0.1847]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "7/100 Running Loss: 1.6000131368637085\n",
      "tensor([[0.1776, 0.2218, 0.2045, 0.2101, 0.1859],\n",
      "        [0.1790, 0.2209, 0.2049, 0.2104, 0.1847],\n",
      "        [0.1790, 0.2210, 0.2050, 0.2103, 0.1847],\n",
      "        [0.1787, 0.2211, 0.2040, 0.2113, 0.1850],\n",
      "        [0.1791, 0.2218, 0.2039, 0.2098, 0.1854],\n",
      "        [0.1783, 0.2221, 0.2043, 0.2097, 0.1856],\n",
      "        [0.1781, 0.2219, 0.2038, 0.2103, 0.1858],\n",
      "        [0.1778, 0.2223, 0.2043, 0.2098, 0.1858],\n",
      "        [0.1792, 0.2209, 0.2039, 0.2108, 0.1851],\n",
      "        [0.1794, 0.2203, 0.2044, 0.2112, 0.1846]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "8/100 Running Loss: 1.5997521877288818\n",
      "tensor([[0.1771, 0.2225, 0.2047, 0.2099, 0.1858],\n",
      "        [0.1785, 0.2215, 0.2051, 0.2103, 0.1846],\n",
      "        [0.1785, 0.2216, 0.2052, 0.2102, 0.1845],\n",
      "        [0.1781, 0.2216, 0.2042, 0.2111, 0.1849],\n",
      "        [0.1786, 0.2225, 0.2041, 0.2096, 0.1853],\n",
      "        [0.1778, 0.2228, 0.2045, 0.2095, 0.1855],\n",
      "        [0.1776, 0.2226, 0.2041, 0.2101, 0.1857],\n",
      "        [0.1774, 0.2229, 0.2044, 0.2097, 0.1856],\n",
      "        [0.1788, 0.2215, 0.2041, 0.2106, 0.1850],\n",
      "        [0.1789, 0.2208, 0.2047, 0.2110, 0.1845]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "9/100 Running Loss: 1.5994939804077148\n",
      "tensor([[0.1766, 0.2231, 0.2049, 0.2097, 0.1856],\n",
      "        [0.1779, 0.2221, 0.2053, 0.2102, 0.1845],\n",
      "        [0.1779, 0.2222, 0.2054, 0.2101, 0.1844],\n",
      "        [0.1776, 0.2222, 0.2045, 0.2109, 0.1848],\n",
      "        [0.1781, 0.2231, 0.2043, 0.2094, 0.1851],\n",
      "        [0.1773, 0.2235, 0.2046, 0.2093, 0.1853],\n",
      "        [0.1771, 0.2232, 0.2043, 0.2098, 0.1856],\n",
      "        [0.1770, 0.2235, 0.2046, 0.2095, 0.1854],\n",
      "        [0.1783, 0.2220, 0.2043, 0.2105, 0.1849],\n",
      "        [0.1784, 0.2213, 0.2049, 0.2108, 0.1845]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "10/100 Running Loss: 1.599236011505127\n",
      "tensor([[0.1761, 0.2238, 0.2051, 0.2095, 0.1855],\n",
      "        [0.1774, 0.2227, 0.2055, 0.2100, 0.1844],\n",
      "        [0.1774, 0.2228, 0.2056, 0.2099, 0.1843],\n",
      "        [0.1771, 0.2228, 0.2048, 0.2107, 0.1847],\n",
      "        [0.1776, 0.2237, 0.2045, 0.2092, 0.1850],\n",
      "        [0.1768, 0.2241, 0.2048, 0.2091, 0.1851],\n",
      "        [0.1766, 0.2239, 0.2045, 0.2095, 0.1854],\n",
      "        [0.1766, 0.2242, 0.2047, 0.2093, 0.1852],\n",
      "        [0.1778, 0.2226, 0.2045, 0.2103, 0.1848],\n",
      "        [0.1779, 0.2218, 0.2052, 0.2107, 0.1844]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "11/100 Running Loss: 1.5989772081375122\n",
      "tensor([[0.1756, 0.2245, 0.2053, 0.2093, 0.1853],\n",
      "        [0.1769, 0.2233, 0.2057, 0.2099, 0.1842],\n",
      "        [0.1768, 0.2234, 0.2058, 0.2098, 0.1842],\n",
      "        [0.1766, 0.2234, 0.2050, 0.2105, 0.1845],\n",
      "        [0.1771, 0.2243, 0.2047, 0.2090, 0.1848],\n",
      "        [0.1764, 0.2248, 0.2050, 0.2088, 0.1849],\n",
      "        [0.1761, 0.2246, 0.2048, 0.2093, 0.1853],\n",
      "        [0.1761, 0.2248, 0.2049, 0.2091, 0.1851],\n",
      "        [0.1774, 0.2231, 0.2047, 0.2101, 0.1846],\n",
      "        [0.1774, 0.2223, 0.2055, 0.2105, 0.1843]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "12/100 Running Loss: 1.598718523979187\n",
      "tensor([[0.1752, 0.2251, 0.2055, 0.2091, 0.1852],\n",
      "        [0.1763, 0.2238, 0.2059, 0.2098, 0.1841],\n",
      "        [0.1763, 0.2240, 0.2060, 0.2097, 0.1841],\n",
      "        [0.1761, 0.2239, 0.2053, 0.2103, 0.1844],\n",
      "        [0.1767, 0.2249, 0.2049, 0.2088, 0.1846],\n",
      "        [0.1759, 0.2255, 0.2052, 0.2086, 0.1848],\n",
      "        [0.1756, 0.2252, 0.2050, 0.2090, 0.1852],\n",
      "        [0.1757, 0.2254, 0.2051, 0.2090, 0.1849],\n",
      "        [0.1769, 0.2237, 0.2049, 0.2100, 0.1845],\n",
      "        [0.1769, 0.2228, 0.2057, 0.2103, 0.1842]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "13/100 Running Loss: 1.598459243774414\n",
      "tensor([[0.1747, 0.2258, 0.2057, 0.2089, 0.1850],\n",
      "        [0.1758, 0.2244, 0.2062, 0.2097, 0.1840],\n",
      "        [0.1757, 0.2246, 0.2062, 0.2095, 0.1840],\n",
      "        [0.1756, 0.2245, 0.2056, 0.2101, 0.1843],\n",
      "        [0.1762, 0.2256, 0.2052, 0.2087, 0.1844],\n",
      "        [0.1754, 0.2261, 0.2054, 0.2085, 0.1846],\n",
      "        [0.1751, 0.2259, 0.2052, 0.2088, 0.1850],\n",
      "        [0.1752, 0.2261, 0.2053, 0.2088, 0.1847],\n",
      "        [0.1764, 0.2242, 0.2052, 0.2098, 0.1844],\n",
      "        [0.1764, 0.2233, 0.2060, 0.2102, 0.1841]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "14/100 Running Loss: 1.5981978178024292\n",
      "tensor([[0.1742, 0.2264, 0.2059, 0.2087, 0.1848],\n",
      "        [0.1752, 0.2250, 0.2064, 0.2095, 0.1838],\n",
      "        [0.1752, 0.2251, 0.2064, 0.2094, 0.1838],\n",
      "        [0.1750, 0.2251, 0.2058, 0.2099, 0.1841],\n",
      "        [0.1757, 0.2262, 0.2054, 0.2085, 0.1843],\n",
      "        [0.1749, 0.2268, 0.2056, 0.2083, 0.1844],\n",
      "        [0.1746, 0.2265, 0.2054, 0.2086, 0.1849],\n",
      "        [0.1747, 0.2268, 0.2055, 0.2086, 0.1845],\n",
      "        [0.1760, 0.2247, 0.2054, 0.2097, 0.1843],\n",
      "        [0.1759, 0.2238, 0.2062, 0.2100, 0.1840]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "15/100 Running Loss: 1.5979341268539429\n",
      "tensor([[0.1737, 0.2271, 0.2061, 0.2085, 0.1846],\n",
      "        [0.1747, 0.2256, 0.2066, 0.2094, 0.1837],\n",
      "        [0.1747, 0.2257, 0.2067, 0.2093, 0.1837],\n",
      "        [0.1745, 0.2257, 0.2061, 0.2096, 0.1840],\n",
      "        [0.1753, 0.2268, 0.2056, 0.2083, 0.1841],\n",
      "        [0.1745, 0.2275, 0.2058, 0.2081, 0.1842],\n",
      "        [0.1741, 0.2272, 0.2056, 0.2084, 0.1847],\n",
      "        [0.1742, 0.2274, 0.2057, 0.2083, 0.1843],\n",
      "        [0.1755, 0.2253, 0.2056, 0.2095, 0.1841],\n",
      "        [0.1754, 0.2244, 0.2065, 0.2098, 0.1839]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "16/100 Running Loss: 1.597670316696167\n",
      "tensor([[0.1732, 0.2278, 0.2063, 0.2083, 0.1844],\n",
      "        [0.1742, 0.2262, 0.2069, 0.2092, 0.1836],\n",
      "        [0.1741, 0.2263, 0.2069, 0.2091, 0.1835],\n",
      "        [0.1740, 0.2263, 0.2064, 0.2094, 0.1838],\n",
      "        [0.1748, 0.2274, 0.2058, 0.2081, 0.1839],\n",
      "        [0.1740, 0.2282, 0.2060, 0.2078, 0.1840],\n",
      "        [0.1736, 0.2278, 0.2058, 0.2081, 0.1845],\n",
      "        [0.1737, 0.2281, 0.2059, 0.2081, 0.1841],\n",
      "        [0.1751, 0.2258, 0.2058, 0.2094, 0.1840],\n",
      "        [0.1748, 0.2249, 0.2068, 0.2097, 0.1838]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "17/100 Running Loss: 1.5974043607711792\n",
      "tensor([[0.1727, 0.2285, 0.2065, 0.2081, 0.1842],\n",
      "        [0.1736, 0.2268, 0.2071, 0.2091, 0.1834],\n",
      "        [0.1736, 0.2269, 0.2071, 0.2090, 0.1834],\n",
      "        [0.1735, 0.2270, 0.2067, 0.2091, 0.1837],\n",
      "        [0.1743, 0.2281, 0.2060, 0.2079, 0.1837],\n",
      "        [0.1735, 0.2289, 0.2063, 0.2076, 0.1838],\n",
      "        [0.1732, 0.2285, 0.2061, 0.2079, 0.1843],\n",
      "        [0.1732, 0.2288, 0.2061, 0.2079, 0.1840],\n",
      "        [0.1746, 0.2264, 0.2060, 0.2092, 0.1838],\n",
      "        [0.1743, 0.2254, 0.2071, 0.2095, 0.1837]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "18/100 Running Loss: 1.5971348285675049\n",
      "tensor([[0.1722, 0.2292, 0.2067, 0.2079, 0.1840],\n",
      "        [0.1731, 0.2273, 0.2073, 0.2090, 0.1833],\n",
      "        [0.1731, 0.2275, 0.2073, 0.2088, 0.1833],\n",
      "        [0.1729, 0.2276, 0.2070, 0.2089, 0.1835],\n",
      "        [0.1738, 0.2288, 0.2062, 0.2077, 0.1835],\n",
      "        [0.1730, 0.2296, 0.2065, 0.2074, 0.1836],\n",
      "        [0.1727, 0.2292, 0.2063, 0.2077, 0.1841],\n",
      "        [0.1727, 0.2295, 0.2063, 0.2077, 0.1838],\n",
      "        [0.1741, 0.2269, 0.2063, 0.2090, 0.1836],\n",
      "        [0.1738, 0.2260, 0.2073, 0.2093, 0.1836]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "19/100 Running Loss: 1.5968612432479858\n",
      "tensor([[0.1717, 0.2299, 0.2069, 0.2077, 0.1838],\n",
      "        [0.1726, 0.2279, 0.2075, 0.2088, 0.1831],\n",
      "        [0.1725, 0.2281, 0.2076, 0.2087, 0.1831],\n",
      "        [0.1724, 0.2283, 0.2074, 0.2086, 0.1834],\n",
      "        [0.1734, 0.2294, 0.2064, 0.2074, 0.1833],\n",
      "        [0.1725, 0.2303, 0.2067, 0.2071, 0.1834],\n",
      "        [0.1721, 0.2299, 0.2065, 0.2075, 0.1839],\n",
      "        [0.1722, 0.2303, 0.2065, 0.2075, 0.1836],\n",
      "        [0.1737, 0.2275, 0.2065, 0.2089, 0.1835],\n",
      "        [0.1732, 0.2265, 0.2076, 0.2092, 0.1835]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "20/100 Running Loss: 1.5965828895568848\n",
      "tensor([[0.1711, 0.2307, 0.2071, 0.2074, 0.1836],\n",
      "        [0.1720, 0.2285, 0.2078, 0.2087, 0.1830],\n",
      "        [0.1720, 0.2287, 0.2078, 0.2085, 0.1830],\n",
      "        [0.1718, 0.2290, 0.2077, 0.2083, 0.1832],\n",
      "        [0.1729, 0.2301, 0.2067, 0.2072, 0.1831],\n",
      "        [0.1720, 0.2311, 0.2069, 0.2069, 0.1831],\n",
      "        [0.1716, 0.2307, 0.2068, 0.2072, 0.1837],\n",
      "        [0.1717, 0.2310, 0.2067, 0.2072, 0.1834],\n",
      "        [0.1732, 0.2281, 0.2067, 0.2087, 0.1833],\n",
      "        [0.1727, 0.2270, 0.2079, 0.2090, 0.1834]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "21/100 Running Loss: 1.5963009595870972\n",
      "tensor([[0.1706, 0.2314, 0.2073, 0.2072, 0.1834],\n",
      "        [0.1715, 0.2291, 0.2081, 0.2085, 0.1828],\n",
      "        [0.1714, 0.2293, 0.2081, 0.2083, 0.1828],\n",
      "        [0.1712, 0.2297, 0.2080, 0.2080, 0.1830],\n",
      "        [0.1724, 0.2308, 0.2069, 0.2070, 0.1829],\n",
      "        [0.1715, 0.2318, 0.2071, 0.2066, 0.1829],\n",
      "        [0.1711, 0.2314, 0.2070, 0.2070, 0.1835],\n",
      "        [0.1712, 0.2317, 0.2069, 0.2070, 0.1832],\n",
      "        [0.1728, 0.2287, 0.2069, 0.2085, 0.1831],\n",
      "        [0.1722, 0.2275, 0.2082, 0.2088, 0.1833]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/guilherme/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/100 Running Loss: 1.5960116386413574\n",
      "tensor([[0.1700, 0.2322, 0.2075, 0.2069, 0.1833],\n",
      "        [0.1710, 0.2298, 0.2083, 0.2083, 0.1827],\n",
      "        [0.1709, 0.2300, 0.2084, 0.2081, 0.1826],\n",
      "        [0.1706, 0.2304, 0.2083, 0.2077, 0.1829],\n",
      "        [0.1719, 0.2316, 0.2071, 0.2067, 0.1827],\n",
      "        [0.1710, 0.2326, 0.2074, 0.2063, 0.1827],\n",
      "        [0.1706, 0.2321, 0.2072, 0.2067, 0.1833],\n",
      "        [0.1707, 0.2325, 0.2071, 0.2067, 0.1830],\n",
      "        [0.1723, 0.2293, 0.2072, 0.2084, 0.1829],\n",
      "        [0.1716, 0.2281, 0.2085, 0.2086, 0.1831]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "23/100 Running Loss: 1.5957186222076416\n",
      "tensor([[0.1695, 0.2330, 0.2078, 0.2067, 0.1831],\n",
      "        [0.1704, 0.2304, 0.2086, 0.2081, 0.1825],\n",
      "        [0.1703, 0.2306, 0.2087, 0.2079, 0.1825],\n",
      "        [0.1700, 0.2312, 0.2086, 0.2075, 0.1827],\n",
      "        [0.1714, 0.2323, 0.2073, 0.2065, 0.1825],\n",
      "        [0.1704, 0.2334, 0.2076, 0.2060, 0.1825],\n",
      "        [0.1701, 0.2329, 0.2075, 0.2065, 0.1831],\n",
      "        [0.1701, 0.2333, 0.2074, 0.2064, 0.1828],\n",
      "        [0.1718, 0.2299, 0.2074, 0.2082, 0.1827],\n",
      "        [0.1711, 0.2287, 0.2088, 0.2085, 0.1830]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "24/100 Running Loss: 1.5954195261001587\n",
      "tensor([[0.1689, 0.2338, 0.2080, 0.2064, 0.1829],\n",
      "        [0.1698, 0.2310, 0.2089, 0.2079, 0.1824],\n",
      "        [0.1698, 0.2312, 0.2090, 0.2077, 0.1823],\n",
      "        [0.1694, 0.2319, 0.2089, 0.2072, 0.1826],\n",
      "        [0.1708, 0.2331, 0.2076, 0.2062, 0.1823],\n",
      "        [0.1699, 0.2342, 0.2078, 0.2058, 0.1823],\n",
      "        [0.1695, 0.2337, 0.2077, 0.2062, 0.1829],\n",
      "        [0.1696, 0.2341, 0.2076, 0.2062, 0.1826],\n",
      "        [0.1713, 0.2305, 0.2076, 0.2080, 0.1826],\n",
      "        [0.1705, 0.2292, 0.2091, 0.2083, 0.1828]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "25/100 Running Loss: 1.5951167345046997\n",
      "tensor([[0.1684, 0.2346, 0.2082, 0.2062, 0.1827],\n",
      "        [0.1692, 0.2316, 0.2092, 0.2077, 0.1822],\n",
      "        [0.1692, 0.2318, 0.2093, 0.2075, 0.1821],\n",
      "        [0.1688, 0.2327, 0.2092, 0.2069, 0.1824],\n",
      "        [0.1703, 0.2339, 0.2078, 0.2059, 0.1820],\n",
      "        [0.1693, 0.2351, 0.2081, 0.2055, 0.1821],\n",
      "        [0.1689, 0.2345, 0.2080, 0.2059, 0.1827],\n",
      "        [0.1690, 0.2349, 0.2078, 0.2059, 0.1824],\n",
      "        [0.1709, 0.2311, 0.2078, 0.2078, 0.1824],\n",
      "        [0.1700, 0.2298, 0.2095, 0.2081, 0.1827]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "26/100 Running Loss: 1.5948050022125244\n",
      "tensor([[0.1678, 0.2354, 0.2084, 0.2059, 0.1825],\n",
      "        [0.1687, 0.2323, 0.2096, 0.2075, 0.1820],\n",
      "        [0.1686, 0.2325, 0.2096, 0.2073, 0.1820],\n",
      "        [0.1682, 0.2335, 0.2095, 0.2066, 0.1822],\n",
      "        [0.1697, 0.2347, 0.2081, 0.2056, 0.1818],\n",
      "        [0.1687, 0.2359, 0.2084, 0.2051, 0.1818],\n",
      "        [0.1683, 0.2354, 0.2083, 0.2055, 0.1825],\n",
      "        [0.1685, 0.2358, 0.2080, 0.2056, 0.1822],\n",
      "        [0.1704, 0.2317, 0.2081, 0.2076, 0.1822],\n",
      "        [0.1694, 0.2304, 0.2098, 0.2079, 0.1825]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "27/100 Running Loss: 1.5944904088974\n",
      "tensor([[0.1672, 0.2363, 0.2086, 0.2056, 0.1823],\n",
      "        [0.1681, 0.2330, 0.2099, 0.2073, 0.1818],\n",
      "        [0.1680, 0.2332, 0.2099, 0.2071, 0.1818],\n",
      "        [0.1676, 0.2343, 0.2098, 0.2063, 0.1820],\n",
      "        [0.1692, 0.2355, 0.2084, 0.2053, 0.1816],\n",
      "        [0.1681, 0.2368, 0.2086, 0.2048, 0.1816],\n",
      "        [0.1677, 0.2363, 0.2085, 0.2052, 0.1823],\n",
      "        [0.1679, 0.2367, 0.2082, 0.2053, 0.1820],\n",
      "        [0.1699, 0.2324, 0.2084, 0.2074, 0.1820],\n",
      "        [0.1689, 0.2310, 0.2101, 0.2077, 0.1823]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "28/100 Running Loss: 1.5941696166992188\n",
      "tensor([[0.1666, 0.2372, 0.2088, 0.2053, 0.1821],\n",
      "        [0.1675, 0.2337, 0.2102, 0.2070, 0.1816],\n",
      "        [0.1674, 0.2339, 0.2103, 0.2068, 0.1816],\n",
      "        [0.1669, 0.2352, 0.2101, 0.2060, 0.1818],\n",
      "        [0.1686, 0.2364, 0.2086, 0.2050, 0.1814],\n",
      "        [0.1675, 0.2377, 0.2088, 0.2045, 0.1814],\n",
      "        [0.1671, 0.2372, 0.2088, 0.2048, 0.1821],\n",
      "        [0.1673, 0.2376, 0.2084, 0.2050, 0.1817],\n",
      "        [0.1694, 0.2330, 0.2086, 0.2071, 0.1818],\n",
      "        [0.1683, 0.2316, 0.2104, 0.2075, 0.1822]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "29/100 Running Loss: 1.5938361883163452\n",
      "tensor([[0.1659, 0.2382, 0.2090, 0.2050, 0.1819],\n",
      "        [0.1669, 0.2344, 0.2105, 0.2067, 0.1815],\n",
      "        [0.1668, 0.2347, 0.2106, 0.2065, 0.1814],\n",
      "        [0.1662, 0.2360, 0.2104, 0.2057, 0.1816],\n",
      "        [0.1680, 0.2372, 0.2090, 0.2047, 0.1811],\n",
      "        [0.1669, 0.2387, 0.2090, 0.2042, 0.1812],\n",
      "        [0.1664, 0.2382, 0.2090, 0.2044, 0.1819],\n",
      "        [0.1667, 0.2386, 0.2086, 0.2046, 0.1815],\n",
      "        [0.1688, 0.2337, 0.2089, 0.2069, 0.1816],\n",
      "        [0.1677, 0.2322, 0.2107, 0.2073, 0.1820]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "30/100 Running Loss: 1.5934910774230957\n",
      "tensor([[0.1653, 0.2393, 0.2092, 0.2046, 0.1817],\n",
      "        [0.1662, 0.2352, 0.2108, 0.2064, 0.1813],\n",
      "        [0.1662, 0.2355, 0.2109, 0.2062, 0.1812],\n",
      "        [0.1655, 0.2369, 0.2107, 0.2054, 0.1814],\n",
      "        [0.1673, 0.2382, 0.2093, 0.2043, 0.1809],\n",
      "        [0.1662, 0.2398, 0.2093, 0.2038, 0.1810],\n",
      "        [0.1658, 0.2392, 0.2093, 0.2040, 0.1817],\n",
      "        [0.1661, 0.2396, 0.2087, 0.2042, 0.1814],\n",
      "        [0.1683, 0.2344, 0.2092, 0.2067, 0.1814],\n",
      "        [0.1671, 0.2329, 0.2111, 0.2071, 0.1818]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "31/100 Running Loss: 1.5931333303451538\n",
      "tensor([[0.1646, 0.2404, 0.2094, 0.2042, 0.1815],\n",
      "        [0.1656, 0.2360, 0.2111, 0.2062, 0.1811],\n",
      "        [0.1656, 0.2363, 0.2112, 0.2060, 0.1810],\n",
      "        [0.1649, 0.2379, 0.2110, 0.2050, 0.1812],\n",
      "        [0.1666, 0.2392, 0.2096, 0.2040, 0.1806],\n",
      "        [0.1655, 0.2409, 0.2095, 0.2034, 0.1808],\n",
      "        [0.1651, 0.2403, 0.2095, 0.2036, 0.1815],\n",
      "        [0.1654, 0.2407, 0.2090, 0.2038, 0.1811],\n",
      "        [0.1677, 0.2351, 0.2095, 0.2064, 0.1813],\n",
      "        [0.1665, 0.2335, 0.2114, 0.2069, 0.1816]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "32/100 Running Loss: 1.5927618741989136\n",
      "tensor([[0.1639, 0.2415, 0.2096, 0.2038, 0.1812],\n",
      "        [0.1650, 0.2369, 0.2115, 0.2058, 0.1809],\n",
      "        [0.1649, 0.2371, 0.2115, 0.2056, 0.1808],\n",
      "        [0.1641, 0.2390, 0.2113, 0.2046, 0.1810],\n",
      "        [0.1660, 0.2402, 0.2099, 0.2036, 0.1804],\n",
      "        [0.1648, 0.2420, 0.2098, 0.2029, 0.1805],\n",
      "        [0.1643, 0.2414, 0.2098, 0.2032, 0.1813],\n",
      "        [0.1648, 0.2418, 0.2092, 0.2033, 0.1809],\n",
      "        [0.1671, 0.2358, 0.2098, 0.2061, 0.1811],\n",
      "        [0.1659, 0.2342, 0.2118, 0.2067, 0.1814]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "33/100 Running Loss: 1.5923786163330078\n",
      "tensor([[0.1632, 0.2427, 0.2098, 0.2033, 0.1810],\n",
      "        [0.1643, 0.2377, 0.2118, 0.2055, 0.1807],\n",
      "        [0.1642, 0.2380, 0.2119, 0.2053, 0.1806],\n",
      "        [0.1634, 0.2400, 0.2116, 0.2042, 0.1808],\n",
      "        [0.1653, 0.2413, 0.2102, 0.2031, 0.1801],\n",
      "        [0.1640, 0.2432, 0.2100, 0.2024, 0.1803],\n",
      "        [0.1636, 0.2425, 0.2100, 0.2027, 0.1811],\n",
      "        [0.1641, 0.2431, 0.2094, 0.2029, 0.1806],\n",
      "        [0.1665, 0.2366, 0.2101, 0.2059, 0.1809],\n",
      "        [0.1652, 0.2350, 0.2121, 0.2065, 0.1812]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "34/100 Running Loss: 1.5919828414916992\n",
      "tensor([[0.1624, 0.2440, 0.2100, 0.2029, 0.1807],\n",
      "        [0.1636, 0.2386, 0.2121, 0.2052, 0.1805],\n",
      "        [0.1635, 0.2389, 0.2122, 0.2050, 0.1804],\n",
      "        [0.1626, 0.2411, 0.2119, 0.2038, 0.1806],\n",
      "        [0.1646, 0.2424, 0.2104, 0.2027, 0.1799],\n",
      "        [0.1632, 0.2445, 0.2103, 0.2019, 0.1800],\n",
      "        [0.1629, 0.2437, 0.2102, 0.2023, 0.1809],\n",
      "        [0.1633, 0.2444, 0.2097, 0.2023, 0.1804],\n",
      "        [0.1659, 0.2374, 0.2105, 0.2056, 0.1806],\n",
      "        [0.1645, 0.2357, 0.2125, 0.2063, 0.1810]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "35/100 Running Loss: 1.5915749073028564\n",
      "tensor([[0.1616, 0.2452, 0.2103, 0.2024, 0.1805],\n",
      "        [0.1628, 0.2396, 0.2124, 0.2048, 0.1803],\n",
      "        [0.1628, 0.2399, 0.2125, 0.2046, 0.1802],\n",
      "        [0.1618, 0.2423, 0.2122, 0.2034, 0.1803],\n",
      "        [0.1638, 0.2436, 0.2107, 0.2023, 0.1796],\n",
      "        [0.1624, 0.2459, 0.2106, 0.2014, 0.1797],\n",
      "        [0.1621, 0.2450, 0.2104, 0.2018, 0.1807],\n",
      "        [0.1625, 0.2457, 0.2099, 0.2018, 0.1801],\n",
      "        [0.1652, 0.2383, 0.2108, 0.2052, 0.1804],\n",
      "        [0.1638, 0.2366, 0.2128, 0.2060, 0.1808]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "36/100 Running Loss: 1.5911548137664795\n",
      "tensor([[0.1608, 0.2466, 0.2105, 0.2019, 0.1802],\n",
      "        [0.1621, 0.2406, 0.2127, 0.2045, 0.1801],\n",
      "        [0.1620, 0.2409, 0.2128, 0.2043, 0.1800],\n",
      "        [0.1610, 0.2435, 0.2125, 0.2029, 0.1801],\n",
      "        [0.1631, 0.2448, 0.2110, 0.2018, 0.1793],\n",
      "        [0.1616, 0.2472, 0.2109, 0.2008, 0.1795],\n",
      "        [0.1613, 0.2463, 0.2106, 0.2013, 0.1805],\n",
      "        [0.1616, 0.2471, 0.2102, 0.2013, 0.1798],\n",
      "        [0.1646, 0.2393, 0.2111, 0.2049, 0.1802],\n",
      "        [0.1631, 0.2374, 0.2131, 0.2057, 0.1806]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "37/100 Running Loss: 1.590724229812622\n",
      "tensor([[0.1601, 0.2479, 0.2106, 0.2015, 0.1799],\n",
      "        [0.1613, 0.2416, 0.2130, 0.2041, 0.1799],\n",
      "        [0.1612, 0.2420, 0.2131, 0.2039, 0.1799],\n",
      "        [0.1601, 0.2447, 0.2128, 0.2024, 0.1799],\n",
      "        [0.1623, 0.2460, 0.2114, 0.2012, 0.1791],\n",
      "        [0.1607, 0.2486, 0.2112, 0.2002, 0.1793],\n",
      "        [0.1605, 0.2476, 0.2109, 0.2008, 0.1802],\n",
      "        [0.1608, 0.2485, 0.2105, 0.2007, 0.1796],\n",
      "        [0.1639, 0.2402, 0.2113, 0.2045, 0.1800],\n",
      "        [0.1624, 0.2383, 0.2134, 0.2055, 0.1805]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "38/100 Running Loss: 1.5902822017669678\n",
      "tensor([[0.1592, 0.2494, 0.2108, 0.2010, 0.1796],\n",
      "        [0.1604, 0.2427, 0.2133, 0.2037, 0.1798],\n",
      "        [0.1604, 0.2431, 0.2134, 0.2035, 0.1797],\n",
      "        [0.1592, 0.2460, 0.2131, 0.2019, 0.1798],\n",
      "        [0.1615, 0.2473, 0.2116, 0.2007, 0.1789],\n",
      "        [0.1599, 0.2501, 0.2115, 0.1996, 0.1790],\n",
      "        [0.1597, 0.2490, 0.2111, 0.2003, 0.1800],\n",
      "        [0.1599, 0.2499, 0.2108, 0.2001, 0.1794],\n",
      "        [0.1632, 0.2413, 0.2116, 0.2041, 0.1798],\n",
      "        [0.1617, 0.2392, 0.2137, 0.2052, 0.1803]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "39/100 Running Loss: 1.5898306369781494\n",
      "tensor([[0.1583, 0.2509, 0.2111, 0.2004, 0.1794],\n",
      "        [0.1596, 0.2438, 0.2136, 0.2033, 0.1796],\n",
      "        [0.1595, 0.2442, 0.2137, 0.2031, 0.1795],\n",
      "        [0.1583, 0.2473, 0.2134, 0.2014, 0.1796],\n",
      "        [0.1606, 0.2487, 0.2119, 0.2001, 0.1787],\n",
      "        [0.1590, 0.2516, 0.2117, 0.1989, 0.1788],\n",
      "        [0.1588, 0.2504, 0.2114, 0.1997, 0.1797],\n",
      "        [0.1590, 0.2514, 0.2110, 0.1995, 0.1792],\n",
      "        [0.1625, 0.2423, 0.2119, 0.2037, 0.1796],\n",
      "        [0.1609, 0.2401, 0.2140, 0.2048, 0.1801]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "40/100 Running Loss: 1.5893625020980835\n",
      "tensor([[0.1574, 0.2524, 0.2113, 0.1998, 0.1791],\n",
      "        [0.1587, 0.2450, 0.2140, 0.2029, 0.1794],\n",
      "        [0.1587, 0.2454, 0.2140, 0.2026, 0.1793],\n",
      "        [0.1573, 0.2487, 0.2137, 0.2009, 0.1794],\n",
      "        [0.1598, 0.2501, 0.2122, 0.1995, 0.1784],\n",
      "        [0.1580, 0.2531, 0.2120, 0.1983, 0.1786],\n",
      "        [0.1579, 0.2519, 0.2116, 0.1991, 0.1795],\n",
      "        [0.1581, 0.2530, 0.2112, 0.1988, 0.1790],\n",
      "        [0.1617, 0.2434, 0.2122, 0.2033, 0.1794],\n",
      "        [0.1601, 0.2411, 0.2143, 0.2045, 0.1800]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "41/100 Running Loss: 1.588874101638794\n",
      "tensor([[0.1564, 0.2540, 0.2115, 0.1991, 0.1789],\n",
      "        [0.1578, 0.2462, 0.2143, 0.2024, 0.1792],\n",
      "        [0.1578, 0.2466, 0.2143, 0.2022, 0.1791],\n",
      "        [0.1563, 0.2501, 0.2140, 0.2004, 0.1792],\n",
      "        [0.1589, 0.2515, 0.2125, 0.1989, 0.1782],\n",
      "        [0.1571, 0.2547, 0.2123, 0.1976, 0.1783],\n",
      "        [0.1569, 0.2534, 0.2118, 0.1985, 0.1793],\n",
      "        [0.1571, 0.2546, 0.2114, 0.1981, 0.1788],\n",
      "        [0.1610, 0.2446, 0.2124, 0.2029, 0.1792],\n",
      "        [0.1592, 0.2421, 0.2147, 0.2041, 0.1798]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "42/100 Running Loss: 1.5883671045303345\n",
      "tensor([[0.1555, 0.2557, 0.2117, 0.1984, 0.1787],\n",
      "        [0.1569, 0.2474, 0.2146, 0.2020, 0.1791],\n",
      "        [0.1569, 0.2478, 0.2146, 0.2017, 0.1789],\n",
      "        [0.1553, 0.2516, 0.2143, 0.1998, 0.1790],\n",
      "        [0.1580, 0.2530, 0.2128, 0.1982, 0.1779],\n",
      "        [0.1561, 0.2564, 0.2125, 0.1969, 0.1781],\n",
      "        [0.1560, 0.2551, 0.2120, 0.1978, 0.1791],\n",
      "        [0.1561, 0.2564, 0.2115, 0.1974, 0.1786],\n",
      "        [0.1602, 0.2457, 0.2127, 0.2024, 0.1790],\n",
      "        [0.1584, 0.2432, 0.2150, 0.2037, 0.1797]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "43/100 Running Loss: 1.5878407955169678\n",
      "tensor([[0.1545, 0.2575, 0.2119, 0.1977, 0.1784],\n",
      "        [0.1560, 0.2487, 0.2149, 0.2015, 0.1789],\n",
      "        [0.1559, 0.2491, 0.2149, 0.2012, 0.1788],\n",
      "        [0.1543, 0.2532, 0.2145, 0.1992, 0.1789],\n",
      "        [0.1571, 0.2546, 0.2131, 0.1976, 0.1776],\n",
      "        [0.1551, 0.2582, 0.2127, 0.1962, 0.1778],\n",
      "        [0.1550, 0.2568, 0.2122, 0.1971, 0.1789],\n",
      "        [0.1551, 0.2581, 0.2117, 0.1967, 0.1783],\n",
      "        [0.1593, 0.2470, 0.2130, 0.2019, 0.1788],\n",
      "        [0.1575, 0.2443, 0.2154, 0.2033, 0.1795]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "44/100 Running Loss: 1.5872993469238281\n",
      "tensor([[0.1535, 0.2593, 0.2121, 0.1969, 0.1782],\n",
      "        [0.1550, 0.2500, 0.2152, 0.2010, 0.1787],\n",
      "        [0.1549, 0.2505, 0.2153, 0.2007, 0.1786],\n",
      "        [0.1532, 0.2548, 0.2148, 0.1986, 0.1786],\n",
      "        [0.1561, 0.2562, 0.2134, 0.1970, 0.1774],\n",
      "        [0.1541, 0.2600, 0.2129, 0.1954, 0.1775],\n",
      "        [0.1540, 0.2585, 0.2125, 0.1964, 0.1787],\n",
      "        [0.1541, 0.2600, 0.2119, 0.1960, 0.1780],\n",
      "        [0.1585, 0.2482, 0.2133, 0.2014, 0.1786],\n",
      "        [0.1566, 0.2454, 0.2157, 0.2029, 0.1794]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "45/100 Running Loss: 1.5867419242858887\n",
      "tensor([[0.1524, 0.2612, 0.2124, 0.1962, 0.1779],\n",
      "        [0.1540, 0.2515, 0.2155, 0.2004, 0.1785],\n",
      "        [0.1539, 0.2519, 0.2155, 0.2002, 0.1784],\n",
      "        [0.1521, 0.2565, 0.2150, 0.1979, 0.1784],\n",
      "        [0.1551, 0.2579, 0.2136, 0.1963, 0.1771],\n",
      "        [0.1530, 0.2620, 0.2131, 0.1947, 0.1772],\n",
      "        [0.1529, 0.2602, 0.2127, 0.1957, 0.1784],\n",
      "        [0.1531, 0.2619, 0.2121, 0.1952, 0.1777],\n",
      "        [0.1576, 0.2495, 0.2136, 0.2009, 0.1784],\n",
      "        [0.1557, 0.2465, 0.2161, 0.2025, 0.1792]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "46/100 Running Loss: 1.586167335510254\n",
      "tensor([[0.1514, 0.2630, 0.2126, 0.1954, 0.1776],\n",
      "        [0.1530, 0.2529, 0.2158, 0.1999, 0.1784],\n",
      "        [0.1529, 0.2534, 0.2158, 0.1996, 0.1783],\n",
      "        [0.1510, 0.2583, 0.2153, 0.1972, 0.1782],\n",
      "        [0.1541, 0.2597, 0.2139, 0.1956, 0.1768],\n",
      "        [0.1519, 0.2639, 0.2133, 0.1939, 0.1769],\n",
      "        [0.1519, 0.2621, 0.2129, 0.1950, 0.1781],\n",
      "        [0.1520, 0.2638, 0.2123, 0.1944, 0.1775],\n",
      "        [0.1567, 0.2509, 0.2139, 0.2003, 0.1782],\n",
      "        [0.1548, 0.2477, 0.2164, 0.2021, 0.1790]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "47/100 Running Loss: 1.5855766534805298\n",
      "tensor([[0.1503, 0.2650, 0.2129, 0.1946, 0.1773],\n",
      "        [0.1520, 0.2544, 0.2160, 0.1993, 0.1782],\n",
      "        [0.1519, 0.2549, 0.2160, 0.1990, 0.1781],\n",
      "        [0.1499, 0.2601, 0.2155, 0.1966, 0.1779],\n",
      "        [0.1531, 0.2615, 0.2141, 0.1948, 0.1765],\n",
      "        [0.1508, 0.2660, 0.2135, 0.1931, 0.1766],\n",
      "        [0.1508, 0.2639, 0.2132, 0.1942, 0.1778],\n",
      "        [0.1509, 0.2658, 0.2125, 0.1936, 0.1771],\n",
      "        [0.1558, 0.2523, 0.2141, 0.1998, 0.1780],\n",
      "        [0.1538, 0.2490, 0.2168, 0.2016, 0.1789]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "48/100 Running Loss: 1.5849682092666626\n",
      "tensor([[0.1492, 0.2669, 0.2131, 0.1938, 0.1769],\n",
      "        [0.1510, 0.2559, 0.2163, 0.1987, 0.1781],\n",
      "        [0.1508, 0.2565, 0.2163, 0.1984, 0.1779],\n",
      "        [0.1488, 0.2619, 0.2158, 0.1959, 0.1776],\n",
      "        [0.1520, 0.2634, 0.2143, 0.1940, 0.1762],\n",
      "        [0.1497, 0.2681, 0.2137, 0.1922, 0.1763],\n",
      "        [0.1497, 0.2659, 0.2135, 0.1935, 0.1775],\n",
      "        [0.1498, 0.2679, 0.2127, 0.1928, 0.1768],\n",
      "        [0.1549, 0.2538, 0.2144, 0.1992, 0.1778],\n",
      "        [0.1529, 0.2503, 0.2171, 0.2011, 0.1787]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "49/100 Running Loss: 1.5843435525894165\n",
      "tensor([[0.1482, 0.2689, 0.2134, 0.1930, 0.1765],\n",
      "        [0.1500, 0.2575, 0.2165, 0.1982, 0.1779],\n",
      "        [0.1498, 0.2581, 0.2166, 0.1978, 0.1777],\n",
      "        [0.1476, 0.2638, 0.2161, 0.1951, 0.1773],\n",
      "        [0.1510, 0.2654, 0.2146, 0.1932, 0.1758],\n",
      "        [0.1486, 0.2702, 0.2140, 0.1913, 0.1758],\n",
      "        [0.1486, 0.2678, 0.2137, 0.1927, 0.1771],\n",
      "        [0.1487, 0.2700, 0.2130, 0.1920, 0.1764],\n",
      "        [0.1540, 0.2553, 0.2147, 0.1986, 0.1775],\n",
      "        [0.1519, 0.2516, 0.2174, 0.2006, 0.1785]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "50/100 Running Loss: 1.583702802658081\n",
      "tensor([[0.1471, 0.2710, 0.2137, 0.1921, 0.1761],\n",
      "        [0.1489, 0.2591, 0.2168, 0.1976, 0.1776],\n",
      "        [0.1487, 0.2597, 0.2169, 0.1972, 0.1774],\n",
      "        [0.1465, 0.2658, 0.2164, 0.1944, 0.1770],\n",
      "        [0.1499, 0.2674, 0.2148, 0.1924, 0.1754],\n",
      "        [0.1474, 0.2724, 0.2143, 0.1904, 0.1754],\n",
      "        [0.1475, 0.2699, 0.2140, 0.1919, 0.1767],\n",
      "        [0.1475, 0.2722, 0.2133, 0.1910, 0.1759],\n",
      "        [0.1530, 0.2568, 0.2150, 0.1979, 0.1773],\n",
      "        [0.1509, 0.2530, 0.2177, 0.2001, 0.1783]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "51/100 Running Loss: 1.5830481052398682\n",
      "tensor([[0.1459, 0.2732, 0.2140, 0.1913, 0.1756],\n",
      "        [0.1478, 0.2607, 0.2172, 0.1970, 0.1774],\n",
      "        [0.1477, 0.2613, 0.2172, 0.1966, 0.1772],\n",
      "        [0.1453, 0.2678, 0.2167, 0.1936, 0.1766],\n",
      "        [0.1488, 0.2696, 0.2151, 0.1915, 0.1750],\n",
      "        [0.1462, 0.2747, 0.2146, 0.1895, 0.1749],\n",
      "        [0.1463, 0.2720, 0.2143, 0.1910, 0.1763],\n",
      "        [0.1464, 0.2745, 0.2136, 0.1901, 0.1755],\n",
      "        [0.1521, 0.2584, 0.2153, 0.1973, 0.1770],\n",
      "        [0.1499, 0.2544, 0.2181, 0.1996, 0.1781]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "52/100 Running Loss: 1.582374095916748\n",
      "tensor([[0.1447, 0.2754, 0.2143, 0.1903, 0.1752],\n",
      "        [0.1468, 0.2623, 0.2175, 0.1963, 0.1771],\n",
      "        [0.1466, 0.2630, 0.2176, 0.1960, 0.1769],\n",
      "        [0.1440, 0.2699, 0.2170, 0.1928, 0.1763],\n",
      "        [0.1476, 0.2717, 0.2154, 0.1907, 0.1746],\n",
      "        [0.1450, 0.2771, 0.2150, 0.1885, 0.1744],\n",
      "        [0.1452, 0.2742, 0.2146, 0.1901, 0.1758],\n",
      "        [0.1452, 0.2768, 0.2139, 0.1892, 0.1750],\n",
      "        [0.1511, 0.2599, 0.2156, 0.1966, 0.1767],\n",
      "        [0.1489, 0.2558, 0.2184, 0.1990, 0.1778]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "53/100 Running Loss: 1.5816844701766968\n",
      "tensor([[0.1436, 0.2777, 0.2146, 0.1894, 0.1747],\n",
      "        [0.1457, 0.2640, 0.2179, 0.1957, 0.1768],\n",
      "        [0.1455, 0.2647, 0.2180, 0.1953, 0.1766],\n",
      "        [0.1428, 0.2720, 0.2174, 0.1919, 0.1758],\n",
      "        [0.1465, 0.2738, 0.2158, 0.1898, 0.1741],\n",
      "        [0.1438, 0.2795, 0.2153, 0.1876, 0.1739],\n",
      "        [0.1440, 0.2765, 0.2150, 0.1892, 0.1753],\n",
      "        [0.1439, 0.2792, 0.2142, 0.1882, 0.1745],\n",
      "        [0.1501, 0.2616, 0.2160, 0.1959, 0.1764],\n",
      "        [0.1479, 0.2572, 0.2188, 0.1985, 0.1776]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "54/100 Running Loss: 1.5809739828109741\n",
      "tensor([[0.1423, 0.2801, 0.2150, 0.1884, 0.1742],\n",
      "        [0.1446, 0.2657, 0.2183, 0.1950, 0.1765],\n",
      "        [0.1444, 0.2664, 0.2184, 0.1946, 0.1762],\n",
      "        [0.1415, 0.2741, 0.2179, 0.1911, 0.1754],\n",
      "        [0.1453, 0.2761, 0.2161, 0.1888, 0.1736],\n",
      "        [0.1425, 0.2820, 0.2156, 0.1866, 0.1733],\n",
      "        [0.1428, 0.2788, 0.2154, 0.1883, 0.1748],\n",
      "        [0.1426, 0.2816, 0.2146, 0.1873, 0.1739],\n",
      "        [0.1491, 0.2632, 0.2164, 0.1953, 0.1761],\n",
      "        [0.1469, 0.2587, 0.2192, 0.1979, 0.1773]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "55/100 Running Loss: 1.5802456140518188\n",
      "tensor([[0.1411, 0.2826, 0.2153, 0.1874, 0.1736],\n",
      "        [0.1434, 0.2674, 0.2188, 0.1943, 0.1761],\n",
      "        [0.1432, 0.2682, 0.2189, 0.1938, 0.1759],\n",
      "        [0.1403, 0.2763, 0.2183, 0.1902, 0.1749],\n",
      "        [0.1441, 0.2784, 0.2165, 0.1879, 0.1731],\n",
      "        [0.1412, 0.2845, 0.2160, 0.1856, 0.1727],\n",
      "        [0.1415, 0.2811, 0.2158, 0.1874, 0.1743],\n",
      "        [0.1413, 0.2841, 0.2150, 0.1862, 0.1734],\n",
      "        [0.1481, 0.2649, 0.2168, 0.1946, 0.1757],\n",
      "        [0.1458, 0.2602, 0.2197, 0.1973, 0.1770]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "56/100 Running Loss: 1.5794957876205444\n",
      "tensor([[0.1397, 0.2851, 0.2157, 0.1864, 0.1730],\n",
      "        [0.1423, 0.2692, 0.2192, 0.1935, 0.1758],\n",
      "        [0.1421, 0.2700, 0.2193, 0.1931, 0.1755],\n",
      "        [0.1390, 0.2785, 0.2188, 0.1893, 0.1745],\n",
      "        [0.1429, 0.2807, 0.2168, 0.1870, 0.1726],\n",
      "        [0.1398, 0.2872, 0.2164, 0.1846, 0.1720],\n",
      "        [0.1403, 0.2834, 0.2162, 0.1864, 0.1737],\n",
      "        [0.1400, 0.2867, 0.2154, 0.1852, 0.1727],\n",
      "        [0.1471, 0.2665, 0.2172, 0.1938, 0.1754],\n",
      "        [0.1448, 0.2616, 0.2201, 0.1967, 0.1767]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "57/100 Running Loss: 1.578721284866333\n",
      "tensor([[0.1384, 0.2877, 0.2161, 0.1854, 0.1724],\n",
      "        [0.1411, 0.2710, 0.2197, 0.1928, 0.1754],\n",
      "        [0.1409, 0.2718, 0.2198, 0.1923, 0.1752],\n",
      "        [0.1377, 0.2808, 0.2193, 0.1883, 0.1739],\n",
      "        [0.1416, 0.2832, 0.2172, 0.1860, 0.1720],\n",
      "        [0.1385, 0.2899, 0.2168, 0.1835, 0.1713],\n",
      "        [0.1390, 0.2859, 0.2166, 0.1854, 0.1731],\n",
      "        [0.1386, 0.2894, 0.2159, 0.1841, 0.1721],\n",
      "        [0.1460, 0.2682, 0.2177, 0.1931, 0.1750],\n",
      "        [0.1437, 0.2631, 0.2205, 0.1962, 0.1764]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "58/100 Running Loss: 1.5779294967651367\n",
      "tensor([[0.1370, 0.2904, 0.2165, 0.1844, 0.1717],\n",
      "        [0.1400, 0.2728, 0.2201, 0.1920, 0.1751],\n",
      "        [0.1397, 0.2736, 0.2202, 0.1916, 0.1748],\n",
      "        [0.1364, 0.2831, 0.2198, 0.1873, 0.1734],\n",
      "        [0.1404, 0.2857, 0.2177, 0.1849, 0.1713],\n",
      "        [0.1371, 0.2926, 0.2173, 0.1824, 0.1706],\n",
      "        [0.1376, 0.2884, 0.2171, 0.1844, 0.1725],\n",
      "        [0.1372, 0.2921, 0.2164, 0.1830, 0.1714],\n",
      "        [0.1450, 0.2700, 0.2182, 0.1923, 0.1745],\n",
      "        [0.1427, 0.2647, 0.2210, 0.1956, 0.1761]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "59/100 Running Loss: 1.5771158933639526\n",
      "tensor([[0.1356, 0.2932, 0.2169, 0.1833, 0.1710],\n",
      "        [0.1388, 0.2746, 0.2206, 0.1913, 0.1747],\n",
      "        [0.1385, 0.2755, 0.2207, 0.1908, 0.1744],\n",
      "        [0.1350, 0.2855, 0.2203, 0.1863, 0.1729],\n",
      "        [0.1391, 0.2883, 0.2181, 0.1839, 0.1707],\n",
      "        [0.1356, 0.2954, 0.2178, 0.1813, 0.1699],\n",
      "        [0.1363, 0.2910, 0.2175, 0.1833, 0.1718],\n",
      "        [0.1358, 0.2948, 0.2169, 0.1818, 0.1706],\n",
      "        [0.1439, 0.2718, 0.2187, 0.1916, 0.1741],\n",
      "        [0.1416, 0.2662, 0.2215, 0.1949, 0.1758]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "60/100 Running Loss: 1.5762745141983032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1342, 0.2960, 0.2173, 0.1821, 0.1703],\n",
      "        [0.1376, 0.2765, 0.2211, 0.1905, 0.1743],\n",
      "        [0.1373, 0.2774, 0.2212, 0.1901, 0.1740],\n",
      "        [0.1337, 0.2879, 0.2208, 0.1853, 0.1723],\n",
      "        [0.1377, 0.2909, 0.2186, 0.1828, 0.1700],\n",
      "        [0.1342, 0.2983, 0.2183, 0.1801, 0.1691],\n",
      "        [0.1349, 0.2937, 0.2180, 0.1822, 0.1711],\n",
      "        [0.1344, 0.2977, 0.2175, 0.1807, 0.1699],\n",
      "        [0.1428, 0.2736, 0.2192, 0.1908, 0.1736],\n",
      "        [0.1405, 0.2677, 0.2220, 0.1943, 0.1754]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "61/100 Running Loss: 1.5754061937332153\n",
      "tensor([[0.1328, 0.2989, 0.2178, 0.1810, 0.1696],\n",
      "        [0.1364, 0.2784, 0.2216, 0.1898, 0.1738],\n",
      "        [0.1361, 0.2793, 0.2217, 0.1893, 0.1736],\n",
      "        [0.1323, 0.2904, 0.2213, 0.1843, 0.1717],\n",
      "        [0.1364, 0.2936, 0.2191, 0.1817, 0.1692],\n",
      "        [0.1327, 0.3014, 0.2189, 0.1789, 0.1682],\n",
      "        [0.1335, 0.2965, 0.2185, 0.1811, 0.1703],\n",
      "        [0.1329, 0.3006, 0.2180, 0.1794, 0.1690],\n",
      "        [0.1417, 0.2754, 0.2198, 0.1899, 0.1731],\n",
      "        [0.1394, 0.2693, 0.2225, 0.1937, 0.1750]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "62/100 Running Loss: 1.574510097503662\n",
      "tensor([[0.1313, 0.3019, 0.2183, 0.1797, 0.1687],\n",
      "        [0.1352, 0.2803, 0.2221, 0.1890, 0.1733],\n",
      "        [0.1349, 0.2813, 0.2222, 0.1885, 0.1731],\n",
      "        [0.1310, 0.2929, 0.2218, 0.1832, 0.1711],\n",
      "        [0.1350, 0.2964, 0.2197, 0.1805, 0.1684],\n",
      "        [0.1312, 0.3045, 0.2195, 0.1776, 0.1673],\n",
      "        [0.1321, 0.2994, 0.2190, 0.1799, 0.1696],\n",
      "        [0.1314, 0.3037, 0.2185, 0.1781, 0.1682],\n",
      "        [0.1406, 0.2774, 0.2204, 0.1891, 0.1725],\n",
      "        [0.1384, 0.2709, 0.2231, 0.1930, 0.1746]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "63/100 Running Loss: 1.5735851526260376\n",
      "tensor([[0.1298, 0.3051, 0.2189, 0.1784, 0.1678],\n",
      "        [0.1340, 0.2823, 0.2227, 0.1882, 0.1728],\n",
      "        [0.1337, 0.2832, 0.2228, 0.1877, 0.1725],\n",
      "        [0.1296, 0.2955, 0.2224, 0.1821, 0.1704],\n",
      "        [0.1336, 0.2993, 0.2203, 0.1793, 0.1676],\n",
      "        [0.1296, 0.3077, 0.2200, 0.1762, 0.1664],\n",
      "        [0.1307, 0.3024, 0.2194, 0.1787, 0.1688],\n",
      "        [0.1299, 0.3069, 0.2191, 0.1768, 0.1673],\n",
      "        [0.1395, 0.2793, 0.2210, 0.1882, 0.1719],\n",
      "        [0.1373, 0.2725, 0.2237, 0.1924, 0.1742]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "64/100 Running Loss: 1.5726326704025269\n",
      "tensor([[0.1282, 0.3084, 0.2194, 0.1771, 0.1669],\n",
      "        [0.1328, 0.2843, 0.2233, 0.1873, 0.1723],\n",
      "        [0.1325, 0.2853, 0.2234, 0.1868, 0.1720],\n",
      "        [0.1282, 0.2982, 0.2230, 0.1809, 0.1696],\n",
      "        [0.1321, 0.3023, 0.2208, 0.1781, 0.1667],\n",
      "        [0.1280, 0.3112, 0.2206, 0.1748, 0.1654],\n",
      "        [0.1292, 0.3055, 0.2199, 0.1774, 0.1679],\n",
      "        [0.1283, 0.3103, 0.2196, 0.1754, 0.1663],\n",
      "        [0.1383, 0.2814, 0.2216, 0.1873, 0.1713],\n",
      "        [0.1362, 0.2742, 0.2243, 0.1917, 0.1737]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "65/100 Running Loss: 1.5716502666473389\n",
      "tensor([[0.1266, 0.3118, 0.2200, 0.1757, 0.1659],\n",
      "        [0.1315, 0.2865, 0.2239, 0.1864, 0.1716],\n",
      "        [0.1312, 0.2875, 0.2240, 0.1859, 0.1713],\n",
      "        [0.1268, 0.3011, 0.2236, 0.1797, 0.1688],\n",
      "        [0.1306, 0.3054, 0.2215, 0.1768, 0.1657],\n",
      "        [0.1264, 0.3147, 0.2211, 0.1733, 0.1644],\n",
      "        [0.1277, 0.3088, 0.2204, 0.1761, 0.1670],\n",
      "        [0.1267, 0.3138, 0.2202, 0.1739, 0.1653],\n",
      "        [0.1372, 0.2835, 0.2222, 0.1864, 0.1707],\n",
      "        [0.1350, 0.2760, 0.2249, 0.1909, 0.1731]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "66/100 Running Loss: 1.5706366300582886\n",
      "tensor([[0.1250, 0.3154, 0.2205, 0.1742, 0.1648],\n",
      "        [0.1303, 0.2887, 0.2246, 0.1855, 0.1709],\n",
      "        [0.1300, 0.2897, 0.2247, 0.1850, 0.1706],\n",
      "        [0.1253, 0.3040, 0.2243, 0.1784, 0.1679],\n",
      "        [0.1291, 0.3086, 0.2221, 0.1754, 0.1647],\n",
      "        [0.1248, 0.3185, 0.2217, 0.1718, 0.1633],\n",
      "        [0.1262, 0.3122, 0.2209, 0.1747, 0.1660],\n",
      "        [0.1251, 0.3175, 0.2208, 0.1724, 0.1642],\n",
      "        [0.1360, 0.2857, 0.2228, 0.1855, 0.1700],\n",
      "        [0.1339, 0.2778, 0.2256, 0.1902, 0.1725]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "67/100 Running Loss: 1.5695880651474\n",
      "tensor([[0.1234, 0.3192, 0.2210, 0.1726, 0.1637],\n",
      "        [0.1290, 0.2910, 0.2253, 0.1845, 0.1702],\n",
      "        [0.1286, 0.2921, 0.2254, 0.1840, 0.1699],\n",
      "        [0.1239, 0.3071, 0.2250, 0.1771, 0.1669],\n",
      "        [0.1276, 0.3120, 0.2227, 0.1740, 0.1637],\n",
      "        [0.1231, 0.3223, 0.2222, 0.1702, 0.1622],\n",
      "        [0.1246, 0.3158, 0.2214, 0.1733, 0.1649],\n",
      "        [0.1235, 0.3213, 0.2213, 0.1709, 0.1631],\n",
      "        [0.1348, 0.2880, 0.2235, 0.1845, 0.1693],\n",
      "        [0.1327, 0.2797, 0.2263, 0.1893, 0.1719]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "68/100 Running Loss: 1.5685046911239624\n",
      "tensor([[0.1218, 0.3232, 0.2216, 0.1709, 0.1625],\n",
      "        [0.1276, 0.2934, 0.2261, 0.1835, 0.1694],\n",
      "        [0.1273, 0.2945, 0.2262, 0.1829, 0.1690],\n",
      "        [0.1223, 0.3103, 0.2257, 0.1757, 0.1659],\n",
      "        [0.1260, 0.3155, 0.2233, 0.1725, 0.1626],\n",
      "        [0.1215, 0.3263, 0.2227, 0.1685, 0.1610],\n",
      "        [0.1230, 0.3195, 0.2219, 0.1718, 0.1638],\n",
      "        [0.1219, 0.3253, 0.2218, 0.1692, 0.1619],\n",
      "        [0.1335, 0.2904, 0.2241, 0.1834, 0.1685],\n",
      "        [0.1315, 0.2817, 0.2270, 0.1885, 0.1713]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "69/100 Running Loss: 1.5673872232437134\n",
      "tensor([[0.1201, 0.3273, 0.2221, 0.1692, 0.1613],\n",
      "        [0.1263, 0.2958, 0.2269, 0.1825, 0.1685],\n",
      "        [0.1260, 0.2970, 0.2270, 0.1819, 0.1682],\n",
      "        [0.1208, 0.3137, 0.2265, 0.1743, 0.1648],\n",
      "        [0.1244, 0.3191, 0.2239, 0.1710, 0.1615],\n",
      "        [0.1198, 0.3305, 0.2232, 0.1668, 0.1597],\n",
      "        [0.1214, 0.3233, 0.2224, 0.1703, 0.1626],\n",
      "        [0.1202, 0.3294, 0.2223, 0.1675, 0.1607],\n",
      "        [0.1323, 0.2929, 0.2248, 0.1824, 0.1676],\n",
      "        [0.1303, 0.2837, 0.2278, 0.1876, 0.1706]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "70/100 Running Loss: 1.5662333965301514\n",
      "tensor([[0.1184, 0.3316, 0.2226, 0.1674, 0.1600],\n",
      "        [0.1250, 0.2983, 0.2277, 0.1814, 0.1677],\n",
      "        [0.1246, 0.2995, 0.2278, 0.1808, 0.1673],\n",
      "        [0.1192, 0.3172, 0.2272, 0.1728, 0.1636],\n",
      "        [0.1228, 0.3230, 0.2245, 0.1693, 0.1603],\n",
      "        [0.1180, 0.3349, 0.2237, 0.1650, 0.1584],\n",
      "        [0.1198, 0.3273, 0.2230, 0.1687, 0.1613],\n",
      "        [0.1185, 0.3337, 0.2228, 0.1657, 0.1593],\n",
      "        [0.1310, 0.2954, 0.2255, 0.1813, 0.1668],\n",
      "        [0.1291, 0.2858, 0.2285, 0.1867, 0.1698]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "71/100 Running Loss: 1.565044641494751\n",
      "tensor([[0.1167, 0.3359, 0.2231, 0.1657, 0.1587],\n",
      "        [0.1236, 0.3009, 0.2285, 0.1803, 0.1667],\n",
      "        [0.1232, 0.3022, 0.2286, 0.1797, 0.1664],\n",
      "        [0.1176, 0.3208, 0.2280, 0.1712, 0.1624],\n",
      "        [0.1212, 0.3269, 0.2251, 0.1677, 0.1591],\n",
      "        [0.1163, 0.3393, 0.2242, 0.1631, 0.1570],\n",
      "        [0.1181, 0.3314, 0.2235, 0.1670, 0.1600],\n",
      "        [0.1167, 0.3381, 0.2232, 0.1639, 0.1579],\n",
      "        [0.1297, 0.2980, 0.2263, 0.1802, 0.1658],\n",
      "        [0.1278, 0.2880, 0.2293, 0.1858, 0.1691]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "72/100 Running Loss: 1.56382155418396\n",
      "tensor([[0.1149, 0.3404, 0.2235, 0.1638, 0.1573],\n",
      "        [0.1222, 0.3036, 0.2293, 0.1791, 0.1658],\n",
      "        [0.1218, 0.3049, 0.2294, 0.1785, 0.1654],\n",
      "        [0.1160, 0.3244, 0.2288, 0.1696, 0.1611],\n",
      "        [0.1196, 0.3310, 0.2257, 0.1659, 0.1578],\n",
      "        [0.1145, 0.3440, 0.2248, 0.1612, 0.1555],\n",
      "        [0.1164, 0.3357, 0.2240, 0.1653, 0.1586],\n",
      "        [0.1150, 0.3428, 0.2237, 0.1620, 0.1565],\n",
      "        [0.1284, 0.3006, 0.2270, 0.1790, 0.1649],\n",
      "        [0.1266, 0.2902, 0.2301, 0.1849, 0.1683]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "73/100 Running Loss: 1.5625603199005127\n",
      "tensor([[0.1132, 0.3450, 0.2240, 0.1619, 0.1559],\n",
      "        [0.1208, 0.3063, 0.2301, 0.1780, 0.1649],\n",
      "        [0.1204, 0.3077, 0.2302, 0.1773, 0.1644],\n",
      "        [0.1143, 0.3282, 0.2296, 0.1680, 0.1598],\n",
      "        [0.1179, 0.3353, 0.2262, 0.1641, 0.1564],\n",
      "        [0.1127, 0.3488, 0.2252, 0.1593, 0.1540],\n",
      "        [0.1147, 0.3402, 0.2245, 0.1635, 0.1572],\n",
      "        [0.1132, 0.3475, 0.2242, 0.1601, 0.1550],\n",
      "        [0.1271, 0.3034, 0.2277, 0.1778, 0.1640],\n",
      "        [0.1253, 0.2925, 0.2309, 0.1839, 0.1674]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "74/100 Running Loss: 1.5612577199935913\n",
      "tensor([[0.1114, 0.3498, 0.2244, 0.1600, 0.1543],\n",
      "        [0.1194, 0.3092, 0.2308, 0.1767, 0.1639],\n",
      "        [0.1190, 0.3105, 0.2310, 0.1761, 0.1634],\n",
      "        [0.1126, 0.3322, 0.2304, 0.1663, 0.1585],\n",
      "        [0.1162, 0.3397, 0.2268, 0.1623, 0.1550],\n",
      "        [0.1109, 0.3538, 0.2257, 0.1572, 0.1524],\n",
      "        [0.1129, 0.3448, 0.2250, 0.1616, 0.1557],\n",
      "        [0.1113, 0.3525, 0.2246, 0.1581, 0.1534],\n",
      "        [0.1257, 0.3062, 0.2284, 0.1766, 0.1630],\n",
      "        [0.1240, 0.2948, 0.2317, 0.1830, 0.1666]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "75/100 Running Loss: 1.559915542602539\n",
      "tensor([[0.1095, 0.3549, 0.2248, 0.1580, 0.1528],\n",
      "        [0.1179, 0.3121, 0.2316, 0.1755, 0.1629],\n",
      "        [0.1175, 0.3135, 0.2317, 0.1748, 0.1624],\n",
      "        [0.1109, 0.3363, 0.2311, 0.1646, 0.1571],\n",
      "        [0.1145, 0.3442, 0.2273, 0.1605, 0.1536],\n",
      "        [0.1090, 0.3590, 0.2261, 0.1551, 0.1507],\n",
      "        [0.1111, 0.3496, 0.2255, 0.1597, 0.1541],\n",
      "        [0.1095, 0.3576, 0.2251, 0.1561, 0.1518],\n",
      "        [0.1244, 0.3092, 0.2291, 0.1753, 0.1620],\n",
      "        [0.1227, 0.2971, 0.2325, 0.1819, 0.1658]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "76/100 Running Loss: 1.5585322380065918\n",
      "tensor([[0.1077, 0.3601, 0.2252, 0.1559, 0.1511],\n",
      "        [0.1165, 0.3150, 0.2324, 0.1742, 0.1619],\n",
      "        [0.1161, 0.3165, 0.2325, 0.1735, 0.1614],\n",
      "        [0.1092, 0.3405, 0.2318, 0.1628, 0.1557],\n",
      "        [0.1127, 0.3490, 0.2278, 0.1585, 0.1520],\n",
      "        [0.1070, 0.3645, 0.2265, 0.1530, 0.1490],\n",
      "        [0.1093, 0.3545, 0.2260, 0.1577, 0.1525],\n",
      "        [0.1076, 0.3629, 0.2254, 0.1540, 0.1501],\n",
      "        [0.1230, 0.3122, 0.2298, 0.1741, 0.1610],\n",
      "        [0.1213, 0.2996, 0.2333, 0.1809, 0.1649]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "77/100 Running Loss: 1.5571093559265137\n",
      "tensor([[0.1058, 0.3654, 0.2256, 0.1538, 0.1494],\n",
      "        [0.1151, 0.3181, 0.2331, 0.1729, 0.1608],\n",
      "        [0.1146, 0.3197, 0.2332, 0.1722, 0.1603],\n",
      "        [0.1075, 0.3449, 0.2324, 0.1609, 0.1542],\n",
      "        [0.1109, 0.3539, 0.2282, 0.1565, 0.1505],\n",
      "        [0.1051, 0.3701, 0.2268, 0.1508, 0.1473],\n",
      "        [0.1074, 0.3596, 0.2264, 0.1556, 0.1509],\n",
      "        [0.1056, 0.3684, 0.2258, 0.1518, 0.1484],\n",
      "        [0.1216, 0.3154, 0.2305, 0.1727, 0.1599],\n",
      "        [0.1200, 0.3021, 0.2341, 0.1798, 0.1640]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "78/100 Running Loss: 1.5556449890136719\n",
      "tensor([[0.1038, 0.3711, 0.2259, 0.1516, 0.1476],\n",
      "        [0.1136, 0.3213, 0.2339, 0.1716, 0.1597],\n",
      "        [0.1131, 0.3229, 0.2339, 0.1708, 0.1592],\n",
      "        [0.1057, 0.3495, 0.2331, 0.1591, 0.1527],\n",
      "        [0.1090, 0.3591, 0.2286, 0.1545, 0.1488],\n",
      "        [0.1030, 0.3759, 0.2271, 0.1485, 0.1454],\n",
      "        [0.1056, 0.3649, 0.2268, 0.1535, 0.1492],\n",
      "        [0.1036, 0.3742, 0.2260, 0.1496, 0.1466],\n",
      "        [0.1201, 0.3187, 0.2312, 0.1713, 0.1588],\n",
      "        [0.1186, 0.3047, 0.2349, 0.1787, 0.1631]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "79/100 Running Loss: 1.5541373491287231\n",
      "tensor([[0.1018, 0.3769, 0.2262, 0.1493, 0.1458],\n",
      "        [0.1120, 0.3246, 0.2346, 0.1702, 0.1586],\n",
      "        [0.1115, 0.3263, 0.2346, 0.1694, 0.1581],\n",
      "        [0.1039, 0.3543, 0.2337, 0.1571, 0.1511],\n",
      "        [0.1071, 0.3645, 0.2290, 0.1523, 0.1471],\n",
      "        [0.1010, 0.3821, 0.2274, 0.1461, 0.1435],\n",
      "        [0.1036, 0.3705, 0.2272, 0.1513, 0.1474],\n",
      "        [0.1016, 0.3801, 0.2263, 0.1473, 0.1447],\n",
      "        [0.1187, 0.3221, 0.2318, 0.1698, 0.1576],\n",
      "        [0.1172, 0.3074, 0.2357, 0.1776, 0.1622]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "80/100 Running Loss: 1.5525811910629272\n",
      "tensor([[0.0998, 0.3830, 0.2265, 0.1469, 0.1439],\n",
      "        [0.1105, 0.3280, 0.2353, 0.1688, 0.1574],\n",
      "        [0.1100, 0.3298, 0.2353, 0.1680, 0.1569],\n",
      "        [0.1020, 0.3591, 0.2343, 0.1551, 0.1494],\n",
      "        [0.1052, 0.3701, 0.2293, 0.1501, 0.1453],\n",
      "        [0.0989, 0.3885, 0.2275, 0.1436, 0.1414],\n",
      "        [0.1017, 0.3763, 0.2275, 0.1491, 0.1455],\n",
      "        [0.0995, 0.3864, 0.2265, 0.1449, 0.1427],\n",
      "        [0.1172, 0.3257, 0.2325, 0.1683, 0.1564],\n",
      "        [0.1158, 0.3102, 0.2365, 0.1764, 0.1612]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "81/100 Running Loss: 1.550970435142517\n",
      "tensor([[0.0977, 0.3894, 0.2266, 0.1444, 0.1418],\n",
      "        [0.1089, 0.3316, 0.2360, 0.1673, 0.1562],\n",
      "        [0.1084, 0.3334, 0.2360, 0.1665, 0.1557],\n",
      "        [0.1002, 0.3642, 0.2349, 0.1530, 0.1476],\n",
      "        [0.1032, 0.3759, 0.2296, 0.1479, 0.1434],\n",
      "        [0.0968, 0.3953, 0.2276, 0.1411, 0.1393],\n",
      "        [0.0997, 0.3823, 0.2278, 0.1467, 0.1435],\n",
      "        [0.0974, 0.3930, 0.2266, 0.1423, 0.1406],\n",
      "        [0.1156, 0.3294, 0.2331, 0.1668, 0.1551],\n",
      "        [0.1144, 0.3131, 0.2372, 0.1752, 0.1601]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "82/100 Running Loss: 1.5492957830429077\n",
      "tensor([[0.0956, 0.3962, 0.2267, 0.1418, 0.1397],\n",
      "        [0.1073, 0.3353, 0.2367, 0.1658, 0.1549],\n",
      "        [0.1068, 0.3372, 0.2367, 0.1650, 0.1544],\n",
      "        [0.0983, 0.3695, 0.2355, 0.1509, 0.1458],\n",
      "        [0.1012, 0.3821, 0.2297, 0.1455, 0.1414],\n",
      "        [0.0946, 0.4023, 0.2276, 0.1384, 0.1371],\n",
      "        [0.0976, 0.3887, 0.2279, 0.1443, 0.1415],\n",
      "        [0.0953, 0.4000, 0.2266, 0.1397, 0.1384],\n",
      "        [0.1141, 0.3333, 0.2337, 0.1652, 0.1537],\n",
      "        [0.1130, 0.3160, 0.2380, 0.1739, 0.1591]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "83/100 Running Loss: 1.5475624799728394\n",
      "tensor([[0.0935, 0.4032, 0.2267, 0.1392, 0.1374],\n",
      "        [0.1057, 0.3392, 0.2373, 0.1642, 0.1536],\n",
      "        [0.1051, 0.3412, 0.2373, 0.1634, 0.1530],\n",
      "        [0.0964, 0.3750, 0.2360, 0.1487, 0.1439],\n",
      "        [0.0992, 0.3885, 0.2298, 0.1431, 0.1394],\n",
      "        [0.0924, 0.4097, 0.2274, 0.1357, 0.1348],\n",
      "        [0.0956, 0.3954, 0.2280, 0.1417, 0.1393],\n",
      "        [0.0931, 0.4071, 0.2265, 0.1371, 0.1362],\n",
      "        [0.1125, 0.3373, 0.2342, 0.1636, 0.1524],\n",
      "        [0.1115, 0.3191, 0.2388, 0.1726, 0.1580]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "84/100 Running Loss: 1.5457762479782104\n",
      "tensor([[0.0913, 0.4105, 0.2266, 0.1365, 0.1351],\n",
      "        [0.1041, 0.3431, 0.2379, 0.1627, 0.1522],\n",
      "        [0.1035, 0.3453, 0.2378, 0.1618, 0.1516],\n",
      "        [0.0945, 0.3808, 0.2363, 0.1464, 0.1419],\n",
      "        [0.0971, 0.3952, 0.2298, 0.1406, 0.1373],\n",
      "        [0.0901, 0.4173, 0.2272, 0.1330, 0.1324],\n",
      "        [0.0935, 0.4023, 0.2280, 0.1392, 0.1371],\n",
      "        [0.0909, 0.4146, 0.2263, 0.1343, 0.1338],\n",
      "        [0.1109, 0.3414, 0.2348, 0.1619, 0.1510],\n",
      "        [0.1101, 0.3223, 0.2395, 0.1713, 0.1568]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "85/100 Running Loss: 1.5439387559890747\n",
      "tensor([[0.0891, 0.4181, 0.2264, 0.1337, 0.1327],\n",
      "        [0.1024, 0.3473, 0.2384, 0.1611, 0.1508],\n",
      "        [0.1018, 0.3496, 0.2383, 0.1602, 0.1502],\n",
      "        [0.0925, 0.3869, 0.2366, 0.1441, 0.1399],\n",
      "        [0.0951, 0.4021, 0.2298, 0.1380, 0.1351],\n",
      "        [0.0879, 0.4251, 0.2269, 0.1301, 0.1299],\n",
      "        [0.0914, 0.4094, 0.2279, 0.1365, 0.1348],\n",
      "        [0.0886, 0.4223, 0.2261, 0.1315, 0.1314],\n",
      "        [0.1093, 0.3457, 0.2353, 0.1602, 0.1495],\n",
      "        [0.1086, 0.3256, 0.2402, 0.1700, 0.1557]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "86/100 Running Loss: 1.5420507192611694\n",
      "tensor([[0.0869, 0.4259, 0.2261, 0.1309, 0.1302],\n",
      "        [0.1008, 0.3517, 0.2388, 0.1594, 0.1493],\n",
      "        [0.1002, 0.3540, 0.2387, 0.1585, 0.1486],\n",
      "        [0.0906, 0.3931, 0.2367, 0.1418, 0.1378],\n",
      "        [0.0929, 0.4093, 0.2296, 0.1354, 0.1328],\n",
      "        [0.0856, 0.4333, 0.2265, 0.1273, 0.1274],\n",
      "        [0.0893, 0.4168, 0.2277, 0.1339, 0.1324],\n",
      "        [0.0864, 0.4304, 0.2257, 0.1287, 0.1289],\n",
      "        [0.1077, 0.3503, 0.2357, 0.1584, 0.1479],\n",
      "        [0.1071, 0.3289, 0.2409, 0.1686, 0.1545]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "87/100 Running Loss: 1.5401033163070679\n",
      "tensor([[0.0846, 0.4340, 0.2256, 0.1280, 0.1277],\n",
      "        [0.0992, 0.3561, 0.2393, 0.1577, 0.1478],\n",
      "        [0.0985, 0.3584, 0.2391, 0.1568, 0.1471],\n",
      "        [0.0886, 0.3995, 0.2368, 0.1394, 0.1357],\n",
      "        [0.0908, 0.4167, 0.2293, 0.1327, 0.1305],\n",
      "        [0.0832, 0.4417, 0.2260, 0.1243, 0.1248],\n",
      "        [0.0871, 0.4245, 0.2274, 0.1311, 0.1300],\n",
      "        [0.0841, 0.4386, 0.2252, 0.1258, 0.1263],\n",
      "        [0.1060, 0.3549, 0.2361, 0.1566, 0.1464],\n",
      "        [0.1056, 0.3323, 0.2415, 0.1672, 0.1533]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "88/100 Running Loss: 1.5381042957305908\n",
      "tensor([[0.0824, 0.4424, 0.2250, 0.1251, 0.1251],\n",
      "        [0.0975, 0.3606, 0.2397, 0.1560, 0.1463],\n",
      "        [0.0968, 0.3630, 0.2395, 0.1550, 0.1456],\n",
      "        [0.0866, 0.4061, 0.2369, 0.1369, 0.1335],\n",
      "        [0.0886, 0.4244, 0.2290, 0.1300, 0.1281],\n",
      "        [0.0809, 0.4504, 0.2253, 0.1213, 0.1221],\n",
      "        [0.0849, 0.4323, 0.2269, 0.1283, 0.1275],\n",
      "        [0.0817, 0.4472, 0.2246, 0.1228, 0.1237],\n",
      "        [0.1044, 0.3596, 0.2365, 0.1548, 0.1448],\n",
      "        [0.1042, 0.3359, 0.2421, 0.1658, 0.1521]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "89/100 Running Loss: 1.536055326461792\n",
      "tensor([[0.0801, 0.4510, 0.2243, 0.1221, 0.1224],\n",
      "        [0.0958, 0.3651, 0.2401, 0.1543, 0.1447],\n",
      "        [0.0951, 0.3677, 0.2399, 0.1533, 0.1440],\n",
      "        [0.0846, 0.4128, 0.2369, 0.1345, 0.1313],\n",
      "        [0.0864, 0.4323, 0.2285, 0.1272, 0.1257],\n",
      "        [0.0785, 0.4595, 0.2244, 0.1182, 0.1194],\n",
      "        [0.0827, 0.4404, 0.2264, 0.1254, 0.1250],\n",
      "        [0.0794, 0.4561, 0.2238, 0.1198, 0.1210],\n",
      "        [0.1027, 0.3645, 0.2368, 0.1529, 0.1431],\n",
      "        [0.1027, 0.3395, 0.2427, 0.1643, 0.1508]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "90/100 Running Loss: 1.5339539051055908\n",
      "tensor([[0.0778, 0.4600, 0.2235, 0.1191, 0.1197],\n",
      "        [0.0942, 0.3697, 0.2404, 0.1525, 0.1432],\n",
      "        [0.0934, 0.3725, 0.2403, 0.1514, 0.1424],\n",
      "        [0.0826, 0.4195, 0.2369, 0.1320, 0.1291],\n",
      "        [0.0842, 0.4405, 0.2278, 0.1244, 0.1232],\n",
      "        [0.0761, 0.4688, 0.2234, 0.1151, 0.1165],\n",
      "        [0.0805, 0.4488, 0.2258, 0.1225, 0.1224],\n",
      "        [0.0770, 0.4652, 0.2228, 0.1167, 0.1182],\n",
      "        [0.1010, 0.3695, 0.2371, 0.1510, 0.1415],\n",
      "        [0.1012, 0.3431, 0.2433, 0.1629, 0.1495]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "91/100 Running Loss: 1.5318018198013306\n",
      "tensor([[0.0754, 0.4692, 0.2225, 0.1160, 0.1169],\n",
      "        [0.0925, 0.3745, 0.2407, 0.1507, 0.1416],\n",
      "        [0.0917, 0.3774, 0.2406, 0.1496, 0.1407],\n",
      "        [0.0805, 0.4265, 0.2367, 0.1294, 0.1268],\n",
      "        [0.0819, 0.4489, 0.2271, 0.1215, 0.1206],\n",
      "        [0.0737, 0.4784, 0.2223, 0.1120, 0.1137],\n",
      "        [0.0782, 0.4575, 0.2250, 0.1196, 0.1197],\n",
      "        [0.0747, 0.4746, 0.2217, 0.1136, 0.1154],\n",
      "        [0.0993, 0.3746, 0.2373, 0.1490, 0.1398],\n",
      "        [0.0998, 0.3468, 0.2438, 0.1614, 0.1482]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "92/100 Running Loss: 1.5296015739440918\n",
      "tensor([[0.0731, 0.4787, 0.2214, 0.1128, 0.1140],\n",
      "        [0.0908, 0.3794, 0.2410, 0.1489, 0.1399],\n",
      "        [0.0900, 0.3824, 0.2408, 0.1477, 0.1391],\n",
      "        [0.0785, 0.4336, 0.2365, 0.1269, 0.1245],\n",
      "        [0.0797, 0.4576, 0.2262, 0.1186, 0.1179],\n",
      "        [0.0713, 0.4882, 0.2210, 0.1087, 0.1107],\n",
      "        [0.0760, 0.4665, 0.2241, 0.1166, 0.1169],\n",
      "        [0.0723, 0.4844, 0.2205, 0.1104, 0.1125],\n",
      "        [0.0975, 0.3799, 0.2375, 0.1470, 0.1380],\n",
      "        [0.0983, 0.3506, 0.2443, 0.1599, 0.1469]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "93/100 Running Loss: 1.5273607969284058\n",
      "tensor([[0.0707, 0.4884, 0.2202, 0.1096, 0.1111],\n",
      "        [0.0891, 0.3844, 0.2413, 0.1470, 0.1383],\n",
      "        [0.0883, 0.3875, 0.2410, 0.1458, 0.1374],\n",
      "        [0.0765, 0.4410, 0.2361, 0.1242, 0.1221],\n",
      "        [0.0774, 0.4666, 0.2252, 0.1156, 0.1153],\n",
      "        [0.0689, 0.4983, 0.2196, 0.1054, 0.1077],\n",
      "        [0.0737, 0.4757, 0.2230, 0.1135, 0.1142],\n",
      "        [0.0699, 0.4942, 0.2192, 0.1072, 0.1095],\n",
      "        [0.0958, 0.3854, 0.2375, 0.1450, 0.1363],\n",
      "        [0.0968, 0.3545, 0.2448, 0.1583, 0.1455]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "94/100 Running Loss: 1.5250788927078247\n",
      "tensor([[0.0683, 0.4984, 0.2189, 0.1063, 0.1081],\n",
      "        [0.0875, 0.3894, 0.2415, 0.1451, 0.1366],\n",
      "        [0.0866, 0.3927, 0.2412, 0.1438, 0.1356],\n",
      "        [0.0745, 0.4485, 0.2357, 0.1216, 0.1197],\n",
      "        [0.0751, 0.4757, 0.2241, 0.1126, 0.1125],\n",
      "        [0.0665, 0.5086, 0.2181, 0.1021, 0.1047],\n",
      "        [0.0714, 0.4852, 0.2217, 0.1104, 0.1113],\n",
      "        [0.0675, 0.5044, 0.2177, 0.1039, 0.1065],\n",
      "        [0.0940, 0.3910, 0.2376, 0.1429, 0.1344],\n",
      "        [0.0953, 0.3585, 0.2453, 0.1567, 0.1441]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "95/100 Running Loss: 1.5227546691894531\n",
      "tensor([[0.0660, 0.5087, 0.2173, 0.1030, 0.1051],\n",
      "        [0.0858, 0.3947, 0.2416, 0.1432, 0.1348],\n",
      "        [0.0849, 0.3981, 0.2413, 0.1419, 0.1339],\n",
      "        [0.0724, 0.4562, 0.2352, 0.1189, 0.1173],\n",
      "        [0.0728, 0.4851, 0.2229, 0.1094, 0.1097],\n",
      "        [0.0641, 0.5192, 0.2164, 0.0987, 0.1016],\n",
      "        [0.0691, 0.4950, 0.2203, 0.1072, 0.1084],\n",
      "        [0.0651, 0.5148, 0.2161, 0.1005, 0.1034],\n",
      "        [0.0923, 0.3968, 0.2375, 0.1408, 0.1326],\n",
      "        [0.0938, 0.3625, 0.2458, 0.1551, 0.1427]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "96/100 Running Loss: 1.5203871726989746\n",
      "tensor([[0.0636, 0.5192, 0.2156, 0.0997, 0.1020],\n",
      "        [0.0841, 0.4000, 0.2416, 0.1412, 0.1331],\n",
      "        [0.0832, 0.4035, 0.2413, 0.1399, 0.1321],\n",
      "        [0.0704, 0.4641, 0.2345, 0.1161, 0.1148],\n",
      "        [0.0705, 0.4947, 0.2216, 0.1063, 0.1069],\n",
      "        [0.0617, 0.5301, 0.2144, 0.0954, 0.0985],\n",
      "        [0.0668, 0.5050, 0.2188, 0.1040, 0.1055],\n",
      "        [0.0627, 0.5255, 0.2142, 0.0972, 0.1004],\n",
      "        [0.0905, 0.4028, 0.2374, 0.1387, 0.1307],\n",
      "        [0.0924, 0.3665, 0.2462, 0.1536, 0.1413]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "97/100 Running Loss: 1.5179802179336548\n",
      "tensor([[0.0612, 0.5299, 0.2136, 0.0963, 0.0989],\n",
      "        [0.0824, 0.4053, 0.2416, 0.1393, 0.1313],\n",
      "        [0.0815, 0.4091, 0.2412, 0.1379, 0.1303],\n",
      "        [0.0683, 0.4722, 0.2337, 0.1133, 0.1123],\n",
      "        [0.0682, 0.5046, 0.2200, 0.1032, 0.1041],\n",
      "        [0.0593, 0.5411, 0.2123, 0.0920, 0.0953],\n",
      "        [0.0645, 0.5152, 0.2170, 0.1007, 0.1025],\n",
      "        [0.0604, 0.5364, 0.2122, 0.0938, 0.0972],\n",
      "        [0.0887, 0.4088, 0.2373, 0.1365, 0.1287],\n",
      "        [0.0909, 0.3705, 0.2467, 0.1520, 0.1399]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "98/100 Running Loss: 1.515535593032837\n",
      "tensor([[0.0589, 0.5409, 0.2115, 0.0929, 0.0958],\n",
      "        [0.0808, 0.4107, 0.2416, 0.1373, 0.1296],\n",
      "        [0.0798, 0.4147, 0.2412, 0.1359, 0.1285],\n",
      "        [0.0663, 0.4804, 0.2328, 0.1106, 0.1099],\n",
      "        [0.0659, 0.5146, 0.2183, 0.1000, 0.1012],\n",
      "        [0.0569, 0.5524, 0.2100, 0.0886, 0.0921],\n",
      "        [0.0622, 0.5256, 0.2152, 0.0975, 0.0995],\n",
      "        [0.0580, 0.5475, 0.2100, 0.0905, 0.0941],\n",
      "        [0.0869, 0.4150, 0.2370, 0.1343, 0.1268],\n",
      "        [0.0895, 0.3745, 0.2471, 0.1504, 0.1385]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "99/100 Running Loss: 1.5130568742752075\n",
      "tensor([[0.0565, 0.5520, 0.2092, 0.0896, 0.0927],\n",
      "        [0.0791, 0.4161, 0.2416, 0.1354, 0.1278],\n",
      "        [0.0782, 0.4202, 0.2410, 0.1339, 0.1267],\n",
      "        [0.0643, 0.4886, 0.2318, 0.1079, 0.1074],\n",
      "        [0.0636, 0.5248, 0.2165, 0.0968, 0.0983],\n",
      "        [0.0545, 0.5638, 0.2075, 0.0852, 0.0890],\n",
      "        [0.0599, 0.5362, 0.2131, 0.0942, 0.0965],\n",
      "        [0.0556, 0.5588, 0.2076, 0.0871, 0.0909],\n",
      "        [0.0851, 0.4212, 0.2368, 0.1321, 0.1248],\n",
      "        [0.0881, 0.3785, 0.2474, 0.1489, 0.1371]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "100/100 Running Loss: 1.510545015335083\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.39      1.00      0.56        21\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.00      0.00      0.00        11\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "   micro avg       0.39      0.39      0.39        54\n",
      "   macro avg       0.10      0.25      0.14        54\n",
      "weighted avg       0.15      0.39      0.22        54\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14\n",
      "tensor([[0.2063, 0.2158, 0.1770, 0.1984, 0.2026],\n",
      "        [0.2080, 0.2168, 0.1749, 0.1984, 0.2019],\n",
      "        [0.2079, 0.2167, 0.1750, 0.1985, 0.2019],\n",
      "        [0.2085, 0.2158, 0.1748, 0.1979, 0.2030],\n",
      "        [0.2071, 0.2169, 0.1753, 0.1984, 0.2024],\n",
      "        [0.2074, 0.2161, 0.1756, 0.1988, 0.2021],\n",
      "        [0.2075, 0.2168, 0.1755, 0.1985, 0.2017],\n",
      "        [0.2073, 0.2160, 0.1759, 0.1988, 0.2020],\n",
      "        [0.2068, 0.2170, 0.1755, 0.1983, 0.2024],\n",
      "        [0.2075, 0.2167, 0.1751, 0.1981, 0.2026]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "1/100 Running Loss: 1.6102664470672607\n",
      "tensor([[0.2057, 0.2162, 0.1771, 0.1985, 0.2024],\n",
      "        [0.2074, 0.2171, 0.1751, 0.1987, 0.2018],\n",
      "        [0.2073, 0.2171, 0.1752, 0.1987, 0.2017],\n",
      "        [0.2078, 0.2161, 0.1750, 0.1981, 0.2029],\n",
      "        [0.2066, 0.2173, 0.1755, 0.1985, 0.2022],\n",
      "        [0.2068, 0.2166, 0.1758, 0.1989, 0.2019],\n",
      "        [0.2070, 0.2172, 0.1757, 0.1986, 0.2015],\n",
      "        [0.2068, 0.2165, 0.1760, 0.1990, 0.2018],\n",
      "        [0.2063, 0.2173, 0.1757, 0.1984, 0.2023],\n",
      "        [0.2069, 0.2170, 0.1754, 0.1983, 0.2024]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "2/100 Running Loss: 1.610027551651001\n",
      "tensor([[0.2051, 0.2167, 0.1773, 0.1987, 0.2022],\n",
      "        [0.2067, 0.2175, 0.1753, 0.1989, 0.2016],\n",
      "        [0.2067, 0.2175, 0.1753, 0.1990, 0.2016],\n",
      "        [0.2072, 0.2164, 0.1753, 0.1984, 0.2028],\n",
      "        [0.2061, 0.2176, 0.1756, 0.1986, 0.2020],\n",
      "        [0.2063, 0.2170, 0.1760, 0.1991, 0.2017],\n",
      "        [0.2065, 0.2176, 0.1758, 0.1988, 0.2013],\n",
      "        [0.2062, 0.2169, 0.1762, 0.1991, 0.2016],\n",
      "        [0.2059, 0.2176, 0.1758, 0.1986, 0.2021],\n",
      "        [0.2063, 0.2173, 0.1756, 0.1985, 0.2022]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "3/100 Running Loss: 1.6097888946533203\n",
      "tensor([[0.2046, 0.2172, 0.1774, 0.1989, 0.2019],\n",
      "        [0.2061, 0.2178, 0.1755, 0.1991, 0.2015],\n",
      "        [0.2061, 0.2178, 0.1755, 0.1992, 0.2014],\n",
      "        [0.2065, 0.2167, 0.1756, 0.1986, 0.2026],\n",
      "        [0.2057, 0.2180, 0.1758, 0.1987, 0.2017],\n",
      "        [0.2058, 0.2174, 0.1761, 0.1992, 0.2015],\n",
      "        [0.2060, 0.2180, 0.1760, 0.1989, 0.2011],\n",
      "        [0.2057, 0.2173, 0.1764, 0.1992, 0.2013],\n",
      "        [0.2054, 0.2179, 0.1761, 0.1987, 0.2019],\n",
      "        [0.2057, 0.2176, 0.1759, 0.1987, 0.2021]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "4/100 Running Loss: 1.6095494031906128\n",
      "tensor([[0.2041, 0.2177, 0.1776, 0.1990, 0.2016],\n",
      "        [0.2055, 0.2181, 0.1757, 0.1994, 0.2014],\n",
      "        [0.2054, 0.2181, 0.1757, 0.1994, 0.2013],\n",
      "        [0.2058, 0.2170, 0.1759, 0.1988, 0.2024],\n",
      "        [0.2052, 0.2184, 0.1760, 0.1988, 0.2015],\n",
      "        [0.2053, 0.2178, 0.1763, 0.1993, 0.2012],\n",
      "        [0.2056, 0.2184, 0.1762, 0.1990, 0.2008],\n",
      "        [0.2052, 0.2178, 0.1765, 0.1994, 0.2011],\n",
      "        [0.2049, 0.2182, 0.1763, 0.1989, 0.2017],\n",
      "        [0.2051, 0.2179, 0.1762, 0.1990, 0.2019]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "5/100 Running Loss: 1.609309196472168\n",
      "tensor([[0.2035, 0.2181, 0.1778, 0.1992, 0.2014],\n",
      "        [0.2049, 0.2184, 0.1759, 0.1996, 0.2012],\n",
      "        [0.2048, 0.2184, 0.1759, 0.1997, 0.2012],\n",
      "        [0.2052, 0.2174, 0.1761, 0.1991, 0.2022],\n",
      "        [0.2047, 0.2187, 0.1763, 0.1990, 0.2013],\n",
      "        [0.2048, 0.2183, 0.1765, 0.1994, 0.2010],\n",
      "        [0.2051, 0.2187, 0.1764, 0.1992, 0.2006],\n",
      "        [0.2047, 0.2182, 0.1767, 0.1995, 0.2009],\n",
      "        [0.2045, 0.2186, 0.1765, 0.1990, 0.2015],\n",
      "        [0.2045, 0.2182, 0.1764, 0.1992, 0.2017]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "6/100 Running Loss: 1.6090716123580933\n",
      "tensor([[0.2030, 0.2185, 0.1780, 0.1993, 0.2012],\n",
      "        [0.2043, 0.2187, 0.1762, 0.1998, 0.2010],\n",
      "        [0.2042, 0.2187, 0.1762, 0.1999, 0.2010],\n",
      "        [0.2045, 0.2177, 0.1764, 0.1993, 0.2020],\n",
      "        [0.2042, 0.2191, 0.1765, 0.1992, 0.2010],\n",
      "        [0.2043, 0.2187, 0.1767, 0.1996, 0.2007],\n",
      "        [0.2046, 0.2191, 0.1766, 0.1993, 0.2004],\n",
      "        [0.2042, 0.2186, 0.1769, 0.1996, 0.2006],\n",
      "        [0.2040, 0.2189, 0.1767, 0.1992, 0.2012],\n",
      "        [0.2039, 0.2185, 0.1767, 0.1994, 0.2015]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "7/100 Running Loss: 1.6088348627090454\n",
      "tensor([[0.2025, 0.2189, 0.1782, 0.1995, 0.2009],\n",
      "        [0.2036, 0.2190, 0.1764, 0.2001, 0.2009],\n",
      "        [0.2035, 0.2190, 0.1765, 0.2002, 0.2008],\n",
      "        [0.2039, 0.2180, 0.1767, 0.1996, 0.2018],\n",
      "        [0.2037, 0.2194, 0.1767, 0.1993, 0.2008],\n",
      "        [0.2038, 0.2191, 0.1769, 0.1997, 0.2005],\n",
      "        [0.2042, 0.2194, 0.1767, 0.1995, 0.2002],\n",
      "        [0.2038, 0.2190, 0.1771, 0.1997, 0.2004],\n",
      "        [0.2036, 0.2192, 0.1769, 0.1993, 0.2010],\n",
      "        [0.2033, 0.2188, 0.1770, 0.1996, 0.2012]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "8/100 Running Loss: 1.608599305152893\n",
      "tensor([[0.2020, 0.2193, 0.1784, 0.1997, 0.2007],\n",
      "        [0.2030, 0.2193, 0.1767, 0.2004, 0.2007],\n",
      "        [0.2029, 0.2193, 0.1767, 0.2004, 0.2006],\n",
      "        [0.2032, 0.2184, 0.1770, 0.1998, 0.2016],\n",
      "        [0.2033, 0.2198, 0.1769, 0.1995, 0.2005],\n",
      "        [0.2033, 0.2196, 0.1771, 0.1998, 0.2002],\n",
      "        [0.2037, 0.2198, 0.1769, 0.1996, 0.1999],\n",
      "        [0.2033, 0.2194, 0.1772, 0.1999, 0.2002],\n",
      "        [0.2031, 0.2195, 0.1771, 0.1995, 0.2008],\n",
      "        [0.2027, 0.2191, 0.1772, 0.1999, 0.2010]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "9/100 Running Loss: 1.6083654165267944\n",
      "tensor([[0.2015, 0.2196, 0.1786, 0.1999, 0.2004],\n",
      "        [0.2023, 0.2196, 0.1770, 0.2007, 0.2005],\n",
      "        [0.2022, 0.2196, 0.1770, 0.2007, 0.2004],\n",
      "        [0.2026, 0.2187, 0.1773, 0.2001, 0.2013],\n",
      "        [0.2027, 0.2201, 0.1772, 0.1997, 0.2003],\n",
      "        [0.2028, 0.2200, 0.1773, 0.2000, 0.2000],\n",
      "        [0.2033, 0.2201, 0.1771, 0.1998, 0.1997],\n",
      "        [0.2028, 0.2198, 0.1774, 0.2001, 0.1999],\n",
      "        [0.2026, 0.2198, 0.1773, 0.1997, 0.2006],\n",
      "        [0.2021, 0.2194, 0.1775, 0.2001, 0.2008]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "10/100 Running Loss: 1.6081337928771973\n",
      "tensor([[0.2010, 0.2200, 0.1788, 0.2001, 0.2002],\n",
      "        [0.2017, 0.2199, 0.1772, 0.2010, 0.2002],\n",
      "        [0.2016, 0.2199, 0.1773, 0.2010, 0.2002],\n",
      "        [0.2020, 0.2190, 0.1776, 0.2003, 0.2011],\n",
      "        [0.2022, 0.2205, 0.1774, 0.1998, 0.2000],\n",
      "        [0.2022, 0.2203, 0.1775, 0.2002, 0.1998],\n",
      "        [0.2028, 0.2205, 0.1773, 0.1999, 0.1995],\n",
      "        [0.2023, 0.2201, 0.1776, 0.2002, 0.1997],\n",
      "        [0.2022, 0.2201, 0.1775, 0.1998, 0.2004],\n",
      "        [0.2015, 0.2198, 0.1777, 0.2003, 0.2006]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "11/100 Running Loss: 1.6079021692276\n",
      "tensor([[0.2005, 0.2203, 0.1790, 0.2003, 0.1999],\n",
      "        [0.2010, 0.2202, 0.1775, 0.2013, 0.2000],\n",
      "        [0.2010, 0.2202, 0.1775, 0.2013, 0.2000],\n",
      "        [0.2013, 0.2194, 0.1779, 0.2005, 0.2009],\n",
      "        [0.2017, 0.2208, 0.1777, 0.2000, 0.1998],\n",
      "        [0.2017, 0.2207, 0.1777, 0.2004, 0.1995],\n",
      "        [0.2024, 0.2209, 0.1775, 0.2000, 0.1993],\n",
      "        [0.2019, 0.2205, 0.1778, 0.2004, 0.1995],\n",
      "        [0.2017, 0.2204, 0.1778, 0.2000, 0.2001],\n",
      "        [0.2009, 0.2201, 0.1780, 0.2006, 0.2004]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "12/100 Running Loss: 1.607673168182373\n",
      "tensor([[0.2000, 0.2207, 0.1792, 0.2004, 0.1996],\n",
      "        [0.2004, 0.2205, 0.1778, 0.2016, 0.1998],\n",
      "        [0.2004, 0.2205, 0.1778, 0.2016, 0.1998],\n",
      "        [0.2007, 0.2197, 0.1782, 0.2007, 0.2006],\n",
      "        [0.2012, 0.2211, 0.1779, 0.2002, 0.1996],\n",
      "        [0.2012, 0.2210, 0.1779, 0.2006, 0.1993],\n",
      "        [0.2019, 0.2212, 0.1777, 0.2002, 0.1990],\n",
      "        [0.2014, 0.2209, 0.1780, 0.2006, 0.1992],\n",
      "        [0.2012, 0.2208, 0.1780, 0.2001, 0.1999],\n",
      "        [0.2003, 0.2204, 0.1783, 0.2008, 0.2003]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "13/100 Running Loss: 1.6074435710906982\n",
      "tensor([[0.1995, 0.2211, 0.1794, 0.2006, 0.1993],\n",
      "        [0.1998, 0.2208, 0.1781, 0.2018, 0.1996],\n",
      "        [0.1997, 0.2207, 0.1781, 0.2019, 0.1995],\n",
      "        [0.2002, 0.2200, 0.1785, 0.2009, 0.2004],\n",
      "        [0.2007, 0.2214, 0.1782, 0.2004, 0.1993],\n",
      "        [0.2006, 0.2213, 0.1782, 0.2008, 0.1991],\n",
      "        [0.2015, 0.2216, 0.1778, 0.2003, 0.1988],\n",
      "        [0.2008, 0.2212, 0.1782, 0.2008, 0.1990],\n",
      "        [0.2007, 0.2211, 0.1783, 0.2003, 0.1997],\n",
      "        [0.1998, 0.2206, 0.1785, 0.2010, 0.2000]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "14/100 Running Loss: 1.6072131395339966\n",
      "tensor([[0.1990, 0.2215, 0.1796, 0.2009, 0.1991],\n",
      "        [0.1991, 0.2211, 0.1784, 0.2021, 0.1993],\n",
      "        [0.1991, 0.2210, 0.1784, 0.2022, 0.1993],\n",
      "        [0.1996, 0.2203, 0.1788, 0.2011, 0.2002],\n",
      "        [0.2002, 0.2217, 0.1785, 0.2005, 0.1991],\n",
      "        [0.2001, 0.2217, 0.1784, 0.2010, 0.1988],\n",
      "        [0.2010, 0.2219, 0.1780, 0.2004, 0.1986],\n",
      "        [0.2003, 0.2216, 0.1784, 0.2010, 0.1987],\n",
      "        [0.2002, 0.2214, 0.1786, 0.2004, 0.1994],\n",
      "        [0.1992, 0.2209, 0.1788, 0.2012, 0.1998]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "15/100 Running Loss: 1.6069871187210083\n",
      "tensor([[0.1985, 0.2218, 0.1799, 0.2011, 0.1988],\n",
      "        [0.1985, 0.2213, 0.1787, 0.2024, 0.1991],\n",
      "        [0.1985, 0.2213, 0.1787, 0.2024, 0.1990],\n",
      "        [0.1990, 0.2206, 0.1792, 0.2013, 0.1999],\n",
      "        [0.1996, 0.2221, 0.1788, 0.2007, 0.1989],\n",
      "        [0.1995, 0.2220, 0.1787, 0.2012, 0.1986],\n",
      "        [0.2006, 0.2222, 0.1783, 0.2005, 0.1984],\n",
      "        [0.1997, 0.2220, 0.1787, 0.2012, 0.1985],\n",
      "        [0.1997, 0.2217, 0.1788, 0.2006, 0.1992],\n",
      "        [0.1987, 0.2212, 0.1791, 0.2014, 0.1996]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "16/100 Running Loss: 1.6067638397216797\n",
      "tensor([[0.1980, 0.2222, 0.1801, 0.2013, 0.1985],\n",
      "        [0.1980, 0.2216, 0.1790, 0.2027, 0.1988],\n",
      "        [0.1979, 0.2216, 0.1790, 0.2027, 0.1988],\n",
      "        [0.1984, 0.2208, 0.1795, 0.2015, 0.1997],\n",
      "        [0.1991, 0.2224, 0.1790, 0.2009, 0.1986],\n",
      "        [0.1990, 0.2223, 0.1789, 0.2014, 0.1983],\n",
      "        [0.2002, 0.2226, 0.1785, 0.2006, 0.1982],\n",
      "        [0.1992, 0.2224, 0.1789, 0.2014, 0.1982],\n",
      "        [0.1992, 0.2219, 0.1791, 0.2008, 0.1990],\n",
      "        [0.1981, 0.2214, 0.1794, 0.2016, 0.1994]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "17/100 Running Loss: 1.6065412759780884\n",
      "tensor([[0.1975, 0.2225, 0.1803, 0.2015, 0.1982],\n",
      "        [0.1974, 0.2218, 0.1793, 0.2029, 0.1986],\n",
      "        [0.1973, 0.2218, 0.1793, 0.2030, 0.1985],\n",
      "        [0.1978, 0.2211, 0.1799, 0.2017, 0.1995],\n",
      "        [0.1987, 0.2226, 0.1793, 0.2011, 0.1983],\n",
      "        [0.1985, 0.2227, 0.1792, 0.2016, 0.1980],\n",
      "        [0.1998, 0.2229, 0.1786, 0.2007, 0.1980],\n",
      "        [0.1987, 0.2227, 0.1791, 0.2016, 0.1979],\n",
      "        [0.1987, 0.2222, 0.1794, 0.2010, 0.1987],\n",
      "        [0.1976, 0.2217, 0.1797, 0.2018, 0.1992]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "18/100 Running Loss: 1.6063226461410522\n",
      "tensor([[0.1970, 0.2229, 0.1806, 0.2017, 0.1979],\n",
      "        [0.1968, 0.2221, 0.1796, 0.2032, 0.1983],\n",
      "        [0.1968, 0.2221, 0.1797, 0.2032, 0.1983],\n",
      "        [0.1972, 0.2213, 0.1802, 0.2020, 0.1993],\n",
      "        [0.1982, 0.2229, 0.1796, 0.2012, 0.1981],\n",
      "        [0.1980, 0.2230, 0.1795, 0.2018, 0.1977],\n",
      "        [0.1994, 0.2232, 0.1789, 0.2009, 0.1978],\n",
      "        [0.1982, 0.2230, 0.1794, 0.2018, 0.1976],\n",
      "        [0.1983, 0.2224, 0.1797, 0.2012, 0.1985],\n",
      "        [0.1971, 0.2219, 0.1800, 0.2020, 0.1990]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "19/100 Running Loss: 1.6061103343963623\n",
      "tensor([[0.1965, 0.2232, 0.1809, 0.2018, 0.1976],\n",
      "        [0.1963, 0.2223, 0.1800, 0.2034, 0.1980],\n",
      "        [0.1962, 0.2223, 0.1800, 0.2035, 0.1980],\n",
      "        [0.1966, 0.2216, 0.1806, 0.2022, 0.1991],\n",
      "        [0.1977, 0.2232, 0.1799, 0.2014, 0.1978],\n",
      "        [0.1975, 0.2233, 0.1798, 0.2020, 0.1974],\n",
      "        [0.1990, 0.2234, 0.1791, 0.2010, 0.1975],\n",
      "        [0.1977, 0.2234, 0.1797, 0.2020, 0.1973],\n",
      "        [0.1978, 0.2227, 0.1800, 0.2013, 0.1982],\n",
      "        [0.1965, 0.2221, 0.1804, 0.2021, 0.1988]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "20/100 Running Loss: 1.6058998107910156\n",
      "tensor([[0.1960, 0.2235, 0.1812, 0.2020, 0.1973],\n",
      "        [0.1957, 0.2225, 0.1803, 0.2037, 0.1978],\n",
      "        [0.1956, 0.2225, 0.1804, 0.2037, 0.1978],\n",
      "        [0.1960, 0.2218, 0.1810, 0.2024, 0.1988],\n",
      "        [0.1972, 0.2234, 0.1802, 0.2016, 0.1976],\n",
      "        [0.1970, 0.2236, 0.1801, 0.2022, 0.1971],\n",
      "        [0.1985, 0.2237, 0.1793, 0.2011, 0.1973],\n",
      "        [0.1972, 0.2237, 0.1800, 0.2021, 0.1970],\n",
      "        [0.1973, 0.2229, 0.1803, 0.2015, 0.1980],\n",
      "        [0.1960, 0.2223, 0.1807, 0.2023, 0.1986]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "21/100 Running Loss: 1.6056855916976929\n",
      "tensor([[0.1955, 0.2239, 0.1815, 0.2022, 0.1970],\n",
      "        [0.1951, 0.2227, 0.1807, 0.2039, 0.1976],\n",
      "        [0.1950, 0.2227, 0.1808, 0.2039, 0.1976],\n",
      "        [0.1954, 0.2220, 0.1814, 0.2026, 0.1986],\n",
      "        [0.1968, 0.2237, 0.1806, 0.2017, 0.1973],\n",
      "        [0.1965, 0.2239, 0.1805, 0.2024, 0.1968],\n",
      "        [0.1981, 0.2241, 0.1796, 0.2012, 0.1970],\n",
      "        [0.1967, 0.2240, 0.1803, 0.2023, 0.1967],\n",
      "        [0.1969, 0.2231, 0.1806, 0.2016, 0.1977],\n",
      "        [0.1955, 0.2225, 0.1811, 0.2025, 0.1984]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "22/100 Running Loss: 1.605472207069397\n",
      "tensor([[0.1950, 0.2242, 0.1818, 0.2023, 0.1967],\n",
      "        [0.1945, 0.2229, 0.1811, 0.2041, 0.1973],\n",
      "        [0.1945, 0.2229, 0.1812, 0.2042, 0.1973],\n",
      "        [0.1948, 0.2222, 0.1818, 0.2028, 0.1984],\n",
      "        [0.1963, 0.2240, 0.1809, 0.2019, 0.1970],\n",
      "        [0.1959, 0.2242, 0.1808, 0.2025, 0.1965],\n",
      "        [0.1977, 0.2244, 0.1798, 0.2014, 0.1968],\n",
      "        [0.1962, 0.2243, 0.1807, 0.2025, 0.1964],\n",
      "        [0.1964, 0.2234, 0.1810, 0.2018, 0.1975],\n",
      "        [0.1949, 0.2227, 0.1815, 0.2027, 0.1982]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "23/100 Running Loss: 1.6052556037902832\n",
      "tensor([[0.1946, 0.2245, 0.1821, 0.2025, 0.1964],\n",
      "        [0.1940, 0.2231, 0.1815, 0.2044, 0.1971],\n",
      "        [0.1939, 0.2231, 0.1815, 0.2044, 0.1971],\n",
      "        [0.1942, 0.2225, 0.1822, 0.2030, 0.1981],\n",
      "        [0.1958, 0.2242, 0.1812, 0.2020, 0.1967],\n",
      "        [0.1954, 0.2245, 0.1812, 0.2027, 0.1962],\n",
      "        [0.1972, 0.2247, 0.1801, 0.2015, 0.1965],\n",
      "        [0.1957, 0.2246, 0.1810, 0.2026, 0.1961],\n",
      "        [0.1959, 0.2236, 0.1813, 0.2019, 0.1972],\n",
      "        [0.1944, 0.2229, 0.1819, 0.2029, 0.1979]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "24/100 Running Loss: 1.6050373315811157\n",
      "tensor([[0.1941, 0.2248, 0.1824, 0.2026, 0.1961],\n",
      "        [0.1934, 0.2233, 0.1819, 0.2046, 0.1968],\n",
      "        [0.1933, 0.2234, 0.1819, 0.2046, 0.1968],\n",
      "        [0.1936, 0.2227, 0.1826, 0.2032, 0.1979],\n",
      "        [0.1953, 0.2245, 0.1816, 0.2022, 0.1964],\n",
      "        [0.1949, 0.2248, 0.1815, 0.2028, 0.1959],\n",
      "        [0.1968, 0.2250, 0.1804, 0.2016, 0.1962],\n",
      "        [0.1952, 0.2249, 0.1814, 0.2028, 0.1958],\n",
      "        [0.1954, 0.2239, 0.1816, 0.2021, 0.1970],\n",
      "        [0.1939, 0.2231, 0.1822, 0.2031, 0.1977]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "25/100 Running Loss: 1.6048164367675781\n",
      "tensor([[0.1937, 0.2251, 0.1826, 0.2028, 0.1957],\n",
      "        [0.1928, 0.2235, 0.1823, 0.2048, 0.1966],\n",
      "        [0.1927, 0.2236, 0.1823, 0.2049, 0.1966],\n",
      "        [0.1931, 0.2228, 0.1830, 0.2035, 0.1976],\n",
      "        [0.1948, 0.2248, 0.1819, 0.2024, 0.1961],\n",
      "        [0.1944, 0.2251, 0.1819, 0.2030, 0.1956],\n",
      "        [0.1963, 0.2253, 0.1807, 0.2018, 0.1959],\n",
      "        [0.1946, 0.2252, 0.1817, 0.2030, 0.1955],\n",
      "        [0.1950, 0.2241, 0.1820, 0.2022, 0.1967],\n",
      "        [0.1933, 0.2234, 0.1826, 0.2033, 0.1974]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "26/100 Running Loss: 1.6045936346054077\n",
      "tensor([[0.1932, 0.2255, 0.1829, 0.2030, 0.1954],\n",
      "        [0.1922, 0.2238, 0.1826, 0.2050, 0.1963],\n",
      "        [0.1922, 0.2238, 0.1827, 0.2050, 0.1963],\n",
      "        [0.1926, 0.2230, 0.1833, 0.2037, 0.1974],\n",
      "        [0.1943, 0.2251, 0.1823, 0.2025, 0.1957],\n",
      "        [0.1939, 0.2254, 0.1822, 0.2032, 0.1953],\n",
      "        [0.1959, 0.2256, 0.1810, 0.2019, 0.1956],\n",
      "        [0.1942, 0.2255, 0.1821, 0.2032, 0.1952],\n",
      "        [0.1945, 0.2244, 0.1823, 0.2024, 0.1964],\n",
      "        [0.1927, 0.2236, 0.1830, 0.2035, 0.1972]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "27/100 Running Loss: 1.6043709516525269\n",
      "tensor([[0.1928, 0.2258, 0.1832, 0.2031, 0.1951],\n",
      "        [0.1917, 0.2240, 0.1830, 0.2052, 0.1961],\n",
      "        [0.1917, 0.2240, 0.1830, 0.2052, 0.1961],\n",
      "        [0.1921, 0.2232, 0.1837, 0.2039, 0.1972],\n",
      "        [0.1938, 0.2254, 0.1827, 0.2027, 0.1954],\n",
      "        [0.1935, 0.2257, 0.1826, 0.2034, 0.1950],\n",
      "        [0.1954, 0.2260, 0.1813, 0.2021, 0.1953],\n",
      "        [0.1937, 0.2257, 0.1824, 0.2033, 0.1949],\n",
      "        [0.1940, 0.2246, 0.1827, 0.2026, 0.1962],\n",
      "        [0.1922, 0.2238, 0.1834, 0.2037, 0.1970]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "28/100 Running Loss: 1.604145884513855\n",
      "tensor([[0.1923, 0.2262, 0.1835, 0.2033, 0.1947],\n",
      "        [0.1912, 0.2242, 0.1833, 0.2054, 0.1959],\n",
      "        [0.1911, 0.2242, 0.1834, 0.2054, 0.1959],\n",
      "        [0.1915, 0.2234, 0.1841, 0.2041, 0.1969],\n",
      "        [0.1934, 0.2257, 0.1830, 0.2028, 0.1951],\n",
      "        [0.1930, 0.2259, 0.1829, 0.2035, 0.1947],\n",
      "        [0.1949, 0.2263, 0.1816, 0.2022, 0.1950],\n",
      "        [0.1932, 0.2260, 0.1827, 0.2035, 0.1945],\n",
      "        [0.1935, 0.2249, 0.1830, 0.2027, 0.1959],\n",
      "        [0.1917, 0.2240, 0.1838, 0.2039, 0.1967]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "29/100 Running Loss: 1.6039189100265503\n",
      "tensor([[0.1918, 0.2265, 0.1838, 0.2034, 0.1944],\n",
      "        [0.1907, 0.2244, 0.1837, 0.2056, 0.1956],\n",
      "        [0.1906, 0.2244, 0.1837, 0.2056, 0.1956],\n",
      "        [0.1910, 0.2235, 0.1845, 0.2043, 0.1967],\n",
      "        [0.1929, 0.2259, 0.1833, 0.2030, 0.1948],\n",
      "        [0.1925, 0.2262, 0.1832, 0.2036, 0.1944],\n",
      "        [0.1944, 0.2267, 0.1819, 0.2023, 0.1947],\n",
      "        [0.1927, 0.2263, 0.1831, 0.2036, 0.1943],\n",
      "        [0.1930, 0.2251, 0.1833, 0.2029, 0.1956],\n",
      "        [0.1911, 0.2242, 0.1841, 0.2041, 0.1965]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "30/100 Running Loss: 1.6036909818649292\n",
      "tensor([[0.1914, 0.2268, 0.1841, 0.2036, 0.1941],\n",
      "        [0.1902, 0.2246, 0.1841, 0.2058, 0.1954],\n",
      "        [0.1901, 0.2246, 0.1841, 0.2058, 0.1954],\n",
      "        [0.1905, 0.2237, 0.1849, 0.2045, 0.1964],\n",
      "        [0.1924, 0.2262, 0.1837, 0.2032, 0.1945],\n",
      "        [0.1920, 0.2265, 0.1836, 0.2038, 0.1941],\n",
      "        [0.1939, 0.2270, 0.1822, 0.2025, 0.1944],\n",
      "        [0.1922, 0.2266, 0.1834, 0.2038, 0.1940],\n",
      "        [0.1925, 0.2254, 0.1837, 0.2030, 0.1953],\n",
      "        [0.1906, 0.2243, 0.1845, 0.2043, 0.1963]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "31/100 Running Loss: 1.6034611463546753\n",
      "tensor([[0.1909, 0.2272, 0.1844, 0.2037, 0.1938],\n",
      "        [0.1896, 0.2248, 0.1845, 0.2060, 0.1951],\n",
      "        [0.1895, 0.2248, 0.1845, 0.2060, 0.1951],\n",
      "        [0.1899, 0.2239, 0.1853, 0.2047, 0.1962],\n",
      "        [0.1919, 0.2264, 0.1841, 0.2033, 0.1942],\n",
      "        [0.1915, 0.2268, 0.1840, 0.2039, 0.1938],\n",
      "        [0.1935, 0.2274, 0.1825, 0.2026, 0.1941],\n",
      "        [0.1918, 0.2269, 0.1837, 0.2039, 0.1937],\n",
      "        [0.1920, 0.2257, 0.1840, 0.2032, 0.1950],\n",
      "        [0.1900, 0.2245, 0.1849, 0.2045, 0.1960]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "32/100 Running Loss: 1.6032284498214722\n",
      "tensor([[0.1904, 0.2275, 0.1847, 0.2038, 0.1935],\n",
      "        [0.1891, 0.2250, 0.1848, 0.2062, 0.1949],\n",
      "        [0.1890, 0.2250, 0.1849, 0.2062, 0.1949],\n",
      "        [0.1893, 0.2241, 0.1857, 0.2050, 0.1959],\n",
      "        [0.1915, 0.2267, 0.1844, 0.2035, 0.1939],\n",
      "        [0.1910, 0.2271, 0.1843, 0.2041, 0.1935],\n",
      "        [0.1930, 0.2277, 0.1828, 0.2027, 0.1938],\n",
      "        [0.1913, 0.2272, 0.1841, 0.2040, 0.1934],\n",
      "        [0.1915, 0.2260, 0.1844, 0.2033, 0.1947],\n",
      "        [0.1895, 0.2247, 0.1853, 0.2047, 0.1958]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "33/100 Running Loss: 1.602994680404663\n",
      "tensor([[0.1899, 0.2279, 0.1851, 0.2040, 0.1932],\n",
      "        [0.1885, 0.2252, 0.1852, 0.2064, 0.1947],\n",
      "        [0.1884, 0.2253, 0.1853, 0.2064, 0.1946],\n",
      "        [0.1888, 0.2243, 0.1861, 0.2052, 0.1957],\n",
      "        [0.1910, 0.2270, 0.1848, 0.2036, 0.1936],\n",
      "        [0.1905, 0.2274, 0.1847, 0.2042, 0.1932],\n",
      "        [0.1926, 0.2280, 0.1831, 0.2029, 0.1935],\n",
      "        [0.1908, 0.2276, 0.1844, 0.2041, 0.1931],\n",
      "        [0.1911, 0.2262, 0.1847, 0.2035, 0.1945],\n",
      "        [0.1889, 0.2249, 0.1857, 0.2050, 0.1955]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "34/100 Running Loss: 1.602757215499878\n",
      "tensor([[0.1894, 0.2282, 0.1854, 0.2041, 0.1928],\n",
      "        [0.1879, 0.2254, 0.1856, 0.2066, 0.1944],\n",
      "        [0.1878, 0.2255, 0.1857, 0.2066, 0.1944],\n",
      "        [0.1882, 0.2245, 0.1865, 0.2054, 0.1954],\n",
      "        [0.1905, 0.2273, 0.1851, 0.2037, 0.1934],\n",
      "        [0.1900, 0.2277, 0.1851, 0.2044, 0.1929],\n",
      "        [0.1921, 0.2283, 0.1834, 0.2030, 0.1932],\n",
      "        [0.1903, 0.2279, 0.1848, 0.2043, 0.1928],\n",
      "        [0.1906, 0.2265, 0.1851, 0.2036, 0.1942],\n",
      "        [0.1883, 0.2251, 0.1861, 0.2052, 0.1953]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "35/100 Running Loss: 1.6025196313858032\n",
      "tensor([[0.1889, 0.2286, 0.1857, 0.2043, 0.1925],\n",
      "        [0.1873, 0.2257, 0.1860, 0.2068, 0.1942],\n",
      "        [0.1872, 0.2257, 0.1861, 0.2068, 0.1942],\n",
      "        [0.1876, 0.2247, 0.1870, 0.2056, 0.1952],\n",
      "        [0.1900, 0.2276, 0.1855, 0.2039, 0.1931],\n",
      "        [0.1895, 0.2280, 0.1855, 0.2045, 0.1926],\n",
      "        [0.1916, 0.2287, 0.1837, 0.2031, 0.1929],\n",
      "        [0.1898, 0.2282, 0.1852, 0.2044, 0.1925],\n",
      "        [0.1901, 0.2268, 0.1854, 0.2038, 0.1939],\n",
      "        [0.1877, 0.2253, 0.1865, 0.2054, 0.1951]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "36/100 Running Loss: 1.6022775173187256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1883, 0.2290, 0.1861, 0.2044, 0.1921],\n",
      "        [0.1867, 0.2259, 0.1864, 0.2070, 0.1940],\n",
      "        [0.1866, 0.2260, 0.1865, 0.2070, 0.1939],\n",
      "        [0.1870, 0.2249, 0.1874, 0.2058, 0.1949],\n",
      "        [0.1895, 0.2279, 0.1858, 0.2040, 0.1928],\n",
      "        [0.1889, 0.2283, 0.1858, 0.2047, 0.1923],\n",
      "        [0.1911, 0.2290, 0.1840, 0.2033, 0.1926],\n",
      "        [0.1892, 0.2285, 0.1855, 0.2046, 0.1922],\n",
      "        [0.1896, 0.2270, 0.1857, 0.2040, 0.1937],\n",
      "        [0.1872, 0.2256, 0.1869, 0.2055, 0.1948]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "37/100 Running Loss: 1.6020324230194092\n",
      "tensor([[0.1877, 0.2294, 0.1865, 0.2046, 0.1918],\n",
      "        [0.1861, 0.2262, 0.1868, 0.2073, 0.1937],\n",
      "        [0.1859, 0.2263, 0.1869, 0.2073, 0.1937],\n",
      "        [0.1863, 0.2251, 0.1879, 0.2061, 0.1946],\n",
      "        [0.1890, 0.2282, 0.1862, 0.2041, 0.1925],\n",
      "        [0.1884, 0.2286, 0.1862, 0.2048, 0.1920],\n",
      "        [0.1907, 0.2293, 0.1843, 0.2034, 0.1923],\n",
      "        [0.1887, 0.2289, 0.1859, 0.2047, 0.1918],\n",
      "        [0.1891, 0.2273, 0.1861, 0.2041, 0.1934],\n",
      "        [0.1866, 0.2258, 0.1873, 0.2057, 0.1946]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "38/100 Running Loss: 1.6017849445343018\n",
      "tensor([[0.1872, 0.2298, 0.1868, 0.2048, 0.1914],\n",
      "        [0.1855, 0.2264, 0.1872, 0.2075, 0.1935],\n",
      "        [0.1853, 0.2265, 0.1873, 0.2075, 0.1934],\n",
      "        [0.1857, 0.2254, 0.1883, 0.2063, 0.1943],\n",
      "        [0.1885, 0.2285, 0.1865, 0.2043, 0.1922],\n",
      "        [0.1878, 0.2290, 0.1866, 0.2050, 0.1917],\n",
      "        [0.1902, 0.2296, 0.1846, 0.2036, 0.1919],\n",
      "        [0.1882, 0.2292, 0.1863, 0.2048, 0.1915],\n",
      "        [0.1886, 0.2276, 0.1864, 0.2042, 0.1931],\n",
      "        [0.1860, 0.2260, 0.1877, 0.2059, 0.1943]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "39/100 Running Loss: 1.6015377044677734\n",
      "tensor([[0.1866, 0.2302, 0.1872, 0.2050, 0.1910],\n",
      "        [0.1848, 0.2267, 0.1876, 0.2077, 0.1932],\n",
      "        [0.1847, 0.2268, 0.1877, 0.2077, 0.1932],\n",
      "        [0.1850, 0.2256, 0.1888, 0.2065, 0.1941],\n",
      "        [0.1880, 0.2288, 0.1869, 0.2044, 0.1919],\n",
      "        [0.1872, 0.2293, 0.1870, 0.2051, 0.1913],\n",
      "        [0.1897, 0.2300, 0.1850, 0.2038, 0.1916],\n",
      "        [0.1876, 0.2296, 0.1866, 0.2050, 0.1912],\n",
      "        [0.1881, 0.2279, 0.1868, 0.2044, 0.1928],\n",
      "        [0.1854, 0.2263, 0.1881, 0.2061, 0.1941]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "40/100 Running Loss: 1.6012927293777466\n",
      "tensor([[0.1860, 0.2306, 0.1875, 0.2052, 0.1906],\n",
      "        [0.1842, 0.2270, 0.1880, 0.2079, 0.1930],\n",
      "        [0.1840, 0.2271, 0.1881, 0.2079, 0.1929],\n",
      "        [0.1843, 0.2259, 0.1893, 0.2068, 0.1938],\n",
      "        [0.1875, 0.2292, 0.1872, 0.2045, 0.1916],\n",
      "        [0.1867, 0.2297, 0.1874, 0.2053, 0.1910],\n",
      "        [0.1892, 0.2303, 0.1853, 0.2040, 0.1913],\n",
      "        [0.1870, 0.2300, 0.1870, 0.2051, 0.1909],\n",
      "        [0.1876, 0.2282, 0.1871, 0.2045, 0.1926],\n",
      "        [0.1848, 0.2265, 0.1886, 0.2063, 0.1938]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "41/100 Running Loss: 1.6010454893112183\n",
      "tensor([[0.1855, 0.2310, 0.1879, 0.2054, 0.1903],\n",
      "        [0.1836, 0.2272, 0.1884, 0.2081, 0.1927],\n",
      "        [0.1834, 0.2274, 0.1885, 0.2082, 0.1926],\n",
      "        [0.1836, 0.2261, 0.1897, 0.2070, 0.1935],\n",
      "        [0.1869, 0.2295, 0.1876, 0.2047, 0.1913],\n",
      "        [0.1861, 0.2300, 0.1877, 0.2055, 0.1907],\n",
      "        [0.1887, 0.2307, 0.1856, 0.2041, 0.1910],\n",
      "        [0.1865, 0.2303, 0.1873, 0.2053, 0.1905],\n",
      "        [0.1871, 0.2285, 0.1875, 0.2046, 0.1923],\n",
      "        [0.1842, 0.2268, 0.1890, 0.2065, 0.1936]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "42/100 Running Loss: 1.600795030593872\n",
      "tensor([[0.1849, 0.2314, 0.1882, 0.2056, 0.1899],\n",
      "        [0.1829, 0.2275, 0.1887, 0.2083, 0.1925],\n",
      "        [0.1827, 0.2276, 0.1889, 0.2084, 0.1924],\n",
      "        [0.1829, 0.2264, 0.1902, 0.2073, 0.1932],\n",
      "        [0.1864, 0.2298, 0.1880, 0.2048, 0.1910],\n",
      "        [0.1855, 0.2303, 0.1881, 0.2057, 0.1903],\n",
      "        [0.1881, 0.2311, 0.1859, 0.2043, 0.1906],\n",
      "        [0.1859, 0.2307, 0.1877, 0.2055, 0.1902],\n",
      "        [0.1865, 0.2288, 0.1878, 0.2048, 0.1920],\n",
      "        [0.1835, 0.2271, 0.1894, 0.2067, 0.1933]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "43/100 Running Loss: 1.6005433797836304\n",
      "tensor([[0.1842, 0.2319, 0.1886, 0.2058, 0.1894],\n",
      "        [0.1823, 0.2278, 0.1891, 0.2086, 0.1922],\n",
      "        [0.1821, 0.2279, 0.1893, 0.2086, 0.1921],\n",
      "        [0.1822, 0.2267, 0.1907, 0.2075, 0.1929],\n",
      "        [0.1858, 0.2302, 0.1883, 0.2050, 0.1907],\n",
      "        [0.1849, 0.2307, 0.1885, 0.2059, 0.1900],\n",
      "        [0.1876, 0.2314, 0.1862, 0.2044, 0.1903],\n",
      "        [0.1853, 0.2310, 0.1880, 0.2057, 0.1898],\n",
      "        [0.1860, 0.2291, 0.1882, 0.2049, 0.1917],\n",
      "        [0.1829, 0.2273, 0.1898, 0.2069, 0.1930]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "44/100 Running Loss: 1.6002869606018066\n",
      "tensor([[0.1836, 0.2323, 0.1890, 0.2061, 0.1890],\n",
      "        [0.1816, 0.2281, 0.1895, 0.2088, 0.1919],\n",
      "        [0.1814, 0.2282, 0.1897, 0.2089, 0.1918],\n",
      "        [0.1815, 0.2270, 0.1911, 0.2078, 0.1926],\n",
      "        [0.1853, 0.2305, 0.1887, 0.2052, 0.1903],\n",
      "        [0.1843, 0.2311, 0.1888, 0.2061, 0.1896],\n",
      "        [0.1871, 0.2318, 0.1866, 0.2046, 0.1899],\n",
      "        [0.1847, 0.2314, 0.1884, 0.2060, 0.1895],\n",
      "        [0.1855, 0.2295, 0.1886, 0.2050, 0.1914],\n",
      "        [0.1823, 0.2276, 0.1903, 0.2071, 0.1927]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "45/100 Running Loss: 1.6000275611877441\n",
      "tensor([[0.1830, 0.2328, 0.1893, 0.2063, 0.1886],\n",
      "        [0.1810, 0.2284, 0.1899, 0.2091, 0.1916],\n",
      "        [0.1808, 0.2285, 0.1901, 0.2091, 0.1915],\n",
      "        [0.1809, 0.2272, 0.1916, 0.2080, 0.1923],\n",
      "        [0.1847, 0.2309, 0.1891, 0.2054, 0.1900],\n",
      "        [0.1837, 0.2314, 0.1892, 0.2063, 0.1893],\n",
      "        [0.1865, 0.2322, 0.1869, 0.2048, 0.1896],\n",
      "        [0.1841, 0.2318, 0.1888, 0.2062, 0.1891],\n",
      "        [0.1849, 0.2298, 0.1889, 0.2052, 0.1911],\n",
      "        [0.1817, 0.2279, 0.1907, 0.2073, 0.1924]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "46/100 Running Loss: 1.5997635126113892\n",
      "tensor([[0.1823, 0.2332, 0.1897, 0.2065, 0.1882],\n",
      "        [0.1803, 0.2287, 0.1904, 0.2093, 0.1913],\n",
      "        [0.1801, 0.2288, 0.1905, 0.2094, 0.1912],\n",
      "        [0.1802, 0.2275, 0.1921, 0.2083, 0.1920],\n",
      "        [0.1841, 0.2312, 0.1894, 0.2056, 0.1896],\n",
      "        [0.1831, 0.2318, 0.1896, 0.2065, 0.1889],\n",
      "        [0.1860, 0.2326, 0.1872, 0.2049, 0.1892],\n",
      "        [0.1835, 0.2322, 0.1891, 0.2064, 0.1887],\n",
      "        [0.1843, 0.2302, 0.1893, 0.2053, 0.1908],\n",
      "        [0.1810, 0.2282, 0.1911, 0.2075, 0.1921]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "47/100 Running Loss: 1.599494457244873\n",
      "tensor([[0.1817, 0.2337, 0.1901, 0.2068, 0.1877],\n",
      "        [0.1796, 0.2291, 0.1908, 0.2096, 0.1909],\n",
      "        [0.1794, 0.2292, 0.1910, 0.2097, 0.1908],\n",
      "        [0.1795, 0.2278, 0.1925, 0.2085, 0.1916],\n",
      "        [0.1835, 0.2316, 0.1898, 0.2057, 0.1893],\n",
      "        [0.1825, 0.2322, 0.1900, 0.2067, 0.1885],\n",
      "        [0.1855, 0.2331, 0.1875, 0.2051, 0.1889],\n",
      "        [0.1829, 0.2326, 0.1895, 0.2066, 0.1884],\n",
      "        [0.1838, 0.2305, 0.1897, 0.2055, 0.1905],\n",
      "        [0.1804, 0.2285, 0.1916, 0.2077, 0.1918]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "48/100 Running Loss: 1.5992193222045898\n",
      "tensor([[0.1810, 0.2342, 0.1905, 0.2070, 0.1873],\n",
      "        [0.1789, 0.2294, 0.1912, 0.2099, 0.1906],\n",
      "        [0.1787, 0.2295, 0.1914, 0.2099, 0.1905],\n",
      "        [0.1787, 0.2281, 0.1930, 0.2088, 0.1913],\n",
      "        [0.1829, 0.2320, 0.1902, 0.2059, 0.1889],\n",
      "        [0.1819, 0.2327, 0.1904, 0.2069, 0.1881],\n",
      "        [0.1849, 0.2335, 0.1879, 0.2052, 0.1885],\n",
      "        [0.1823, 0.2331, 0.1899, 0.2068, 0.1880],\n",
      "        [0.1832, 0.2309, 0.1901, 0.2057, 0.1902],\n",
      "        [0.1797, 0.2288, 0.1920, 0.2079, 0.1915]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "49/100 Running Loss: 1.5989367961883545\n",
      "tensor([[0.1803, 0.2347, 0.1909, 0.2072, 0.1868],\n",
      "        [0.1782, 0.2297, 0.1917, 0.2101, 0.1903],\n",
      "        [0.1780, 0.2298, 0.1919, 0.2102, 0.1901],\n",
      "        [0.1780, 0.2285, 0.1935, 0.2091, 0.1909],\n",
      "        [0.1823, 0.2324, 0.1906, 0.2061, 0.1885],\n",
      "        [0.1812, 0.2331, 0.1908, 0.2071, 0.1877],\n",
      "        [0.1844, 0.2339, 0.1882, 0.2054, 0.1881],\n",
      "        [0.1816, 0.2335, 0.1903, 0.2070, 0.1876],\n",
      "        [0.1826, 0.2312, 0.1905, 0.2058, 0.1899],\n",
      "        [0.1790, 0.2292, 0.1925, 0.2082, 0.1912]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "50/100 Running Loss: 1.5986480712890625\n",
      "tensor([[0.1796, 0.2353, 0.1913, 0.2075, 0.1863],\n",
      "        [0.1775, 0.2301, 0.1921, 0.2104, 0.1899],\n",
      "        [0.1772, 0.2302, 0.1923, 0.2105, 0.1898],\n",
      "        [0.1772, 0.2288, 0.1940, 0.2094, 0.1906],\n",
      "        [0.1817, 0.2328, 0.1910, 0.2063, 0.1881],\n",
      "        [0.1805, 0.2335, 0.1912, 0.2074, 0.1873],\n",
      "        [0.1838, 0.2344, 0.1885, 0.2055, 0.1878],\n",
      "        [0.1809, 0.2340, 0.1907, 0.2073, 0.1871],\n",
      "        [0.1820, 0.2316, 0.1909, 0.2060, 0.1895],\n",
      "        [0.1783, 0.2295, 0.1929, 0.2084, 0.1908]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "51/100 Running Loss: 1.5983535051345825\n",
      "tensor([[0.1789, 0.2358, 0.1917, 0.2077, 0.1859],\n",
      "        [0.1767, 0.2304, 0.1926, 0.2107, 0.1896],\n",
      "        [0.1764, 0.2305, 0.1928, 0.2108, 0.1894],\n",
      "        [0.1764, 0.2292, 0.1945, 0.2097, 0.1902],\n",
      "        [0.1811, 0.2333, 0.1915, 0.2065, 0.1877],\n",
      "        [0.1799, 0.2340, 0.1917, 0.2076, 0.1869],\n",
      "        [0.1832, 0.2349, 0.1889, 0.2056, 0.1874],\n",
      "        [0.1803, 0.2345, 0.1911, 0.2075, 0.1867],\n",
      "        [0.1814, 0.2320, 0.1913, 0.2062, 0.1892],\n",
      "        [0.1776, 0.2298, 0.1934, 0.2086, 0.1905]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "52/100 Running Loss: 1.5980511903762817\n",
      "tensor([[0.1782, 0.2364, 0.1921, 0.2080, 0.1853],\n",
      "        [0.1759, 0.2308, 0.1931, 0.2110, 0.1892],\n",
      "        [0.1757, 0.2309, 0.1933, 0.2111, 0.1890],\n",
      "        [0.1756, 0.2295, 0.1950, 0.2100, 0.1899],\n",
      "        [0.1804, 0.2337, 0.1919, 0.2067, 0.1873],\n",
      "        [0.1792, 0.2345, 0.1921, 0.2078, 0.1864],\n",
      "        [0.1826, 0.2354, 0.1892, 0.2058, 0.1870],\n",
      "        [0.1796, 0.2350, 0.1915, 0.2077, 0.1863],\n",
      "        [0.1808, 0.2324, 0.1917, 0.2064, 0.1888],\n",
      "        [0.1769, 0.2302, 0.1939, 0.2089, 0.1901]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "53/100 Running Loss: 1.5977418422698975\n",
      "tensor([[0.1774, 0.2370, 0.1926, 0.2082, 0.1848],\n",
      "        [0.1751, 0.2312, 0.1936, 0.2113, 0.1888],\n",
      "        [0.1748, 0.2313, 0.1938, 0.2114, 0.1886],\n",
      "        [0.1748, 0.2299, 0.1955, 0.2103, 0.1895],\n",
      "        [0.1798, 0.2342, 0.1923, 0.2069, 0.1869],\n",
      "        [0.1785, 0.2350, 0.1925, 0.2080, 0.1860],\n",
      "        [0.1820, 0.2359, 0.1896, 0.2059, 0.1865],\n",
      "        [0.1789, 0.2355, 0.1919, 0.2080, 0.1858],\n",
      "        [0.1801, 0.2328, 0.1921, 0.2066, 0.1884],\n",
      "        [0.1762, 0.2305, 0.1945, 0.2091, 0.1897]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "54/100 Running Loss: 1.5974246263504028\n",
      "tensor([[0.1767, 0.2376, 0.1930, 0.2085, 0.1842],\n",
      "        [0.1743, 0.2316, 0.1941, 0.2117, 0.1884],\n",
      "        [0.1740, 0.2318, 0.1943, 0.2117, 0.1882],\n",
      "        [0.1739, 0.2303, 0.1961, 0.2106, 0.1891],\n",
      "        [0.1791, 0.2347, 0.1928, 0.2070, 0.1864],\n",
      "        [0.1777, 0.2355, 0.1930, 0.2083, 0.1855],\n",
      "        [0.1814, 0.2364, 0.1900, 0.2061, 0.1861],\n",
      "        [0.1781, 0.2360, 0.1923, 0.2082, 0.1853],\n",
      "        [0.1795, 0.2332, 0.1925, 0.2067, 0.1880],\n",
      "        [0.1754, 0.2309, 0.1950, 0.2094, 0.1893]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "55/100 Running Loss: 1.5970994234085083\n",
      "tensor([[0.1759, 0.2382, 0.1935, 0.2088, 0.1836],\n",
      "        [0.1735, 0.2320, 0.1946, 0.2120, 0.1879],\n",
      "        [0.1732, 0.2322, 0.1948, 0.2121, 0.1877],\n",
      "        [0.1731, 0.2307, 0.1966, 0.2109, 0.1886],\n",
      "        [0.1784, 0.2352, 0.1932, 0.2072, 0.1860],\n",
      "        [0.1770, 0.2361, 0.1935, 0.2085, 0.1849],\n",
      "        [0.1808, 0.2370, 0.1904, 0.2062, 0.1857],\n",
      "        [0.1774, 0.2366, 0.1928, 0.2085, 0.1848],\n",
      "        [0.1788, 0.2337, 0.1930, 0.2069, 0.1876],\n",
      "        [0.1746, 0.2313, 0.1955, 0.2096, 0.1889]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "56/100 Running Loss: 1.5967652797698975\n",
      "tensor([[0.1751, 0.2388, 0.1940, 0.2090, 0.1830],\n",
      "        [0.1726, 0.2325, 0.1951, 0.2123, 0.1875],\n",
      "        [0.1723, 0.2327, 0.1954, 0.2124, 0.1872],\n",
      "        [0.1722, 0.2312, 0.1972, 0.2113, 0.1882],\n",
      "        [0.1777, 0.2357, 0.1937, 0.2075, 0.1855],\n",
      "        [0.1762, 0.2366, 0.1940, 0.2088, 0.1844],\n",
      "        [0.1801, 0.2375, 0.1908, 0.2064, 0.1852],\n",
      "        [0.1766, 0.2371, 0.1933, 0.2088, 0.1842],\n",
      "        [0.1782, 0.2341, 0.1934, 0.2072, 0.1871],\n",
      "        [0.1738, 0.2317, 0.1961, 0.2099, 0.1885]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "57/100 Running Loss: 1.5964208841323853\n",
      "tensor([[0.1743, 0.2395, 0.1945, 0.2093, 0.1824],\n",
      "        [0.1717, 0.2329, 0.1957, 0.2127, 0.1869],\n",
      "        [0.1714, 0.2332, 0.1960, 0.2128, 0.1867],\n",
      "        [0.1712, 0.2317, 0.1977, 0.2116, 0.1878],\n",
      "        [0.1770, 0.2362, 0.1942, 0.2077, 0.1850],\n",
      "        [0.1755, 0.2372, 0.1944, 0.2090, 0.1839],\n",
      "        [0.1794, 0.2381, 0.1912, 0.2066, 0.1847],\n",
      "        [0.1758, 0.2377, 0.1937, 0.2091, 0.1837],\n",
      "        [0.1775, 0.2346, 0.1939, 0.2074, 0.1867],\n",
      "        [0.1729, 0.2321, 0.1967, 0.2102, 0.1880]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "58/100 Running Loss: 1.5960675477981567\n",
      "tensor([[0.1734, 0.2402, 0.1950, 0.2096, 0.1817],\n",
      "        [0.1708, 0.2334, 0.1963, 0.2131, 0.1864],\n",
      "        [0.1704, 0.2337, 0.1966, 0.2132, 0.1862],\n",
      "        [0.1703, 0.2322, 0.1983, 0.2120, 0.1873],\n",
      "        [0.1762, 0.2367, 0.1946, 0.2079, 0.1845],\n",
      "        [0.1746, 0.2378, 0.1949, 0.2093, 0.1833],\n",
      "        [0.1787, 0.2387, 0.1916, 0.2068, 0.1842],\n",
      "        [0.1750, 0.2383, 0.1942, 0.2093, 0.1831],\n",
      "        [0.1768, 0.2351, 0.1943, 0.2076, 0.1862],\n",
      "        [0.1721, 0.2325, 0.1973, 0.2105, 0.1875]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "59/100 Running Loss: 1.5957022905349731\n",
      "tensor([[0.1725, 0.2409, 0.1956, 0.2099, 0.1811],\n",
      "        [0.1698, 0.2339, 0.1969, 0.2135, 0.1859],\n",
      "        [0.1695, 0.2342, 0.1972, 0.2136, 0.1856],\n",
      "        [0.1693, 0.2327, 0.1989, 0.2124, 0.1868],\n",
      "        [0.1755, 0.2373, 0.1951, 0.2081, 0.1840],\n",
      "        [0.1738, 0.2385, 0.1955, 0.2096, 0.1827],\n",
      "        [0.1780, 0.2393, 0.1920, 0.2070, 0.1837],\n",
      "        [0.1741, 0.2390, 0.1947, 0.2096, 0.1825],\n",
      "        [0.1760, 0.2356, 0.1948, 0.2079, 0.1857],\n",
      "        [0.1712, 0.2330, 0.1979, 0.2109, 0.1870]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "60/100 Running Loss: 1.595328450202942\n",
      "tensor([[0.1716, 0.2417, 0.1962, 0.2102, 0.1804],\n",
      "        [0.1688, 0.2344, 0.1976, 0.2139, 0.1853],\n",
      "        [0.1685, 0.2347, 0.1978, 0.2140, 0.1850],\n",
      "        [0.1683, 0.2332, 0.1995, 0.2127, 0.1863],\n",
      "        [0.1747, 0.2378, 0.1956, 0.2084, 0.1834],\n",
      "        [0.1729, 0.2391, 0.1960, 0.2099, 0.1821],\n",
      "        [0.1773, 0.2399, 0.1925, 0.2071, 0.1832],\n",
      "        [0.1733, 0.2397, 0.1953, 0.2099, 0.1819],\n",
      "        [0.1753, 0.2361, 0.1953, 0.2081, 0.1852],\n",
      "        [0.1703, 0.2334, 0.1986, 0.2112, 0.1865]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "61/100 Running Loss: 1.5949429273605347\n",
      "tensor([[0.1706, 0.2425, 0.1968, 0.2106, 0.1796],\n",
      "        [0.1678, 0.2349, 0.1982, 0.2144, 0.1847],\n",
      "        [0.1674, 0.2352, 0.1985, 0.2144, 0.1844],\n",
      "        [0.1672, 0.2337, 0.2001, 0.2131, 0.1858],\n",
      "        [0.1739, 0.2384, 0.1962, 0.2087, 0.1829],\n",
      "        [0.1720, 0.2398, 0.1966, 0.2102, 0.1814],\n",
      "        [0.1765, 0.2406, 0.1930, 0.2073, 0.1826],\n",
      "        [0.1723, 0.2403, 0.1959, 0.2103, 0.1812],\n",
      "        [0.1745, 0.2367, 0.1958, 0.2084, 0.1847],\n",
      "        [0.1693, 0.2339, 0.1993, 0.2115, 0.1860]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "62/100 Running Loss: 1.5945442914962769\n",
      "tensor([[0.1696, 0.2433, 0.1974, 0.2108, 0.1789],\n",
      "        [0.1667, 0.2354, 0.1989, 0.2148, 0.1841],\n",
      "        [0.1664, 0.2357, 0.1992, 0.2149, 0.1838],\n",
      "        [0.1662, 0.2342, 0.2008, 0.2135, 0.1852],\n",
      "        [0.1730, 0.2390, 0.1967, 0.2090, 0.1823],\n",
      "        [0.1711, 0.2405, 0.1973, 0.2105, 0.1807],\n",
      "        [0.1758, 0.2412, 0.1934, 0.2075, 0.1820],\n",
      "        [0.1714, 0.2411, 0.1965, 0.2106, 0.1805],\n",
      "        [0.1737, 0.2372, 0.1963, 0.2086, 0.1841],\n",
      "        [0.1683, 0.2344, 0.2000, 0.2119, 0.1855]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "63/100 Running Loss: 1.5941332578659058\n",
      "tensor([[0.1686, 0.2441, 0.1981, 0.2111, 0.1781],\n",
      "        [0.1657, 0.2359, 0.1997, 0.2153, 0.1834],\n",
      "        [0.1653, 0.2362, 0.2000, 0.2154, 0.1831],\n",
      "        [0.1651, 0.2348, 0.2016, 0.2139, 0.1847],\n",
      "        [0.1721, 0.2397, 0.1973, 0.2092, 0.1816],\n",
      "        [0.1701, 0.2412, 0.1979, 0.2108, 0.1800],\n",
      "        [0.1749, 0.2419, 0.1940, 0.2077, 0.1814],\n",
      "        [0.1704, 0.2418, 0.1971, 0.2109, 0.1798],\n",
      "        [0.1729, 0.2378, 0.1969, 0.2089, 0.1836],\n",
      "        [0.1673, 0.2349, 0.2007, 0.2122, 0.1849]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "64/100 Running Loss: 1.5937082767486572\n",
      "tensor([[0.1675, 0.2450, 0.1988, 0.2114, 0.1773],\n",
      "        [0.1646, 0.2365, 0.2004, 0.2157, 0.1828],\n",
      "        [0.1642, 0.2368, 0.2007, 0.2158, 0.1825],\n",
      "        [0.1639, 0.2354, 0.2023, 0.2143, 0.1841],\n",
      "        [0.1712, 0.2403, 0.1980, 0.2095, 0.1809],\n",
      "        [0.1691, 0.2419, 0.1986, 0.2111, 0.1793],\n",
      "        [0.1741, 0.2426, 0.1946, 0.2079, 0.1808],\n",
      "        [0.1694, 0.2426, 0.1977, 0.2112, 0.1790],\n",
      "        [0.1720, 0.2383, 0.1974, 0.2092, 0.1830],\n",
      "        [0.1662, 0.2355, 0.2014, 0.2126, 0.1843]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "65/100 Running Loss: 1.5932705402374268\n",
      "tensor([[0.1664, 0.2459, 0.1996, 0.2117, 0.1765],\n",
      "        [0.1634, 0.2371, 0.2012, 0.2162, 0.1821],\n",
      "        [0.1630, 0.2374, 0.2016, 0.2163, 0.1817],\n",
      "        [0.1627, 0.2360, 0.2031, 0.2147, 0.1834],\n",
      "        [0.1703, 0.2410, 0.1986, 0.2098, 0.1803],\n",
      "        [0.1680, 0.2427, 0.1993, 0.2114, 0.1785],\n",
      "        [0.1732, 0.2433, 0.1952, 0.2081, 0.1802],\n",
      "        [0.1684, 0.2434, 0.1984, 0.2116, 0.1783],\n",
      "        [0.1711, 0.2389, 0.1980, 0.2095, 0.1824],\n",
      "        [0.1651, 0.2361, 0.2022, 0.2130, 0.1837]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "66/100 Running Loss: 1.5928200483322144\n",
      "tensor([[0.1653, 0.2468, 0.2003, 0.2119, 0.1757],\n",
      "        [0.1621, 0.2378, 0.2020, 0.2167, 0.1814],\n",
      "        [0.1618, 0.2381, 0.2024, 0.2168, 0.1810],\n",
      "        [0.1615, 0.2366, 0.2040, 0.2152, 0.1827],\n",
      "        [0.1694, 0.2417, 0.1993, 0.2101, 0.1796],\n",
      "        [0.1670, 0.2435, 0.2001, 0.2117, 0.1777],\n",
      "        [0.1723, 0.2441, 0.1958, 0.2083, 0.1795],\n",
      "        [0.1673, 0.2442, 0.1991, 0.2119, 0.1775],\n",
      "        [0.1702, 0.2396, 0.1987, 0.2098, 0.1817],\n",
      "        [0.1639, 0.2367, 0.2030, 0.2134, 0.1831]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "67/100 Running Loss: 1.592355728149414\n",
      "tensor([[0.1641, 0.2477, 0.2011, 0.2122, 0.1748],\n",
      "        [0.1609, 0.2385, 0.2028, 0.2171, 0.1806],\n",
      "        [0.1605, 0.2388, 0.2032, 0.2172, 0.1803],\n",
      "        [0.1602, 0.2373, 0.2049, 0.2156, 0.1820],\n",
      "        [0.1683, 0.2424, 0.2000, 0.2104, 0.1788],\n",
      "        [0.1659, 0.2443, 0.2008, 0.2120, 0.1769],\n",
      "        [0.1713, 0.2449, 0.1965, 0.2084, 0.1788],\n",
      "        [0.1662, 0.2450, 0.1999, 0.2121, 0.1767],\n",
      "        [0.1693, 0.2402, 0.1994, 0.2101, 0.1811],\n",
      "        [0.1628, 0.2373, 0.2038, 0.2138, 0.1824]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "68/100 Running Loss: 1.5918759107589722\n",
      "tensor([[0.1629, 0.2487, 0.2020, 0.2124, 0.1740],\n",
      "        [0.1596, 0.2392, 0.2037, 0.2176, 0.1799],\n",
      "        [0.1591, 0.2395, 0.2041, 0.2177, 0.1795],\n",
      "        [0.1589, 0.2380, 0.2058, 0.2160, 0.1812],\n",
      "        [0.1673, 0.2432, 0.2008, 0.2107, 0.1780],\n",
      "        [0.1647, 0.2452, 0.2016, 0.2123, 0.1761],\n",
      "        [0.1704, 0.2458, 0.1972, 0.2086, 0.1781],\n",
      "        [0.1651, 0.2459, 0.2007, 0.2124, 0.1759],\n",
      "        [0.1683, 0.2409, 0.2001, 0.2104, 0.1804],\n",
      "        [0.1616, 0.2379, 0.2046, 0.2142, 0.1817]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "69/100 Running Loss: 1.5913803577423096\n",
      "tensor([[0.1617, 0.2497, 0.2029, 0.2127, 0.1730],\n",
      "        [0.1582, 0.2400, 0.2046, 0.2181, 0.1791],\n",
      "        [0.1578, 0.2403, 0.2050, 0.2182, 0.1787],\n",
      "        [0.1575, 0.2388, 0.2068, 0.2164, 0.1804],\n",
      "        [0.1662, 0.2440, 0.2016, 0.2109, 0.1772],\n",
      "        [0.1636, 0.2461, 0.2025, 0.2126, 0.1752],\n",
      "        [0.1693, 0.2467, 0.1980, 0.2087, 0.1773],\n",
      "        [0.1639, 0.2468, 0.2016, 0.2127, 0.1750],\n",
      "        [0.1673, 0.2416, 0.2008, 0.2107, 0.1796],\n",
      "        [0.1604, 0.2386, 0.2055, 0.2146, 0.1810]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "70/100 Running Loss: 1.5908676385879517\n",
      "tensor([[0.1604, 0.2508, 0.2037, 0.2129, 0.1721],\n",
      "        [0.1569, 0.2407, 0.2056, 0.2186, 0.1782],\n",
      "        [0.1564, 0.2411, 0.2060, 0.2187, 0.1778],\n",
      "        [0.1561, 0.2396, 0.2079, 0.2168, 0.1796],\n",
      "        [0.1651, 0.2449, 0.2024, 0.2112, 0.1764],\n",
      "        [0.1624, 0.2470, 0.2034, 0.2129, 0.1744],\n",
      "        [0.1683, 0.2476, 0.1988, 0.2089, 0.1765],\n",
      "        [0.1628, 0.2477, 0.2024, 0.2129, 0.1742],\n",
      "        [0.1663, 0.2423, 0.2015, 0.2110, 0.1789],\n",
      "        [0.1591, 0.2392, 0.2064, 0.2151, 0.1802]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "71/100 Running Loss: 1.5903373956680298\n",
      "tensor([[0.1591, 0.2519, 0.2046, 0.2132, 0.1711],\n",
      "        [0.1555, 0.2415, 0.2066, 0.2191, 0.1774],\n",
      "        [0.1550, 0.2419, 0.2070, 0.2192, 0.1769],\n",
      "        [0.1547, 0.2404, 0.2090, 0.2173, 0.1786],\n",
      "        [0.1640, 0.2458, 0.2032, 0.2115, 0.1755],\n",
      "        [0.1612, 0.2479, 0.2043, 0.2132, 0.1735],\n",
      "        [0.1672, 0.2485, 0.1996, 0.2090, 0.1757],\n",
      "        [0.1615, 0.2487, 0.2034, 0.2132, 0.1733],\n",
      "        [0.1652, 0.2430, 0.2023, 0.2113, 0.1781],\n",
      "        [0.1578, 0.2399, 0.2073, 0.2155, 0.1794]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "72/100 Running Loss: 1.5897852182388306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1578, 0.2531, 0.2056, 0.2134, 0.1701],\n",
      "        [0.1541, 0.2423, 0.2076, 0.2196, 0.1765],\n",
      "        [0.1536, 0.2427, 0.2081, 0.2196, 0.1760],\n",
      "        [0.1533, 0.2412, 0.2101, 0.2178, 0.1777],\n",
      "        [0.1628, 0.2467, 0.2041, 0.2117, 0.1746],\n",
      "        [0.1599, 0.2489, 0.2052, 0.2134, 0.1726],\n",
      "        [0.1661, 0.2495, 0.2004, 0.2091, 0.1749],\n",
      "        [0.1603, 0.2497, 0.2043, 0.2134, 0.1723],\n",
      "        [0.1641, 0.2438, 0.2031, 0.2116, 0.1773],\n",
      "        [0.1566, 0.2406, 0.2083, 0.2159, 0.1785]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "73/100 Running Loss: 1.589213490486145\n",
      "tensor([[0.1564, 0.2543, 0.2065, 0.2137, 0.1691],\n",
      "        [0.1526, 0.2431, 0.2087, 0.2200, 0.1755],\n",
      "        [0.1521, 0.2435, 0.2092, 0.2201, 0.1751],\n",
      "        [0.1518, 0.2420, 0.2112, 0.2182, 0.1768],\n",
      "        [0.1616, 0.2477, 0.2050, 0.2120, 0.1737],\n",
      "        [0.1586, 0.2499, 0.2062, 0.2136, 0.1716],\n",
      "        [0.1649, 0.2506, 0.2013, 0.2092, 0.1740],\n",
      "        [0.1590, 0.2507, 0.2053, 0.2136, 0.1714],\n",
      "        [0.1630, 0.2447, 0.2040, 0.2119, 0.1765],\n",
      "        [0.1552, 0.2414, 0.2094, 0.2164, 0.1776]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "74/100 Running Loss: 1.5886166095733643\n",
      "tensor([[0.1550, 0.2555, 0.2075, 0.2139, 0.1680],\n",
      "        [0.1512, 0.2439, 0.2099, 0.2204, 0.1746],\n",
      "        [0.1507, 0.2443, 0.2104, 0.2205, 0.1741],\n",
      "        [0.1503, 0.2428, 0.2124, 0.2187, 0.1758],\n",
      "        [0.1604, 0.2487, 0.2060, 0.2121, 0.1728],\n",
      "        [0.1573, 0.2510, 0.2072, 0.2139, 0.1707],\n",
      "        [0.1638, 0.2517, 0.2022, 0.2093, 0.1731],\n",
      "        [0.1576, 0.2518, 0.2062, 0.2139, 0.1704],\n",
      "        [0.1618, 0.2456, 0.2049, 0.2122, 0.1756],\n",
      "        [0.1538, 0.2422, 0.2105, 0.2168, 0.1767]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "75/100 Running Loss: 1.5879952907562256\n",
      "tensor([[0.1536, 0.2569, 0.2086, 0.2141, 0.1668],\n",
      "        [0.1497, 0.2447, 0.2111, 0.2208, 0.1737],\n",
      "        [0.1492, 0.2451, 0.2116, 0.2208, 0.1732],\n",
      "        [0.1488, 0.2436, 0.2136, 0.2191, 0.1748],\n",
      "        [0.1591, 0.2497, 0.2070, 0.2123, 0.1719],\n",
      "        [0.1559, 0.2521, 0.2082, 0.2141, 0.1697],\n",
      "        [0.1625, 0.2528, 0.2031, 0.2094, 0.1721],\n",
      "        [0.1563, 0.2530, 0.2072, 0.2141, 0.1694],\n",
      "        [0.1606, 0.2465, 0.2058, 0.2124, 0.1747],\n",
      "        [0.1524, 0.2429, 0.2117, 0.2172, 0.1758]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "76/100 Running Loss: 1.5873539447784424\n",
      "tensor([[0.1521, 0.2582, 0.2096, 0.2144, 0.1657],\n",
      "        [0.1482, 0.2456, 0.2123, 0.2211, 0.1728],\n",
      "        [0.1477, 0.2460, 0.2128, 0.2211, 0.1723],\n",
      "        [0.1473, 0.2444, 0.2149, 0.2196, 0.1738],\n",
      "        [0.1578, 0.2509, 0.2080, 0.2125, 0.1709],\n",
      "        [0.1545, 0.2533, 0.2093, 0.2143, 0.1686],\n",
      "        [0.1613, 0.2540, 0.2041, 0.2095, 0.1711],\n",
      "        [0.1549, 0.2542, 0.2083, 0.2142, 0.1684],\n",
      "        [0.1594, 0.2474, 0.2068, 0.2126, 0.1738],\n",
      "        [0.1510, 0.2437, 0.2128, 0.2176, 0.1749]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "77/100 Running Loss: 1.5866891145706177\n",
      "tensor([[0.1506, 0.2596, 0.2106, 0.2145, 0.1646],\n",
      "        [0.1467, 0.2465, 0.2135, 0.2214, 0.1718],\n",
      "        [0.1462, 0.2469, 0.2141, 0.2215, 0.1714],\n",
      "        [0.1458, 0.2452, 0.2163, 0.2200, 0.1728],\n",
      "        [0.1564, 0.2520, 0.2090, 0.2126, 0.1699],\n",
      "        [0.1531, 0.2546, 0.2104, 0.2144, 0.1675],\n",
      "        [0.1600, 0.2552, 0.2050, 0.2096, 0.1701],\n",
      "        [0.1535, 0.2555, 0.2093, 0.2144, 0.1673],\n",
      "        [0.1581, 0.2484, 0.2078, 0.2128, 0.1729],\n",
      "        [0.1495, 0.2445, 0.2140, 0.2180, 0.1739]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "78/100 Running Loss: 1.5860055685043335\n",
      "tensor([[0.1491, 0.2611, 0.2117, 0.2147, 0.1634],\n",
      "        [0.1452, 0.2474, 0.2149, 0.2217, 0.1708],\n",
      "        [0.1446, 0.2478, 0.2154, 0.2218, 0.1703],\n",
      "        [0.1442, 0.2461, 0.2177, 0.2203, 0.1718],\n",
      "        [0.1551, 0.2532, 0.2101, 0.2128, 0.1688],\n",
      "        [0.1516, 0.2559, 0.2115, 0.2146, 0.1664],\n",
      "        [0.1587, 0.2565, 0.2060, 0.2097, 0.1690],\n",
      "        [0.1520, 0.2569, 0.2104, 0.2145, 0.1662],\n",
      "        [0.1568, 0.2494, 0.2089, 0.2130, 0.1719],\n",
      "        [0.1480, 0.2454, 0.2153, 0.2183, 0.1730]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "79/100 Running Loss: 1.5853036642074585\n",
      "tensor([[0.1475, 0.2626, 0.2128, 0.2149, 0.1622],\n",
      "        [0.1436, 0.2484, 0.2162, 0.2221, 0.1698],\n",
      "        [0.1430, 0.2488, 0.2168, 0.2221, 0.1693],\n",
      "        [0.1426, 0.2470, 0.2191, 0.2207, 0.1706],\n",
      "        [0.1536, 0.2545, 0.2112, 0.2129, 0.1677],\n",
      "        [0.1501, 0.2572, 0.2126, 0.2147, 0.1653],\n",
      "        [0.1574, 0.2579, 0.2071, 0.2098, 0.1679],\n",
      "        [0.1505, 0.2582, 0.2115, 0.2146, 0.1651],\n",
      "        [0.1554, 0.2505, 0.2100, 0.2132, 0.1708],\n",
      "        [0.1465, 0.2463, 0.2166, 0.2186, 0.1719]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "80/100 Running Loss: 1.5845855474472046\n",
      "tensor([[0.1459, 0.2642, 0.2140, 0.2150, 0.1609],\n",
      "        [0.1420, 0.2494, 0.2176, 0.2224, 0.1687],\n",
      "        [0.1414, 0.2499, 0.2182, 0.2224, 0.1681],\n",
      "        [0.1409, 0.2480, 0.2206, 0.2210, 0.1694],\n",
      "        [0.1522, 0.2558, 0.2124, 0.2130, 0.1666],\n",
      "        [0.1486, 0.2586, 0.2138, 0.2149, 0.1642],\n",
      "        [0.1560, 0.2593, 0.2081, 0.2098, 0.1668],\n",
      "        [0.1490, 0.2596, 0.2127, 0.2148, 0.1639],\n",
      "        [0.1541, 0.2516, 0.2112, 0.2133, 0.1698],\n",
      "        [0.1450, 0.2473, 0.2180, 0.2189, 0.1709]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "81/100 Running Loss: 1.5838488340377808\n",
      "tensor([[0.1442, 0.2658, 0.2153, 0.2151, 0.1596],\n",
      "        [0.1403, 0.2504, 0.2191, 0.2227, 0.1675],\n",
      "        [0.1397, 0.2510, 0.2196, 0.2227, 0.1669],\n",
      "        [0.1392, 0.2491, 0.2222, 0.2213, 0.1682],\n",
      "        [0.1507, 0.2572, 0.2135, 0.2131, 0.1655],\n",
      "        [0.1470, 0.2601, 0.2149, 0.2150, 0.1630],\n",
      "        [0.1546, 0.2608, 0.2092, 0.2098, 0.1657],\n",
      "        [0.1474, 0.2611, 0.2138, 0.2149, 0.1627],\n",
      "        [0.1527, 0.2528, 0.2123, 0.2135, 0.1688],\n",
      "        [0.1434, 0.2483, 0.2194, 0.2192, 0.1697]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "82/100 Running Loss: 1.583091378211975\n",
      "tensor([[0.1425, 0.2676, 0.2165, 0.2152, 0.1582],\n",
      "        [0.1386, 0.2516, 0.2205, 0.2230, 0.1663],\n",
      "        [0.1380, 0.2521, 0.2211, 0.2231, 0.1657],\n",
      "        [0.1375, 0.2503, 0.2237, 0.2216, 0.1669],\n",
      "        [0.1492, 0.2586, 0.2147, 0.2132, 0.1643],\n",
      "        [0.1454, 0.2616, 0.2162, 0.2151, 0.1617],\n",
      "        [0.1531, 0.2623, 0.2103, 0.2099, 0.1645],\n",
      "        [0.1458, 0.2627, 0.2150, 0.2149, 0.1615],\n",
      "        [0.1512, 0.2540, 0.2134, 0.2136, 0.1677],\n",
      "        [0.1418, 0.2493, 0.2209, 0.2195, 0.1686]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "83/100 Running Loss: 1.5823074579238892\n",
      "tensor([[0.1407, 0.2694, 0.2179, 0.2153, 0.1567],\n",
      "        [0.1368, 0.2528, 0.2220, 0.2233, 0.1650],\n",
      "        [0.1362, 0.2534, 0.2226, 0.2234, 0.1644],\n",
      "        [0.1357, 0.2515, 0.2253, 0.2220, 0.1656],\n",
      "        [0.1477, 0.2601, 0.2159, 0.2132, 0.1631],\n",
      "        [0.1437, 0.2632, 0.2175, 0.2152, 0.1604],\n",
      "        [0.1516, 0.2638, 0.2115, 0.2098, 0.1633],\n",
      "        [0.1442, 0.2643, 0.2163, 0.2150, 0.1602],\n",
      "        [0.1498, 0.2553, 0.2146, 0.2137, 0.1666],\n",
      "        [0.1401, 0.2504, 0.2224, 0.2197, 0.1673]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "84/100 Running Loss: 1.5815006494522095\n",
      "tensor([[0.1389, 0.2713, 0.2192, 0.2153, 0.1552],\n",
      "        [0.1350, 0.2540, 0.2236, 0.2236, 0.1638],\n",
      "        [0.1344, 0.2546, 0.2242, 0.2236, 0.1631],\n",
      "        [0.1339, 0.2527, 0.2270, 0.2222, 0.1642],\n",
      "        [0.1461, 0.2617, 0.2172, 0.2133, 0.1618],\n",
      "        [0.1420, 0.2649, 0.2188, 0.2152, 0.1591],\n",
      "        [0.1501, 0.2653, 0.2127, 0.2098, 0.1620],\n",
      "        [0.1425, 0.2660, 0.2176, 0.2150, 0.1589],\n",
      "        [0.1482, 0.2566, 0.2159, 0.2138, 0.1654],\n",
      "        [0.1384, 0.2514, 0.2240, 0.2200, 0.1661]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "85/100 Running Loss: 1.5806716680526733\n",
      "tensor([[0.1371, 0.2733, 0.2207, 0.2153, 0.1537],\n",
      "        [0.1332, 0.2552, 0.2252, 0.2239, 0.1624],\n",
      "        [0.1326, 0.2559, 0.2259, 0.2239, 0.1618],\n",
      "        [0.1320, 0.2540, 0.2287, 0.2225, 0.1627],\n",
      "        [0.1444, 0.2633, 0.2185, 0.2133, 0.1605],\n",
      "        [0.1403, 0.2666, 0.2202, 0.2152, 0.1577],\n",
      "        [0.1485, 0.2669, 0.2140, 0.2097, 0.1608],\n",
      "        [0.1408, 0.2677, 0.2189, 0.2150, 0.1575],\n",
      "        [0.1467, 0.2580, 0.2171, 0.2139, 0.1642],\n",
      "        [0.1367, 0.2526, 0.2256, 0.2202, 0.1648]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "86/100 Running Loss: 1.579818844795227\n",
      "tensor([[0.1352, 0.2752, 0.2222, 0.2152, 0.1521],\n",
      "        [0.1314, 0.2565, 0.2269, 0.2242, 0.1610],\n",
      "        [0.1307, 0.2572, 0.2276, 0.2242, 0.1604],\n",
      "        [0.1301, 0.2554, 0.2305, 0.2228, 0.1613],\n",
      "        [0.1428, 0.2649, 0.2199, 0.2133, 0.1591],\n",
      "        [0.1385, 0.2684, 0.2216, 0.2152, 0.1562],\n",
      "        [0.1469, 0.2686, 0.2154, 0.2097, 0.1595],\n",
      "        [0.1390, 0.2695, 0.2203, 0.2150, 0.1561],\n",
      "        [0.1451, 0.2595, 0.2185, 0.2140, 0.1630],\n",
      "        [0.1349, 0.2538, 0.2273, 0.2204, 0.1635]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "87/100 Running Loss: 1.5789437294006348\n",
      "tensor([[0.1333, 0.2772, 0.2238, 0.2151, 0.1505],\n",
      "        [0.1295, 0.2578, 0.2286, 0.2245, 0.1596],\n",
      "        [0.1288, 0.2585, 0.2293, 0.2244, 0.1589],\n",
      "        [0.1281, 0.2568, 0.2324, 0.2230, 0.1597],\n",
      "        [0.1410, 0.2667, 0.2213, 0.2132, 0.1578],\n",
      "        [0.1367, 0.2703, 0.2231, 0.2152, 0.1547],\n",
      "        [0.1452, 0.2703, 0.2167, 0.2095, 0.1582],\n",
      "        [0.1372, 0.2714, 0.2218, 0.2150, 0.1546],\n",
      "        [0.1435, 0.2609, 0.2198, 0.2140, 0.1617],\n",
      "        [0.1331, 0.2551, 0.2290, 0.2206, 0.1622]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "88/100 Running Loss: 1.5780487060546875\n",
      "tensor([[0.1313, 0.2792, 0.2255, 0.2151, 0.1489],\n",
      "        [0.1275, 0.2591, 0.2304, 0.2247, 0.1582],\n",
      "        [0.1268, 0.2599, 0.2311, 0.2246, 0.1575],\n",
      "        [0.1261, 0.2581, 0.2343, 0.2232, 0.1582],\n",
      "        [0.1393, 0.2684, 0.2228, 0.2132, 0.1563],\n",
      "        [0.1348, 0.2721, 0.2247, 0.2151, 0.1532],\n",
      "        [0.1436, 0.2721, 0.2181, 0.2094, 0.1568],\n",
      "        [0.1354, 0.2732, 0.2234, 0.2149, 0.1531],\n",
      "        [0.1418, 0.2625, 0.2213, 0.2140, 0.1604],\n",
      "        [0.1312, 0.2564, 0.2308, 0.2208, 0.1608]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "89/100 Running Loss: 1.577133059501648\n",
      "tensor([[0.1293, 0.2812, 0.2273, 0.2149, 0.1472],\n",
      "        [0.1255, 0.2605, 0.2323, 0.2249, 0.1567],\n",
      "        [0.1248, 0.2613, 0.2330, 0.2248, 0.1559],\n",
      "        [0.1241, 0.2596, 0.2364, 0.2233, 0.1566],\n",
      "        [0.1375, 0.2702, 0.2243, 0.2131, 0.1549],\n",
      "        [0.1329, 0.2740, 0.2264, 0.2150, 0.1517],\n",
      "        [0.1419, 0.2739, 0.2196, 0.2092, 0.1555],\n",
      "        [0.1335, 0.2751, 0.2250, 0.2148, 0.1516],\n",
      "        [0.1401, 0.2640, 0.2227, 0.2140, 0.1591],\n",
      "        [0.1293, 0.2577, 0.2327, 0.2210, 0.1594]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "90/100 Running Loss: 1.576196312904358\n",
      "tensor([[0.1273, 0.2834, 0.2290, 0.2148, 0.1455],\n",
      "        [0.1235, 0.2619, 0.2342, 0.2251, 0.1552],\n",
      "        [0.1228, 0.2628, 0.2350, 0.2250, 0.1544],\n",
      "        [0.1220, 0.2610, 0.2385, 0.2234, 0.1550],\n",
      "        [0.1357, 0.2721, 0.2259, 0.2129, 0.1534],\n",
      "        [0.1310, 0.2758, 0.2281, 0.2149, 0.1502],\n",
      "        [0.1401, 0.2758, 0.2210, 0.2090, 0.1540],\n",
      "        [0.1316, 0.2770, 0.2266, 0.2148, 0.1500],\n",
      "        [0.1384, 0.2655, 0.2243, 0.2140, 0.1578],\n",
      "        [0.1274, 0.2590, 0.2346, 0.2211, 0.1579]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "91/100 Running Loss: 1.5752382278442383\n",
      "tensor([[0.1252, 0.2858, 0.2307, 0.2146, 0.1437],\n",
      "        [0.1215, 0.2634, 0.2362, 0.2252, 0.1537],\n",
      "        [0.1207, 0.2643, 0.2370, 0.2251, 0.1528],\n",
      "        [0.1199, 0.2626, 0.2406, 0.2235, 0.1534],\n",
      "        [0.1339, 0.2739, 0.2276, 0.2128, 0.1519],\n",
      "        [0.1291, 0.2778, 0.2298, 0.2147, 0.1485],\n",
      "        [0.1383, 0.2778, 0.2225, 0.2087, 0.1526],\n",
      "        [0.1297, 0.2791, 0.2282, 0.2146, 0.1484],\n",
      "        [0.1367, 0.2671, 0.2259, 0.2139, 0.1564],\n",
      "        [0.1254, 0.2604, 0.2366, 0.2212, 0.1565]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "92/100 Running Loss: 1.5742558240890503\n",
      "tensor([[0.1231, 0.2882, 0.2325, 0.2143, 0.1419],\n",
      "        [0.1194, 0.2650, 0.2383, 0.2253, 0.1521],\n",
      "        [0.1186, 0.2659, 0.2391, 0.2252, 0.1512],\n",
      "        [0.1177, 0.2642, 0.2429, 0.2235, 0.1517],\n",
      "        [0.1320, 0.2758, 0.2293, 0.2126, 0.1503],\n",
      "        [0.1271, 0.2800, 0.2315, 0.2146, 0.1468],\n",
      "        [0.1365, 0.2798, 0.2241, 0.2085, 0.1511],\n",
      "        [0.1277, 0.2812, 0.2299, 0.2145, 0.1467],\n",
      "        [0.1349, 0.2687, 0.2275, 0.2138, 0.1550],\n",
      "        [0.1234, 0.2618, 0.2386, 0.2212, 0.1550]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "93/100 Running Loss: 1.5732513666152954\n",
      "tensor([[0.1210, 0.2907, 0.2343, 0.2140, 0.1400],\n",
      "        [0.1172, 0.2666, 0.2404, 0.2254, 0.1504],\n",
      "        [0.1165, 0.2676, 0.2412, 0.2252, 0.1495],\n",
      "        [0.1155, 0.2659, 0.2452, 0.2235, 0.1499],\n",
      "        [0.1301, 0.2778, 0.2310, 0.2124, 0.1487],\n",
      "        [0.1251, 0.2822, 0.2333, 0.2143, 0.1451],\n",
      "        [0.1347, 0.2818, 0.2257, 0.2082, 0.1495],\n",
      "        [0.1257, 0.2835, 0.2316, 0.2142, 0.1450],\n",
      "        [0.1331, 0.2704, 0.2292, 0.2137, 0.1536],\n",
      "        [0.1214, 0.2633, 0.2407, 0.2212, 0.1534]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "94/100 Running Loss: 1.5722229480743408\n",
      "tensor([[0.1189, 0.2934, 0.2361, 0.2136, 0.1380],\n",
      "        [0.1151, 0.2682, 0.2426, 0.2254, 0.1487],\n",
      "        [0.1143, 0.2693, 0.2434, 0.2252, 0.1478],\n",
      "        [0.1133, 0.2677, 0.2475, 0.2234, 0.1480],\n",
      "        [0.1281, 0.2800, 0.2327, 0.2121, 0.1470],\n",
      "        [0.1230, 0.2845, 0.2351, 0.2140, 0.1433],\n",
      "        [0.1328, 0.2839, 0.2274, 0.2080, 0.1479],\n",
      "        [0.1237, 0.2859, 0.2333, 0.2140, 0.1432],\n",
      "        [0.1312, 0.2722, 0.2309, 0.2135, 0.1521],\n",
      "        [0.1193, 0.2648, 0.2429, 0.2212, 0.1518]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "95/100 Running Loss: 1.5711677074432373\n",
      "tensor([[0.1167, 0.2962, 0.2379, 0.2132, 0.1360],\n",
      "        [0.1129, 0.2700, 0.2448, 0.2253, 0.1469],\n",
      "        [0.1121, 0.2711, 0.2456, 0.2251, 0.1460],\n",
      "        [0.1111, 0.2696, 0.2499, 0.2233, 0.1461],\n",
      "        [0.1262, 0.2822, 0.2345, 0.2118, 0.1453],\n",
      "        [0.1210, 0.2870, 0.2370, 0.2137, 0.1414],\n",
      "        [0.1309, 0.2860, 0.2290, 0.2077, 0.1463],\n",
      "        [0.1216, 0.2883, 0.2351, 0.2136, 0.1413],\n",
      "        [0.1294, 0.2741, 0.2327, 0.2134, 0.1505],\n",
      "        [0.1172, 0.2663, 0.2451, 0.2211, 0.1502]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "96/100 Running Loss: 1.5700856447219849\n",
      "tensor([[0.1145, 0.2989, 0.2398, 0.2128, 0.1339],\n",
      "        [0.1107, 0.2719, 0.2470, 0.2253, 0.1451],\n",
      "        [0.1099, 0.2730, 0.2479, 0.2250, 0.1441],\n",
      "        [0.1088, 0.2716, 0.2523, 0.2231, 0.1441],\n",
      "        [0.1242, 0.2846, 0.2363, 0.2114, 0.1435],\n",
      "        [0.1189, 0.2895, 0.2388, 0.2133, 0.1395],\n",
      "        [0.1290, 0.2884, 0.2307, 0.2074, 0.1446],\n",
      "        [0.1196, 0.2909, 0.2369, 0.2132, 0.1394],\n",
      "        [0.1275, 0.2761, 0.2344, 0.2131, 0.1489],\n",
      "        [0.1151, 0.2680, 0.2473, 0.2211, 0.1485]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "97/100 Running Loss: 1.5689852237701416\n",
      "tensor([[0.1122, 0.3018, 0.2418, 0.2123, 0.1318],\n",
      "        [0.1085, 0.2737, 0.2493, 0.2252, 0.1433],\n",
      "        [0.1077, 0.2749, 0.2503, 0.2249, 0.1422],\n",
      "        [0.1065, 0.2735, 0.2549, 0.2229, 0.1421],\n",
      "        [0.1221, 0.2870, 0.2381, 0.2110, 0.1417],\n",
      "        [0.1167, 0.2921, 0.2407, 0.2129, 0.1376],\n",
      "        [0.1271, 0.2908, 0.2323, 0.2070, 0.1428],\n",
      "        [0.1174, 0.2935, 0.2387, 0.2128, 0.1375],\n",
      "        [0.1256, 0.2781, 0.2362, 0.2129, 0.1473],\n",
      "        [0.1130, 0.2698, 0.2496, 0.2209, 0.1467]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "98/100 Running Loss: 1.5678670406341553\n",
      "tensor([[0.1100, 0.3048, 0.2437, 0.2118, 0.1296],\n",
      "        [0.1063, 0.2756, 0.2517, 0.2250, 0.1414],\n",
      "        [0.1054, 0.2769, 0.2527, 0.2247, 0.1403],\n",
      "        [0.1042, 0.2755, 0.2575, 0.2227, 0.1401],\n",
      "        [0.1201, 0.2894, 0.2400, 0.2106, 0.1399],\n",
      "        [0.1146, 0.2948, 0.2427, 0.2124, 0.1356],\n",
      "        [0.1251, 0.2933, 0.2340, 0.2066, 0.1410],\n",
      "        [0.1153, 0.2962, 0.2406, 0.2123, 0.1356],\n",
      "        [0.1236, 0.2802, 0.2380, 0.2126, 0.1456],\n",
      "        [0.1108, 0.2715, 0.2520, 0.2208, 0.1449]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "99/100 Running Loss: 1.5667166709899902\n",
      "tensor([[0.1076, 0.3080, 0.2457, 0.2113, 0.1274],\n",
      "        [0.1040, 0.2775, 0.2542, 0.2248, 0.1394],\n",
      "        [0.1031, 0.2788, 0.2552, 0.2245, 0.1383],\n",
      "        [0.1019, 0.2776, 0.2602, 0.2224, 0.1380],\n",
      "        [0.1180, 0.2919, 0.2419, 0.2101, 0.1381],\n",
      "        [0.1124, 0.2975, 0.2446, 0.2118, 0.1336],\n",
      "        [0.1231, 0.2959, 0.2357, 0.2062, 0.1391],\n",
      "        [0.1131, 0.2990, 0.2425, 0.2118, 0.1336],\n",
      "        [0.1216, 0.2823, 0.2399, 0.2123, 0.1439],\n",
      "        [0.1087, 0.2733, 0.2544, 0.2206, 0.1431]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "100/100 Running Loss: 1.5655393600463867\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.38      1.00      0.55        20\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.00      0.00      0.00        11\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "   micro avg       0.38      0.38      0.38        53\n",
      "   macro avg       0.09      0.25      0.14        53\n",
      "weighted avg       0.14      0.38      0.21        53\n",
      "\n",
      "0.13698630136986303\n",
      "tensor([[0.2035, 0.2093, 0.1872, 0.2081, 0.1919],\n",
      "        [0.2026, 0.2098, 0.1876, 0.2073, 0.1927],\n",
      "        [0.2026, 0.2097, 0.1877, 0.2073, 0.1927],\n",
      "        [0.2027, 0.2105, 0.1865, 0.2074, 0.1929],\n",
      "        [0.2030, 0.2099, 0.1870, 0.2079, 0.1922],\n",
      "        [0.2030, 0.2099, 0.1869, 0.2079, 0.1923],\n",
      "        [0.2029, 0.2093, 0.1880, 0.2076, 0.1922],\n",
      "        [0.2028, 0.2100, 0.1869, 0.2080, 0.1924],\n",
      "        [0.2028, 0.2098, 0.1871, 0.2078, 0.1924],\n",
      "        [0.2027, 0.2103, 0.1869, 0.2072, 0.1930]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "1/100 Running Loss: 1.6086663007736206\n",
      "tensor([[0.2031, 0.2097, 0.1873, 0.2081, 0.1917],\n",
      "        [0.2022, 0.2101, 0.1878, 0.2074, 0.1926],\n",
      "        [0.2022, 0.2100, 0.1878, 0.2074, 0.1926],\n",
      "        [0.2023, 0.2109, 0.1867, 0.2073, 0.1927],\n",
      "        [0.2027, 0.2102, 0.1871, 0.2080, 0.1921],\n",
      "        [0.2026, 0.2103, 0.1871, 0.2079, 0.1922],\n",
      "        [0.2025, 0.2098, 0.1881, 0.2075, 0.1920],\n",
      "        [0.2024, 0.2104, 0.1870, 0.2080, 0.1922],\n",
      "        [0.2025, 0.2102, 0.1873, 0.2078, 0.1922],\n",
      "        [0.2023, 0.2107, 0.1870, 0.2072, 0.1928]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "2/100 Running Loss: 1.6084768772125244\n",
      "tensor([[0.2028, 0.2101, 0.1874, 0.2082, 0.1915],\n",
      "        [0.2018, 0.2104, 0.1879, 0.2074, 0.1925],\n",
      "        [0.2018, 0.2104, 0.1880, 0.2075, 0.1924],\n",
      "        [0.2019, 0.2113, 0.1869, 0.2073, 0.1926],\n",
      "        [0.2023, 0.2105, 0.1872, 0.2080, 0.1919],\n",
      "        [0.2022, 0.2106, 0.1872, 0.2079, 0.1920],\n",
      "        [0.2022, 0.2103, 0.1882, 0.2075, 0.1918],\n",
      "        [0.2020, 0.2107, 0.1872, 0.2080, 0.1921],\n",
      "        [0.2021, 0.2105, 0.1874, 0.2079, 0.1921],\n",
      "        [0.2019, 0.2111, 0.1872, 0.2072, 0.1926]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "3/100 Running Loss: 1.6082881689071655\n",
      "tensor([[0.2024, 0.2106, 0.1875, 0.2082, 0.1914],\n",
      "        [0.2014, 0.2108, 0.1881, 0.2075, 0.1923],\n",
      "        [0.2014, 0.2107, 0.1881, 0.2075, 0.1923],\n",
      "        [0.2015, 0.2117, 0.1870, 0.2073, 0.1924],\n",
      "        [0.2020, 0.2109, 0.1874, 0.2080, 0.1918],\n",
      "        [0.2019, 0.2110, 0.1873, 0.2079, 0.1919],\n",
      "        [0.2018, 0.2108, 0.1883, 0.2075, 0.1916],\n",
      "        [0.2017, 0.2111, 0.1873, 0.2079, 0.1920],\n",
      "        [0.2018, 0.2109, 0.1875, 0.2079, 0.1919],\n",
      "        [0.2015, 0.2114, 0.1873, 0.2072, 0.1924]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "4/100 Running Loss: 1.608100175857544\n",
      "tensor([[0.2021, 0.2110, 0.1876, 0.2082, 0.1912],\n",
      "        [0.2010, 0.2111, 0.1882, 0.2075, 0.1922],\n",
      "        [0.2010, 0.2111, 0.1883, 0.2076, 0.1921],\n",
      "        [0.2011, 0.2122, 0.1872, 0.2073, 0.1923],\n",
      "        [0.2017, 0.2112, 0.1875, 0.2080, 0.1916],\n",
      "        [0.2015, 0.2114, 0.1875, 0.2079, 0.1917],\n",
      "        [0.2014, 0.2113, 0.1884, 0.2075, 0.1914],\n",
      "        [0.2013, 0.2115, 0.1875, 0.2079, 0.1918],\n",
      "        [0.2015, 0.2112, 0.1877, 0.2079, 0.1917],\n",
      "        [0.2012, 0.2118, 0.1875, 0.2073, 0.1923]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "5/100 Running Loss: 1.6079107522964478\n",
      "tensor([[0.2017, 0.2114, 0.1877, 0.2082, 0.1910],\n",
      "        [0.2006, 0.2115, 0.1884, 0.2075, 0.1921],\n",
      "        [0.2006, 0.2114, 0.1884, 0.2076, 0.1920],\n",
      "        [0.2007, 0.2126, 0.1873, 0.2073, 0.1921],\n",
      "        [0.2013, 0.2116, 0.1876, 0.2080, 0.1914],\n",
      "        [0.2012, 0.2118, 0.1876, 0.2079, 0.1916],\n",
      "        [0.2011, 0.2117, 0.1886, 0.2074, 0.1912],\n",
      "        [0.2010, 0.2119, 0.1876, 0.2079, 0.1917],\n",
      "        [0.2011, 0.2116, 0.1878, 0.2079, 0.1916],\n",
      "        [0.2008, 0.2122, 0.1877, 0.2073, 0.1921]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "6/100 Running Loss: 1.607719898223877\n",
      "tensor([[0.2013, 0.2119, 0.1878, 0.2082, 0.1908],\n",
      "        [0.2002, 0.2118, 0.1885, 0.2075, 0.1919],\n",
      "        [0.2002, 0.2118, 0.1886, 0.2076, 0.1918],\n",
      "        [0.2003, 0.2130, 0.1875, 0.2073, 0.1919],\n",
      "        [0.2010, 0.2120, 0.1878, 0.2080, 0.1913],\n",
      "        [0.2008, 0.2122, 0.1877, 0.2078, 0.1914],\n",
      "        [0.2007, 0.2122, 0.1887, 0.2074, 0.1910],\n",
      "        [0.2006, 0.2123, 0.1878, 0.2078, 0.1915],\n",
      "        [0.2008, 0.2119, 0.1880, 0.2079, 0.1914],\n",
      "        [0.2004, 0.2126, 0.1879, 0.2073, 0.1919]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "7/100 Running Loss: 1.607526421546936\n",
      "tensor([[0.2010, 0.2123, 0.1880, 0.2082, 0.1906],\n",
      "        [0.1998, 0.2122, 0.1887, 0.2076, 0.1918],\n",
      "        [0.1998, 0.2122, 0.1887, 0.2076, 0.1917],\n",
      "        [0.1999, 0.2134, 0.1877, 0.2073, 0.1917],\n",
      "        [0.2006, 0.2124, 0.1879, 0.2079, 0.1911],\n",
      "        [0.2005, 0.2127, 0.1879, 0.2078, 0.1912],\n",
      "        [0.2003, 0.2127, 0.1888, 0.2074, 0.1908],\n",
      "        [0.2003, 0.2127, 0.1879, 0.2078, 0.1913],\n",
      "        [0.2004, 0.2123, 0.1882, 0.2079, 0.1912],\n",
      "        [0.2000, 0.2129, 0.1881, 0.2074, 0.1917]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "8/100 Running Loss: 1.6073311567306519\n",
      "tensor([[0.2006, 0.2127, 0.1881, 0.2082, 0.1905],\n",
      "        [0.1993, 0.2126, 0.1888, 0.2076, 0.1917],\n",
      "        [0.1994, 0.2126, 0.1889, 0.2076, 0.1916],\n",
      "        [0.1995, 0.2139, 0.1879, 0.2073, 0.1915],\n",
      "        [0.2003, 0.2128, 0.1881, 0.2079, 0.1909],\n",
      "        [0.2001, 0.2131, 0.1880, 0.2077, 0.1910],\n",
      "        [0.1999, 0.2132, 0.1889, 0.2073, 0.1906],\n",
      "        [0.2000, 0.2131, 0.1881, 0.2078, 0.1911],\n",
      "        [0.2001, 0.2126, 0.1883, 0.2079, 0.1910],\n",
      "        [0.1996, 0.2133, 0.1883, 0.2074, 0.1915]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "9/100 Running Loss: 1.6071336269378662\n",
      "tensor([[0.2002, 0.2132, 0.1882, 0.2081, 0.1903],\n",
      "        [0.1989, 0.2130, 0.1890, 0.2076, 0.1915],\n",
      "        [0.1990, 0.2129, 0.1890, 0.2076, 0.1915],\n",
      "        [0.1991, 0.2143, 0.1880, 0.2073, 0.1913],\n",
      "        [0.1999, 0.2132, 0.1883, 0.2079, 0.1907],\n",
      "        [0.1998, 0.2136, 0.1882, 0.2077, 0.1908],\n",
      "        [0.1995, 0.2137, 0.1890, 0.2073, 0.1904],\n",
      "        [0.1996, 0.2136, 0.1882, 0.2077, 0.1909],\n",
      "        [0.1997, 0.2130, 0.1885, 0.2079, 0.1909],\n",
      "        [0.1991, 0.2137, 0.1885, 0.2074, 0.1913]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "10/100 Running Loss: 1.6069343090057373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1998, 0.2136, 0.1884, 0.2081, 0.1901],\n",
      "        [0.1985, 0.2133, 0.1891, 0.2076, 0.1914],\n",
      "        [0.1985, 0.2133, 0.1892, 0.2076, 0.1913],\n",
      "        [0.1987, 0.2148, 0.1882, 0.2072, 0.1911],\n",
      "        [0.1995, 0.2137, 0.1884, 0.2079, 0.1905],\n",
      "        [0.1994, 0.2140, 0.1884, 0.2076, 0.1906],\n",
      "        [0.1991, 0.2142, 0.1892, 0.2073, 0.1902],\n",
      "        [0.1992, 0.2141, 0.1884, 0.2077, 0.1907],\n",
      "        [0.1993, 0.2134, 0.1887, 0.2079, 0.1907],\n",
      "        [0.1987, 0.2140, 0.1887, 0.2075, 0.1911]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "11/100 Running Loss: 1.606732964515686\n",
      "tensor([[0.1994, 0.2141, 0.1885, 0.2081, 0.1899],\n",
      "        [0.1981, 0.2137, 0.1893, 0.2076, 0.1913],\n",
      "        [0.1981, 0.2137, 0.1894, 0.2076, 0.1912],\n",
      "        [0.1983, 0.2152, 0.1884, 0.2072, 0.1909],\n",
      "        [0.1992, 0.2141, 0.1886, 0.2078, 0.1903],\n",
      "        [0.1990, 0.2145, 0.1886, 0.2075, 0.1904],\n",
      "        [0.1987, 0.2147, 0.1893, 0.2073, 0.1900],\n",
      "        [0.1989, 0.2145, 0.1885, 0.2076, 0.1904],\n",
      "        [0.1990, 0.2138, 0.1888, 0.2079, 0.1905],\n",
      "        [0.1983, 0.2144, 0.1889, 0.2075, 0.1909]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "12/100 Running Loss: 1.6065289974212646\n",
      "tensor([[0.1990, 0.2145, 0.1886, 0.2081, 0.1897],\n",
      "        [0.1977, 0.2141, 0.1895, 0.2076, 0.1911],\n",
      "        [0.1977, 0.2141, 0.1896, 0.2076, 0.1910],\n",
      "        [0.1979, 0.2157, 0.1886, 0.2072, 0.1907],\n",
      "        [0.1988, 0.2146, 0.1888, 0.2078, 0.1901],\n",
      "        [0.1986, 0.2150, 0.1888, 0.2075, 0.1901],\n",
      "        [0.1984, 0.2152, 0.1894, 0.2072, 0.1898],\n",
      "        [0.1985, 0.2150, 0.1887, 0.2076, 0.1902],\n",
      "        [0.1986, 0.2142, 0.1890, 0.2079, 0.1904],\n",
      "        [0.1979, 0.2148, 0.1891, 0.2075, 0.1907]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "13/100 Running Loss: 1.6063202619552612\n",
      "tensor([[0.1986, 0.2150, 0.1888, 0.2081, 0.1895],\n",
      "        [0.1973, 0.2145, 0.1897, 0.2076, 0.1910],\n",
      "        [0.1973, 0.2145, 0.1897, 0.2076, 0.1909],\n",
      "        [0.1975, 0.2161, 0.1888, 0.2071, 0.1905],\n",
      "        [0.1984, 0.2150, 0.1890, 0.2078, 0.1898],\n",
      "        [0.1982, 0.2155, 0.1890, 0.2074, 0.1899],\n",
      "        [0.1980, 0.2157, 0.1895, 0.2071, 0.1897],\n",
      "        [0.1981, 0.2155, 0.1889, 0.2075, 0.1900],\n",
      "        [0.1982, 0.2146, 0.1892, 0.2079, 0.1902],\n",
      "        [0.1975, 0.2151, 0.1893, 0.2075, 0.1905]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "14/100 Running Loss: 1.6061151027679443\n",
      "tensor([[0.1982, 0.2155, 0.1889, 0.2080, 0.1893],\n",
      "        [0.1969, 0.2149, 0.1898, 0.2075, 0.1909],\n",
      "        [0.1969, 0.2149, 0.1899, 0.2075, 0.1908],\n",
      "        [0.1971, 0.2165, 0.1890, 0.2071, 0.1903],\n",
      "        [0.1980, 0.2155, 0.1891, 0.2078, 0.1896],\n",
      "        [0.1978, 0.2160, 0.1891, 0.2074, 0.1897],\n",
      "        [0.1976, 0.2163, 0.1896, 0.2070, 0.1895],\n",
      "        [0.1977, 0.2159, 0.1891, 0.2075, 0.1898],\n",
      "        [0.1978, 0.2149, 0.1893, 0.2079, 0.1900],\n",
      "        [0.1971, 0.2155, 0.1896, 0.2075, 0.1903]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "15/100 Running Loss: 1.6059092283248901\n",
      "tensor([[0.1978, 0.2160, 0.1891, 0.2080, 0.1891],\n",
      "        [0.1965, 0.2153, 0.1900, 0.2075, 0.1907],\n",
      "        [0.1965, 0.2153, 0.1901, 0.2075, 0.1906],\n",
      "        [0.1968, 0.2170, 0.1892, 0.2070, 0.1901],\n",
      "        [0.1977, 0.2160, 0.1893, 0.2077, 0.1894],\n",
      "        [0.1974, 0.2164, 0.1893, 0.2073, 0.1895],\n",
      "        [0.1973, 0.2168, 0.1897, 0.2069, 0.1893],\n",
      "        [0.1973, 0.2164, 0.1892, 0.2074, 0.1896],\n",
      "        [0.1974, 0.2153, 0.1895, 0.2079, 0.1898],\n",
      "        [0.1967, 0.2159, 0.1898, 0.2075, 0.1901]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "16/100 Running Loss: 1.6057050228118896\n",
      "tensor([[0.1974, 0.2165, 0.1892, 0.2079, 0.1889],\n",
      "        [0.1961, 0.2157, 0.1902, 0.2074, 0.1906],\n",
      "        [0.1960, 0.2157, 0.1903, 0.2075, 0.1905],\n",
      "        [0.1964, 0.2174, 0.1894, 0.2069, 0.1900],\n",
      "        [0.1973, 0.2165, 0.1894, 0.2077, 0.1892],\n",
      "        [0.1970, 0.2169, 0.1895, 0.2072, 0.1893],\n",
      "        [0.1969, 0.2173, 0.1899, 0.2068, 0.1891],\n",
      "        [0.1970, 0.2169, 0.1894, 0.2074, 0.1893],\n",
      "        [0.1971, 0.2157, 0.1897, 0.2079, 0.1896],\n",
      "        [0.1963, 0.2163, 0.1900, 0.2074, 0.1900]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "17/100 Running Loss: 1.6054983139038086\n",
      "tensor([[0.1970, 0.2171, 0.1894, 0.2078, 0.1887],\n",
      "        [0.1956, 0.2161, 0.1904, 0.2074, 0.1905],\n",
      "        [0.1956, 0.2161, 0.1905, 0.2074, 0.1904],\n",
      "        [0.1960, 0.2179, 0.1896, 0.2067, 0.1898],\n",
      "        [0.1969, 0.2170, 0.1896, 0.2076, 0.1890],\n",
      "        [0.1966, 0.2174, 0.1897, 0.2071, 0.1891],\n",
      "        [0.1965, 0.2179, 0.1900, 0.2067, 0.1890],\n",
      "        [0.1966, 0.2174, 0.1896, 0.2073, 0.1891],\n",
      "        [0.1967, 0.2161, 0.1899, 0.2079, 0.1894],\n",
      "        [0.1960, 0.2167, 0.1902, 0.2074, 0.1898]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "18/100 Running Loss: 1.605289340019226\n",
      "tensor([[0.1966, 0.2176, 0.1895, 0.2077, 0.1886],\n",
      "        [0.1953, 0.2165, 0.1906, 0.2073, 0.1904],\n",
      "        [0.1952, 0.2165, 0.1907, 0.2073, 0.1903],\n",
      "        [0.1956, 0.2183, 0.1898, 0.2066, 0.1896],\n",
      "        [0.1965, 0.2175, 0.1898, 0.2075, 0.1888],\n",
      "        [0.1963, 0.2179, 0.1899, 0.2070, 0.1890],\n",
      "        [0.1961, 0.2184, 0.1901, 0.2065, 0.1888],\n",
      "        [0.1963, 0.2179, 0.1897, 0.2072, 0.1889],\n",
      "        [0.1964, 0.2165, 0.1900, 0.2078, 0.1893],\n",
      "        [0.1956, 0.2171, 0.1904, 0.2073, 0.1896]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "19/100 Running Loss: 1.6050783395767212\n",
      "tensor([[0.1962, 0.2182, 0.1896, 0.2075, 0.1884],\n",
      "        [0.1949, 0.2168, 0.1908, 0.2073, 0.1902],\n",
      "        [0.1949, 0.2169, 0.1909, 0.2073, 0.1901],\n",
      "        [0.1952, 0.2188, 0.1901, 0.2065, 0.1895],\n",
      "        [0.1961, 0.2180, 0.1899, 0.2074, 0.1886],\n",
      "        [0.1959, 0.2184, 0.1901, 0.2069, 0.1888],\n",
      "        [0.1958, 0.2190, 0.1903, 0.2064, 0.1886],\n",
      "        [0.1959, 0.2184, 0.1899, 0.2071, 0.1887],\n",
      "        [0.1960, 0.2169, 0.1902, 0.2077, 0.1891],\n",
      "        [0.1952, 0.2175, 0.1907, 0.2073, 0.1894]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "20/100 Running Loss: 1.604864478111267\n",
      "tensor([[0.1958, 0.2188, 0.1898, 0.2074, 0.1882],\n",
      "        [0.1945, 0.2172, 0.1910, 0.2072, 0.1900],\n",
      "        [0.1945, 0.2173, 0.1911, 0.2072, 0.1900],\n",
      "        [0.1947, 0.2193, 0.1903, 0.2063, 0.1893],\n",
      "        [0.1957, 0.2185, 0.1901, 0.2073, 0.1884],\n",
      "        [0.1955, 0.2189, 0.1902, 0.2068, 0.1886],\n",
      "        [0.1954, 0.2196, 0.1904, 0.2063, 0.1884],\n",
      "        [0.1955, 0.2189, 0.1900, 0.2070, 0.1885],\n",
      "        [0.1957, 0.2174, 0.1904, 0.2076, 0.1890],\n",
      "        [0.1949, 0.2179, 0.1908, 0.2072, 0.1892]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "21/100 Running Loss: 1.604648232460022\n",
      "tensor([[0.1954, 0.2193, 0.1900, 0.2072, 0.1881],\n",
      "        [0.1942, 0.2177, 0.1912, 0.2071, 0.1899],\n",
      "        [0.1941, 0.2177, 0.1912, 0.2071, 0.1898],\n",
      "        [0.1943, 0.2198, 0.1905, 0.2062, 0.1892],\n",
      "        [0.1953, 0.2190, 0.1902, 0.2072, 0.1883],\n",
      "        [0.1951, 0.2194, 0.1904, 0.2067, 0.1884],\n",
      "        [0.1950, 0.2202, 0.1905, 0.2061, 0.1882],\n",
      "        [0.1952, 0.2195, 0.1902, 0.2068, 0.1883],\n",
      "        [0.1953, 0.2178, 0.1906, 0.2075, 0.1888],\n",
      "        [0.1945, 0.2183, 0.1910, 0.2071, 0.1890]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "22/100 Running Loss: 1.6044318675994873\n",
      "tensor([[0.1950, 0.2199, 0.1901, 0.2071, 0.1879],\n",
      "        [0.1938, 0.2181, 0.1913, 0.2071, 0.1897],\n",
      "        [0.1938, 0.2181, 0.1914, 0.2070, 0.1896],\n",
      "        [0.1939, 0.2203, 0.1908, 0.2061, 0.1890],\n",
      "        [0.1949, 0.2195, 0.1904, 0.2071, 0.1881],\n",
      "        [0.1947, 0.2199, 0.1906, 0.2066, 0.1882],\n",
      "        [0.1946, 0.2207, 0.1907, 0.2059, 0.1881],\n",
      "        [0.1948, 0.2200, 0.1904, 0.2067, 0.1882],\n",
      "        [0.1949, 0.2182, 0.1908, 0.2074, 0.1887],\n",
      "        [0.1941, 0.2188, 0.1912, 0.2070, 0.1888]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "23/100 Running Loss: 1.6042141914367676\n",
      "tensor([[0.1946, 0.2204, 0.1903, 0.2069, 0.1878],\n",
      "        [0.1934, 0.2185, 0.1915, 0.2070, 0.1896],\n",
      "        [0.1934, 0.2185, 0.1916, 0.2070, 0.1895],\n",
      "        [0.1935, 0.2207, 0.1910, 0.2060, 0.1888],\n",
      "        [0.1946, 0.2200, 0.1906, 0.2070, 0.1879],\n",
      "        [0.1944, 0.2205, 0.1907, 0.2064, 0.1880],\n",
      "        [0.1942, 0.2213, 0.1908, 0.2058, 0.1879],\n",
      "        [0.1944, 0.2205, 0.1905, 0.2066, 0.1880],\n",
      "        [0.1946, 0.2186, 0.1910, 0.2073, 0.1885],\n",
      "        [0.1938, 0.2193, 0.1914, 0.2069, 0.1886]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "24/100 Running Loss: 1.6039942502975464\n",
      "tensor([[0.1941, 0.2210, 0.1905, 0.2067, 0.1877],\n",
      "        [0.1931, 0.2188, 0.1917, 0.2070, 0.1894],\n",
      "        [0.1930, 0.2189, 0.1918, 0.2070, 0.1893],\n",
      "        [0.1931, 0.2212, 0.1912, 0.2059, 0.1886],\n",
      "        [0.1942, 0.2205, 0.1907, 0.2068, 0.1877],\n",
      "        [0.1940, 0.2210, 0.1909, 0.2063, 0.1878],\n",
      "        [0.1939, 0.2219, 0.1910, 0.2056, 0.1877],\n",
      "        [0.1940, 0.2210, 0.1907, 0.2066, 0.1878],\n",
      "        [0.1942, 0.2190, 0.1912, 0.2072, 0.1883],\n",
      "        [0.1934, 0.2197, 0.1916, 0.2068, 0.1885]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "25/100 Running Loss: 1.6037750244140625\n",
      "tensor([[0.1937, 0.2216, 0.1906, 0.2066, 0.1875],\n",
      "        [0.1926, 0.2192, 0.1919, 0.2070, 0.1892],\n",
      "        [0.1926, 0.2193, 0.1920, 0.2070, 0.1891],\n",
      "        [0.1926, 0.2217, 0.1914, 0.2058, 0.1885],\n",
      "        [0.1939, 0.2211, 0.1909, 0.2067, 0.1875],\n",
      "        [0.1936, 0.2215, 0.1911, 0.2063, 0.1876],\n",
      "        [0.1935, 0.2225, 0.1911, 0.2055, 0.1875],\n",
      "        [0.1936, 0.2215, 0.1909, 0.2065, 0.1876],\n",
      "        [0.1939, 0.2195, 0.1913, 0.2071, 0.1882],\n",
      "        [0.1931, 0.2202, 0.1918, 0.2067, 0.1883]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "26/100 Running Loss: 1.603555679321289\n",
      "tensor([[0.1933, 0.2221, 0.1908, 0.2065, 0.1873],\n",
      "        [0.1922, 0.2196, 0.1921, 0.2070, 0.1891],\n",
      "        [0.1922, 0.2197, 0.1922, 0.2070, 0.1890],\n",
      "        [0.1922, 0.2221, 0.1916, 0.2057, 0.1883],\n",
      "        [0.1935, 0.2216, 0.1910, 0.2066, 0.1873],\n",
      "        [0.1932, 0.2220, 0.1913, 0.2062, 0.1874],\n",
      "        [0.1931, 0.2230, 0.1913, 0.2053, 0.1873],\n",
      "        [0.1932, 0.2220, 0.1911, 0.2064, 0.1873],\n",
      "        [0.1936, 0.2199, 0.1915, 0.2070, 0.1880],\n",
      "        [0.1927, 0.2207, 0.1919, 0.2066, 0.1881]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "27/100 Running Loss: 1.6033374071121216\n",
      "tensor([[0.1929, 0.2226, 0.1910, 0.2064, 0.1871],\n",
      "        [0.1918, 0.2200, 0.1923, 0.2070, 0.1889],\n",
      "        [0.1918, 0.2201, 0.1924, 0.2069, 0.1888],\n",
      "        [0.1918, 0.2225, 0.1918, 0.2057, 0.1881],\n",
      "        [0.1931, 0.2221, 0.1912, 0.2065, 0.1871],\n",
      "        [0.1928, 0.2225, 0.1914, 0.2061, 0.1872],\n",
      "        [0.1927, 0.2236, 0.1914, 0.2052, 0.1870],\n",
      "        [0.1928, 0.2225, 0.1912, 0.2063, 0.1871],\n",
      "        [0.1933, 0.2204, 0.1917, 0.2069, 0.1878],\n",
      "        [0.1923, 0.2211, 0.1921, 0.2065, 0.1879]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "28/100 Running Loss: 1.603122591972351\n",
      "tensor([[0.1925, 0.2231, 0.1912, 0.2063, 0.1869],\n",
      "        [0.1914, 0.2204, 0.1925, 0.2070, 0.1887],\n",
      "        [0.1914, 0.2204, 0.1926, 0.2070, 0.1886],\n",
      "        [0.1915, 0.2229, 0.1920, 0.2056, 0.1879],\n",
      "        [0.1928, 0.2226, 0.1913, 0.2064, 0.1869],\n",
      "        [0.1925, 0.2229, 0.1916, 0.2061, 0.1869],\n",
      "        [0.1924, 0.2241, 0.1916, 0.2051, 0.1868],\n",
      "        [0.1925, 0.2229, 0.1914, 0.2063, 0.1869],\n",
      "        [0.1929, 0.2208, 0.1918, 0.2068, 0.1876],\n",
      "        [0.1920, 0.2216, 0.1922, 0.2065, 0.1877]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "29/100 Running Loss: 1.602912187576294\n",
      "tensor([[0.1921, 0.2235, 0.1914, 0.2062, 0.1868],\n",
      "        [0.1911, 0.2207, 0.1927, 0.2070, 0.1885],\n",
      "        [0.1910, 0.2207, 0.1928, 0.2070, 0.1885],\n",
      "        [0.1911, 0.2233, 0.1923, 0.2056, 0.1877],\n",
      "        [0.1925, 0.2230, 0.1915, 0.2064, 0.1866],\n",
      "        [0.1921, 0.2234, 0.1917, 0.2060, 0.1867],\n",
      "        [0.1920, 0.2246, 0.1918, 0.2050, 0.1866],\n",
      "        [0.1921, 0.2234, 0.1916, 0.2062, 0.1867],\n",
      "        [0.1926, 0.2213, 0.1920, 0.2068, 0.1874],\n",
      "        [0.1916, 0.2220, 0.1924, 0.2064, 0.1876]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "30/100 Running Loss: 1.6027042865753174\n",
      "tensor([[0.1917, 0.2240, 0.1916, 0.2061, 0.1866],\n",
      "        [0.1908, 0.2210, 0.1929, 0.2070, 0.1883],\n",
      "        [0.1907, 0.2211, 0.1930, 0.2070, 0.1883],\n",
      "        [0.1908, 0.2237, 0.1925, 0.2055, 0.1876],\n",
      "        [0.1921, 0.2235, 0.1917, 0.2063, 0.1864],\n",
      "        [0.1917, 0.2239, 0.1919, 0.2060, 0.1865],\n",
      "        [0.1917, 0.2251, 0.1919, 0.2049, 0.1864],\n",
      "        [0.1918, 0.2239, 0.1917, 0.2061, 0.1865],\n",
      "        [0.1922, 0.2217, 0.1922, 0.2067, 0.1872],\n",
      "        [0.1911, 0.2224, 0.1926, 0.2064, 0.1874]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "31/100 Running Loss: 1.602491855621338\n",
      "tensor([[0.1914, 0.2245, 0.1917, 0.2060, 0.1864],\n",
      "        [0.1904, 0.2213, 0.1931, 0.2071, 0.1881],\n",
      "        [0.1903, 0.2214, 0.1931, 0.2071, 0.1881],\n",
      "        [0.1904, 0.2241, 0.1927, 0.2054, 0.1874],\n",
      "        [0.1918, 0.2239, 0.1918, 0.2062, 0.1862],\n",
      "        [0.1913, 0.2244, 0.1920, 0.2060, 0.1863],\n",
      "        [0.1913, 0.2256, 0.1921, 0.2048, 0.1862],\n",
      "        [0.1914, 0.2243, 0.1919, 0.2061, 0.1863],\n",
      "        [0.1918, 0.2222, 0.1923, 0.2067, 0.1870],\n",
      "        [0.1908, 0.2228, 0.1928, 0.2063, 0.1873]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "32/100 Running Loss: 1.602274775505066\n",
      "tensor([[0.1910, 0.2250, 0.1919, 0.2059, 0.1861],\n",
      "        [0.1901, 0.2217, 0.1933, 0.2071, 0.1879],\n",
      "        [0.1900, 0.2217, 0.1933, 0.2071, 0.1878],\n",
      "        [0.1900, 0.2245, 0.1930, 0.2054, 0.1872],\n",
      "        [0.1914, 0.2244, 0.1920, 0.2062, 0.1860],\n",
      "        [0.1909, 0.2249, 0.1922, 0.2060, 0.1861],\n",
      "        [0.1910, 0.2261, 0.1923, 0.2047, 0.1859],\n",
      "        [0.1910, 0.2248, 0.1920, 0.2060, 0.1861],\n",
      "        [0.1915, 0.2226, 0.1925, 0.2067, 0.1867],\n",
      "        [0.1904, 0.2232, 0.1930, 0.2063, 0.1871]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "33/100 Running Loss: 1.6020514965057373\n",
      "tensor([[0.1906, 0.2256, 0.1921, 0.2059, 0.1859],\n",
      "        [0.1897, 0.2220, 0.1934, 0.2072, 0.1877],\n",
      "        [0.1896, 0.2221, 0.1935, 0.2072, 0.1876],\n",
      "        [0.1896, 0.2249, 0.1932, 0.2053, 0.1869],\n",
      "        [0.1910, 0.2249, 0.1921, 0.2061, 0.1858],\n",
      "        [0.1905, 0.2254, 0.1923, 0.2060, 0.1858],\n",
      "        [0.1907, 0.2266, 0.1924, 0.2046, 0.1857],\n",
      "        [0.1906, 0.2253, 0.1922, 0.2060, 0.1858],\n",
      "        [0.1911, 0.2230, 0.1927, 0.2067, 0.1865],\n",
      "        [0.1900, 0.2236, 0.1932, 0.2063, 0.1869]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "34/100 Running Loss: 1.601823091506958\n",
      "tensor([[0.1902, 0.2261, 0.1922, 0.2058, 0.1857],\n",
      "        [0.1893, 0.2224, 0.1936, 0.2073, 0.1875],\n",
      "        [0.1892, 0.2224, 0.1937, 0.2072, 0.1874],\n",
      "        [0.1892, 0.2254, 0.1934, 0.2052, 0.1867],\n",
      "        [0.1906, 0.2254, 0.1923, 0.2061, 0.1855],\n",
      "        [0.1901, 0.2259, 0.1924, 0.2060, 0.1856],\n",
      "        [0.1904, 0.2271, 0.1926, 0.2045, 0.1854],\n",
      "        [0.1902, 0.2259, 0.1923, 0.2060, 0.1855],\n",
      "        [0.1908, 0.2234, 0.1928, 0.2067, 0.1863],\n",
      "        [0.1896, 0.2241, 0.1934, 0.2063, 0.1867]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "35/100 Running Loss: 1.601584792137146\n",
      "tensor([[0.1898, 0.2267, 0.1923, 0.2058, 0.1854],\n",
      "        [0.1889, 0.2227, 0.1939, 0.2073, 0.1872],\n",
      "        [0.1888, 0.2228, 0.1939, 0.2073, 0.1872],\n",
      "        [0.1888, 0.2259, 0.1936, 0.2052, 0.1864],\n",
      "        [0.1902, 0.2260, 0.1924, 0.2061, 0.1853],\n",
      "        [0.1897, 0.2265, 0.1926, 0.2060, 0.1853],\n",
      "        [0.1901, 0.2276, 0.1928, 0.2044, 0.1852],\n",
      "        [0.1898, 0.2264, 0.1925, 0.2060, 0.1853],\n",
      "        [0.1904, 0.2238, 0.1930, 0.2066, 0.1861],\n",
      "        [0.1892, 0.2245, 0.1936, 0.2063, 0.1865]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "36/100 Running Loss: 1.6013400554656982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1894, 0.2273, 0.1925, 0.2058, 0.1851],\n",
      "        [0.1884, 0.2231, 0.1941, 0.2074, 0.1870],\n",
      "        [0.1884, 0.2232, 0.1941, 0.2074, 0.1870],\n",
      "        [0.1883, 0.2265, 0.1938, 0.2052, 0.1861],\n",
      "        [0.1898, 0.2266, 0.1925, 0.2061, 0.1850],\n",
      "        [0.1893, 0.2271, 0.1927, 0.2060, 0.1850],\n",
      "        [0.1897, 0.2282, 0.1929, 0.2043, 0.1849],\n",
      "        [0.1894, 0.2269, 0.1926, 0.2061, 0.1850],\n",
      "        [0.1901, 0.2242, 0.1932, 0.2066, 0.1858],\n",
      "        [0.1888, 0.2249, 0.1938, 0.2063, 0.1862]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "37/100 Running Loss: 1.6010875701904297\n",
      "tensor([[0.1889, 0.2279, 0.1926, 0.2057, 0.1848],\n",
      "        [0.1880, 0.2235, 0.1943, 0.2075, 0.1868],\n",
      "        [0.1879, 0.2236, 0.1943, 0.2075, 0.1867],\n",
      "        [0.1878, 0.2271, 0.1940, 0.2053, 0.1858],\n",
      "        [0.1893, 0.2272, 0.1927, 0.2061, 0.1847],\n",
      "        [0.1889, 0.2277, 0.1928, 0.2059, 0.1847],\n",
      "        [0.1893, 0.2288, 0.1930, 0.2043, 0.1846],\n",
      "        [0.1890, 0.2275, 0.1927, 0.2061, 0.1847],\n",
      "        [0.1897, 0.2247, 0.1934, 0.2066, 0.1856],\n",
      "        [0.1883, 0.2253, 0.1940, 0.2064, 0.1860]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "38/100 Running Loss: 1.6008278131484985\n",
      "tensor([[0.1885, 0.2286, 0.1927, 0.2057, 0.1845],\n",
      "        [0.1875, 0.2239, 0.1945, 0.2076, 0.1865],\n",
      "        [0.1874, 0.2240, 0.1945, 0.2076, 0.1865],\n",
      "        [0.1873, 0.2277, 0.1941, 0.2053, 0.1855],\n",
      "        [0.1889, 0.2277, 0.1928, 0.2061, 0.1844],\n",
      "        [0.1885, 0.2282, 0.1930, 0.2059, 0.1844],\n",
      "        [0.1889, 0.2295, 0.1931, 0.2043, 0.1842],\n",
      "        [0.1885, 0.2281, 0.1929, 0.2061, 0.1844],\n",
      "        [0.1892, 0.2252, 0.1936, 0.2067, 0.1854],\n",
      "        [0.1879, 0.2258, 0.1942, 0.2064, 0.1857]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "39/100 Running Loss: 1.600561499595642\n",
      "tensor([[0.1880, 0.2292, 0.1928, 0.2057, 0.1842],\n",
      "        [0.1870, 0.2244, 0.1947, 0.2077, 0.1862],\n",
      "        [0.1870, 0.2244, 0.1947, 0.2077, 0.1862],\n",
      "        [0.1868, 0.2284, 0.1943, 0.2053, 0.1852],\n",
      "        [0.1884, 0.2284, 0.1929, 0.2062, 0.1841],\n",
      "        [0.1880, 0.2288, 0.1931, 0.2060, 0.1840],\n",
      "        [0.1884, 0.2302, 0.1933, 0.2043, 0.1839],\n",
      "        [0.1881, 0.2287, 0.1930, 0.2061, 0.1841],\n",
      "        [0.1888, 0.2257, 0.1937, 0.2067, 0.1851],\n",
      "        [0.1874, 0.2263, 0.1943, 0.2065, 0.1855]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "40/100 Running Loss: 1.600284457206726\n",
      "tensor([[0.1875, 0.2299, 0.1930, 0.2057, 0.1839],\n",
      "        [0.1865, 0.2248, 0.1949, 0.2078, 0.1860],\n",
      "        [0.1865, 0.2249, 0.1949, 0.2078, 0.1859],\n",
      "        [0.1863, 0.2291, 0.1944, 0.2054, 0.1848],\n",
      "        [0.1880, 0.2290, 0.1931, 0.2062, 0.1838],\n",
      "        [0.1876, 0.2295, 0.1933, 0.2060, 0.1837],\n",
      "        [0.1879, 0.2309, 0.1934, 0.2043, 0.1835],\n",
      "        [0.1876, 0.2293, 0.1932, 0.2061, 0.1838],\n",
      "        [0.1883, 0.2263, 0.1939, 0.2068, 0.1848],\n",
      "        [0.1869, 0.2269, 0.1945, 0.2066, 0.1852]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "41/100 Running Loss: 1.5999959707260132\n",
      "tensor([[0.1870, 0.2306, 0.1931, 0.2057, 0.1835],\n",
      "        [0.1860, 0.2253, 0.1951, 0.2079, 0.1857],\n",
      "        [0.1859, 0.2253, 0.1951, 0.2080, 0.1857],\n",
      "        [0.1858, 0.2298, 0.1946, 0.2054, 0.1845],\n",
      "        [0.1875, 0.2296, 0.1932, 0.2062, 0.1834],\n",
      "        [0.1871, 0.2302, 0.1934, 0.2060, 0.1834],\n",
      "        [0.1874, 0.2316, 0.1935, 0.2043, 0.1831],\n",
      "        [0.1871, 0.2300, 0.1933, 0.2061, 0.1834],\n",
      "        [0.1878, 0.2268, 0.1940, 0.2068, 0.1846],\n",
      "        [0.1864, 0.2274, 0.1947, 0.2066, 0.1849]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "42/100 Running Loss: 1.5997042655944824\n",
      "tensor([[0.1865, 0.2313, 0.1933, 0.2057, 0.1832],\n",
      "        [0.1854, 0.2258, 0.1953, 0.2081, 0.1855],\n",
      "        [0.1854, 0.2258, 0.1953, 0.2081, 0.1854],\n",
      "        [0.1852, 0.2305, 0.1947, 0.2055, 0.1841],\n",
      "        [0.1870, 0.2303, 0.1934, 0.2062, 0.1831],\n",
      "        [0.1866, 0.2309, 0.1935, 0.2060, 0.1830],\n",
      "        [0.1869, 0.2324, 0.1937, 0.2043, 0.1827],\n",
      "        [0.1866, 0.2306, 0.1934, 0.2062, 0.1831],\n",
      "        [0.1873, 0.2274, 0.1941, 0.2068, 0.1843],\n",
      "        [0.1858, 0.2280, 0.1949, 0.2067, 0.1846]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "43/100 Running Loss: 1.599404215812683\n",
      "tensor([[0.1860, 0.2321, 0.1934, 0.2057, 0.1828],\n",
      "        [0.1849, 0.2263, 0.1955, 0.2082, 0.1852],\n",
      "        [0.1848, 0.2264, 0.1955, 0.2082, 0.1851],\n",
      "        [0.1846, 0.2312, 0.1949, 0.2055, 0.1837],\n",
      "        [0.1865, 0.2310, 0.1935, 0.2062, 0.1828],\n",
      "        [0.1861, 0.2316, 0.1937, 0.2060, 0.1827],\n",
      "        [0.1864, 0.2332, 0.1938, 0.2042, 0.1823],\n",
      "        [0.1861, 0.2314, 0.1936, 0.2062, 0.1828],\n",
      "        [0.1868, 0.2280, 0.1943, 0.2069, 0.1840],\n",
      "        [0.1853, 0.2286, 0.1950, 0.2068, 0.1843]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "44/100 Running Loss: 1.599096655845642\n",
      "tensor([[0.1854, 0.2329, 0.1935, 0.2057, 0.1824],\n",
      "        [0.1843, 0.2268, 0.1957, 0.2083, 0.1849],\n",
      "        [0.1842, 0.2269, 0.1957, 0.2083, 0.1849],\n",
      "        [0.1841, 0.2320, 0.1951, 0.2055, 0.1834],\n",
      "        [0.1860, 0.2317, 0.1937, 0.2062, 0.1824],\n",
      "        [0.1856, 0.2323, 0.1938, 0.2060, 0.1823],\n",
      "        [0.1859, 0.2341, 0.1939, 0.2042, 0.1819],\n",
      "        [0.1856, 0.2321, 0.1937, 0.2062, 0.1824],\n",
      "        [0.1863, 0.2286, 0.1944, 0.2069, 0.1837],\n",
      "        [0.1847, 0.2292, 0.1952, 0.2068, 0.1840]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "45/100 Running Loss: 1.5987783670425415\n",
      "tensor([[0.1849, 0.2337, 0.1936, 0.2057, 0.1821],\n",
      "        [0.1837, 0.2274, 0.1959, 0.2085, 0.1846],\n",
      "        [0.1836, 0.2275, 0.1959, 0.2085, 0.1846],\n",
      "        [0.1835, 0.2328, 0.1952, 0.2056, 0.1830],\n",
      "        [0.1855, 0.2324, 0.1938, 0.2062, 0.1821],\n",
      "        [0.1850, 0.2331, 0.1939, 0.2060, 0.1820],\n",
      "        [0.1854, 0.2350, 0.1939, 0.2041, 0.1815],\n",
      "        [0.1851, 0.2329, 0.1938, 0.2061, 0.1821],\n",
      "        [0.1858, 0.2292, 0.1946, 0.2070, 0.1834],\n",
      "        [0.1842, 0.2299, 0.1954, 0.2069, 0.1837]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "46/100 Running Loss: 1.5984526872634888\n",
      "tensor([[0.1843, 0.2346, 0.1938, 0.2057, 0.1817],\n",
      "        [0.1831, 0.2279, 0.1961, 0.2086, 0.1843],\n",
      "        [0.1830, 0.2280, 0.1961, 0.2086, 0.1843],\n",
      "        [0.1829, 0.2336, 0.1954, 0.2056, 0.1826],\n",
      "        [0.1849, 0.2332, 0.1940, 0.2062, 0.1817],\n",
      "        [0.1845, 0.2339, 0.1941, 0.2059, 0.1816],\n",
      "        [0.1849, 0.2360, 0.1940, 0.2040, 0.1811],\n",
      "        [0.1846, 0.2336, 0.1940, 0.2061, 0.1817],\n",
      "        [0.1853, 0.2298, 0.1948, 0.2070, 0.1831],\n",
      "        [0.1836, 0.2306, 0.1956, 0.2069, 0.1833]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "47/100 Running Loss: 1.5981175899505615\n",
      "tensor([[0.1837, 0.2354, 0.1939, 0.2057, 0.1813],\n",
      "        [0.1825, 0.2285, 0.1962, 0.2087, 0.1840],\n",
      "        [0.1824, 0.2286, 0.1963, 0.2087, 0.1840],\n",
      "        [0.1823, 0.2345, 0.1955, 0.2056, 0.1822],\n",
      "        [0.1844, 0.2340, 0.1941, 0.2062, 0.1813],\n",
      "        [0.1839, 0.2348, 0.1942, 0.2059, 0.1812],\n",
      "        [0.1844, 0.2370, 0.1941, 0.2040, 0.1806],\n",
      "        [0.1840, 0.2345, 0.1941, 0.2061, 0.1813],\n",
      "        [0.1848, 0.2304, 0.1949, 0.2070, 0.1828],\n",
      "        [0.1830, 0.2313, 0.1958, 0.2069, 0.1830]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "48/100 Running Loss: 1.5977705717086792\n",
      "tensor([[0.1831, 0.2364, 0.1940, 0.2057, 0.1809],\n",
      "        [0.1819, 0.2292, 0.1964, 0.2088, 0.1837],\n",
      "        [0.1818, 0.2293, 0.1965, 0.2088, 0.1837],\n",
      "        [0.1817, 0.2354, 0.1957, 0.2055, 0.1818],\n",
      "        [0.1838, 0.2348, 0.1943, 0.2061, 0.1809],\n",
      "        [0.1833, 0.2356, 0.1944, 0.2059, 0.1808],\n",
      "        [0.1839, 0.2380, 0.1942, 0.2039, 0.1801],\n",
      "        [0.1834, 0.2353, 0.1942, 0.2061, 0.1809],\n",
      "        [0.1843, 0.2311, 0.1951, 0.2071, 0.1824],\n",
      "        [0.1824, 0.2320, 0.1960, 0.2070, 0.1827]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "49/100 Running Loss: 1.5974149703979492\n",
      "tensor([[0.1826, 0.2373, 0.1941, 0.2056, 0.1804],\n",
      "        [0.1812, 0.2298, 0.1966, 0.2089, 0.1834],\n",
      "        [0.1812, 0.2299, 0.1967, 0.2089, 0.1834],\n",
      "        [0.1810, 0.2363, 0.1958, 0.2055, 0.1814],\n",
      "        [0.1833, 0.2357, 0.1944, 0.2061, 0.1805],\n",
      "        [0.1828, 0.2366, 0.1945, 0.2059, 0.1803],\n",
      "        [0.1833, 0.2390, 0.1942, 0.2038, 0.1796],\n",
      "        [0.1829, 0.2362, 0.1944, 0.2061, 0.1805],\n",
      "        [0.1838, 0.2318, 0.1953, 0.2071, 0.1821],\n",
      "        [0.1818, 0.2328, 0.1962, 0.2069, 0.1823]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "50/100 Running Loss: 1.5970500707626343\n",
      "tensor([[0.1820, 0.2383, 0.1942, 0.2055, 0.1800],\n",
      "        [0.1806, 0.2305, 0.1968, 0.2090, 0.1831],\n",
      "        [0.1806, 0.2306, 0.1968, 0.2089, 0.1831],\n",
      "        [0.1804, 0.2373, 0.1959, 0.2054, 0.1809],\n",
      "        [0.1827, 0.2366, 0.1946, 0.2061, 0.1800],\n",
      "        [0.1822, 0.2375, 0.1946, 0.2058, 0.1799],\n",
      "        [0.1828, 0.2401, 0.1943, 0.2037, 0.1791],\n",
      "        [0.1823, 0.2371, 0.1945, 0.2060, 0.1801],\n",
      "        [0.1832, 0.2325, 0.1954, 0.2071, 0.1817],\n",
      "        [0.1812, 0.2336, 0.1964, 0.2069, 0.1820]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "51/100 Running Loss: 1.5966687202453613\n",
      "tensor([[0.1814, 0.2394, 0.1943, 0.2054, 0.1795],\n",
      "        [0.1800, 0.2312, 0.1970, 0.2090, 0.1828],\n",
      "        [0.1799, 0.2313, 0.1970, 0.2090, 0.1827],\n",
      "        [0.1798, 0.2384, 0.1960, 0.2052, 0.1805],\n",
      "        [0.1821, 0.2375, 0.1948, 0.2060, 0.1796],\n",
      "        [0.1816, 0.2385, 0.1947, 0.2058, 0.1795],\n",
      "        [0.1822, 0.2412, 0.1944, 0.2036, 0.1786],\n",
      "        [0.1817, 0.2381, 0.1946, 0.2060, 0.1796],\n",
      "        [0.1827, 0.2332, 0.1956, 0.2071, 0.1814],\n",
      "        [0.1806, 0.2345, 0.1966, 0.2068, 0.1816]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "52/100 Running Loss: 1.5962772369384766\n",
      "tensor([[0.1808, 0.2405, 0.1943, 0.2053, 0.1790],\n",
      "        [0.1793, 0.2320, 0.1972, 0.2091, 0.1825],\n",
      "        [0.1793, 0.2321, 0.1972, 0.2090, 0.1824],\n",
      "        [0.1792, 0.2395, 0.1962, 0.2051, 0.1800],\n",
      "        [0.1815, 0.2385, 0.1949, 0.2060, 0.1791],\n",
      "        [0.1810, 0.2395, 0.1948, 0.2057, 0.1790],\n",
      "        [0.1816, 0.2424, 0.1945, 0.2034, 0.1781],\n",
      "        [0.1811, 0.2391, 0.1947, 0.2059, 0.1792],\n",
      "        [0.1821, 0.2339, 0.1958, 0.2071, 0.1810],\n",
      "        [0.1799, 0.2353, 0.1968, 0.2068, 0.1812]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "53/100 Running Loss: 1.595874309539795\n",
      "tensor([[0.1802, 0.2416, 0.1945, 0.2052, 0.1785],\n",
      "        [0.1787, 0.2327, 0.1974, 0.2091, 0.1822],\n",
      "        [0.1786, 0.2329, 0.1974, 0.2091, 0.1821],\n",
      "        [0.1785, 0.2406, 0.1963, 0.2050, 0.1796],\n",
      "        [0.1809, 0.2395, 0.1951, 0.2058, 0.1786],\n",
      "        [0.1804, 0.2406, 0.1949, 0.2056, 0.1785],\n",
      "        [0.1810, 0.2435, 0.1947, 0.2033, 0.1775],\n",
      "        [0.1805, 0.2401, 0.1948, 0.2059, 0.1787],\n",
      "        [0.1816, 0.2347, 0.1960, 0.2071, 0.1806],\n",
      "        [0.1793, 0.2362, 0.1970, 0.2067, 0.1808]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "54/100 Running Loss: 1.595455527305603\n",
      "tensor([[0.1796, 0.2427, 0.1946, 0.2051, 0.1780],\n",
      "        [0.1779, 0.2334, 0.1976, 0.2092, 0.1819],\n",
      "        [0.1779, 0.2336, 0.1976, 0.2091, 0.1818],\n",
      "        [0.1778, 0.2417, 0.1965, 0.2048, 0.1791],\n",
      "        [0.1803, 0.2406, 0.1952, 0.2057, 0.1782],\n",
      "        [0.1797, 0.2418, 0.1950, 0.2054, 0.1780],\n",
      "        [0.1804, 0.2447, 0.1948, 0.2031, 0.1770],\n",
      "        [0.1798, 0.2412, 0.1950, 0.2058, 0.1783],\n",
      "        [0.1810, 0.2355, 0.1962, 0.2071, 0.1802],\n",
      "        [0.1786, 0.2371, 0.1972, 0.2067, 0.1805]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "55/100 Running Loss: 1.5950171947479248\n",
      "tensor([[0.1789, 0.2439, 0.1948, 0.2050, 0.1774],\n",
      "        [0.1772, 0.2341, 0.1979, 0.2092, 0.1816],\n",
      "        [0.1772, 0.2343, 0.1979, 0.2092, 0.1815],\n",
      "        [0.1771, 0.2429, 0.1967, 0.2047, 0.1786],\n",
      "        [0.1797, 0.2417, 0.1953, 0.2056, 0.1777],\n",
      "        [0.1791, 0.2429, 0.1951, 0.2053, 0.1775],\n",
      "        [0.1797, 0.2460, 0.1949, 0.2030, 0.1764],\n",
      "        [0.1792, 0.2423, 0.1951, 0.2057, 0.1778],\n",
      "        [0.1804, 0.2363, 0.1964, 0.2070, 0.1799],\n",
      "        [0.1779, 0.2379, 0.1975, 0.2066, 0.1801]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "56/100 Running Loss: 1.5945619344711304\n",
      "tensor([[0.1782, 0.2451, 0.1949, 0.2048, 0.1769],\n",
      "        [0.1764, 0.2348, 0.1982, 0.2093, 0.1814],\n",
      "        [0.1764, 0.2350, 0.1982, 0.2093, 0.1812],\n",
      "        [0.1764, 0.2440, 0.1969, 0.2045, 0.1781],\n",
      "        [0.1791, 0.2428, 0.1955, 0.2054, 0.1772],\n",
      "        [0.1784, 0.2441, 0.1953, 0.2051, 0.1770],\n",
      "        [0.1790, 0.2473, 0.1951, 0.2027, 0.1758],\n",
      "        [0.1785, 0.2435, 0.1952, 0.2055, 0.1773],\n",
      "        [0.1798, 0.2371, 0.1966, 0.2070, 0.1795],\n",
      "        [0.1772, 0.2387, 0.1977, 0.2066, 0.1798]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "57/100 Running Loss: 1.5940862894058228\n",
      "tensor([[0.1775, 0.2464, 0.1951, 0.2047, 0.1763],\n",
      "        [0.1756, 0.2355, 0.1985, 0.2094, 0.1811],\n",
      "        [0.1756, 0.2357, 0.1985, 0.2093, 0.1809],\n",
      "        [0.1756, 0.2452, 0.1972, 0.2044, 0.1776],\n",
      "        [0.1784, 0.2440, 0.1956, 0.2053, 0.1766],\n",
      "        [0.1777, 0.2454, 0.1954, 0.2050, 0.1765],\n",
      "        [0.1783, 0.2487, 0.1953, 0.2025, 0.1753],\n",
      "        [0.1778, 0.2447, 0.1954, 0.2054, 0.1768],\n",
      "        [0.1791, 0.2380, 0.1968, 0.2070, 0.1791],\n",
      "        [0.1764, 0.2395, 0.1980, 0.2066, 0.1794]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "58/100 Running Loss: 1.5935884714126587\n",
      "tensor([[0.1768, 0.2478, 0.1953, 0.2045, 0.1757],\n",
      "        [0.1748, 0.2362, 0.1988, 0.2095, 0.1808],\n",
      "        [0.1748, 0.2364, 0.1988, 0.2094, 0.1806],\n",
      "        [0.1748, 0.2465, 0.1974, 0.2042, 0.1771],\n",
      "        [0.1777, 0.2453, 0.1958, 0.2051, 0.1761],\n",
      "        [0.1770, 0.2467, 0.1956, 0.2047, 0.1760],\n",
      "        [0.1775, 0.2501, 0.1954, 0.2022, 0.1747],\n",
      "        [0.1771, 0.2459, 0.1955, 0.2052, 0.1763],\n",
      "        [0.1785, 0.2388, 0.1971, 0.2069, 0.1787],\n",
      "        [0.1756, 0.2404, 0.1984, 0.2066, 0.1791]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "59/100 Running Loss: 1.593069076538086\n",
      "tensor([[0.1761, 0.2492, 0.1954, 0.2043, 0.1750],\n",
      "        [0.1739, 0.2369, 0.1991, 0.2096, 0.1805],\n",
      "        [0.1739, 0.2372, 0.1991, 0.2095, 0.1803],\n",
      "        [0.1739, 0.2477, 0.1977, 0.2041, 0.1765],\n",
      "        [0.1769, 0.2466, 0.1960, 0.2049, 0.1756],\n",
      "        [0.1762, 0.2481, 0.1958, 0.2045, 0.1754],\n",
      "        [0.1767, 0.2516, 0.1956, 0.2020, 0.1741],\n",
      "        [0.1764, 0.2472, 0.1957, 0.2050, 0.1757],\n",
      "        [0.1778, 0.2396, 0.1974, 0.2069, 0.1783],\n",
      "        [0.1748, 0.2413, 0.1987, 0.2066, 0.1786]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "60/100 Running Loss: 1.5925251245498657\n",
      "tensor([[0.1753, 0.2507, 0.1956, 0.2040, 0.1744],\n",
      "        [0.1731, 0.2377, 0.1994, 0.2096, 0.1801],\n",
      "        [0.1730, 0.2380, 0.1994, 0.2096, 0.1800],\n",
      "        [0.1730, 0.2489, 0.1981, 0.2039, 0.1760],\n",
      "        [0.1762, 0.2479, 0.1962, 0.2046, 0.1750],\n",
      "        [0.1754, 0.2496, 0.1960, 0.2042, 0.1748],\n",
      "        [0.1760, 0.2531, 0.1957, 0.2017, 0.1735],\n",
      "        [0.1756, 0.2486, 0.1959, 0.2048, 0.1752],\n",
      "        [0.1771, 0.2405, 0.1976, 0.2069, 0.1780],\n",
      "        [0.1740, 0.2422, 0.1990, 0.2066, 0.1782]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "61/100 Running Loss: 1.59196138381958\n",
      "tensor([[0.1745, 0.2523, 0.1958, 0.2038, 0.1737],\n",
      "        [0.1722, 0.2386, 0.1998, 0.2097, 0.1797],\n",
      "        [0.1721, 0.2388, 0.1998, 0.2097, 0.1796],\n",
      "        [0.1721, 0.2503, 0.1984, 0.2038, 0.1754],\n",
      "        [0.1754, 0.2494, 0.1964, 0.2044, 0.1744],\n",
      "        [0.1746, 0.2511, 0.1962, 0.2040, 0.1742],\n",
      "        [0.1751, 0.2548, 0.1958, 0.2014, 0.1729],\n",
      "        [0.1748, 0.2500, 0.1961, 0.2045, 0.1746],\n",
      "        [0.1763, 0.2414, 0.1979, 0.2068, 0.1775],\n",
      "        [0.1731, 0.2432, 0.1994, 0.2066, 0.1777]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "62/100 Running Loss: 1.5913805961608887\n",
      "tensor([[0.1736, 0.2540, 0.1959, 0.2035, 0.1731],\n",
      "        [0.1713, 0.2395, 0.2001, 0.2098, 0.1793],\n",
      "        [0.1712, 0.2397, 0.2001, 0.2097, 0.1792],\n",
      "        [0.1712, 0.2517, 0.1987, 0.2036, 0.1748],\n",
      "        [0.1746, 0.2509, 0.1966, 0.2041, 0.1738],\n",
      "        [0.1737, 0.2526, 0.1964, 0.2037, 0.1736],\n",
      "        [0.1743, 0.2564, 0.1960, 0.2011, 0.1723],\n",
      "        [0.1739, 0.2515, 0.1963, 0.2043, 0.1741],\n",
      "        [0.1756, 0.2424, 0.1982, 0.2068, 0.1770],\n",
      "        [0.1722, 0.2443, 0.1997, 0.2065, 0.1772]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "63/100 Running Loss: 1.5907824039459229\n",
      "tensor([[0.1727, 0.2557, 0.1961, 0.2032, 0.1724],\n",
      "        [0.1703, 0.2404, 0.2005, 0.2099, 0.1789],\n",
      "        [0.1703, 0.2407, 0.2005, 0.2098, 0.1788],\n",
      "        [0.1702, 0.2533, 0.1990, 0.2034, 0.1742],\n",
      "        [0.1738, 0.2524, 0.1968, 0.2038, 0.1732],\n",
      "        [0.1728, 0.2543, 0.1966, 0.2034, 0.1729],\n",
      "        [0.1734, 0.2582, 0.1961, 0.2007, 0.1716],\n",
      "        [0.1730, 0.2531, 0.1965, 0.2040, 0.1734],\n",
      "        [0.1748, 0.2435, 0.1985, 0.2067, 0.1765],\n",
      "        [0.1713, 0.2454, 0.2001, 0.2065, 0.1767]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "64/100 Running Loss: 1.5901671648025513\n",
      "tensor([[0.1718, 0.2574, 0.1962, 0.2029, 0.1717],\n",
      "        [0.1693, 0.2415, 0.2008, 0.2099, 0.1784],\n",
      "        [0.1693, 0.2418, 0.2008, 0.2098, 0.1783],\n",
      "        [0.1692, 0.2549, 0.1993, 0.2032, 0.1735],\n",
      "        [0.1729, 0.2541, 0.1970, 0.2035, 0.1725],\n",
      "        [0.1719, 0.2560, 0.1967, 0.2031, 0.1723],\n",
      "        [0.1725, 0.2600, 0.1963, 0.2004, 0.1709],\n",
      "        [0.1721, 0.2548, 0.1967, 0.2037, 0.1728],\n",
      "        [0.1740, 0.2447, 0.1988, 0.2065, 0.1760],\n",
      "        [0.1703, 0.2466, 0.2005, 0.2064, 0.1762]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "65/100 Running Loss: 1.589532494544983\n",
      "tensor([[0.1708, 0.2593, 0.1964, 0.2025, 0.1710],\n",
      "        [0.1683, 0.2426, 0.2012, 0.2099, 0.1780],\n",
      "        [0.1683, 0.2429, 0.2012, 0.2098, 0.1778],\n",
      "        [0.1682, 0.2566, 0.1995, 0.2029, 0.1728],\n",
      "        [0.1720, 0.2558, 0.1971, 0.2032, 0.1718],\n",
      "        [0.1710, 0.2579, 0.1969, 0.2027, 0.1715],\n",
      "        [0.1715, 0.2619, 0.1964, 0.2000, 0.1702],\n",
      "        [0.1712, 0.2565, 0.1969, 0.2034, 0.1720],\n",
      "        [0.1732, 0.2459, 0.1991, 0.2064, 0.1755],\n",
      "        [0.1694, 0.2478, 0.2008, 0.2064, 0.1756]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "66/100 Running Loss: 1.5888752937316895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1698, 0.2612, 0.1966, 0.2022, 0.1703],\n",
      "        [0.1673, 0.2437, 0.2016, 0.2099, 0.1774],\n",
      "        [0.1672, 0.2441, 0.2016, 0.2098, 0.1773],\n",
      "        [0.1671, 0.2584, 0.1998, 0.2026, 0.1720],\n",
      "        [0.1711, 0.2576, 0.1973, 0.2029, 0.1711],\n",
      "        [0.1700, 0.2598, 0.1970, 0.2024, 0.1708],\n",
      "        [0.1705, 0.2639, 0.1965, 0.1996, 0.1695],\n",
      "        [0.1702, 0.2583, 0.1970, 0.2031, 0.1713],\n",
      "        [0.1723, 0.2472, 0.1994, 0.2062, 0.1749],\n",
      "        [0.1684, 0.2491, 0.2012, 0.2063, 0.1751]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "67/100 Running Loss: 1.5881941318511963\n",
      "tensor([[0.1688, 0.2632, 0.1967, 0.2018, 0.1695],\n",
      "        [0.1663, 0.2449, 0.2020, 0.2099, 0.1769],\n",
      "        [0.1662, 0.2454, 0.2020, 0.2097, 0.1767],\n",
      "        [0.1660, 0.2603, 0.2001, 0.2024, 0.1712],\n",
      "        [0.1701, 0.2595, 0.1975, 0.2025, 0.1704],\n",
      "        [0.1690, 0.2618, 0.1972, 0.2020, 0.1700],\n",
      "        [0.1694, 0.2660, 0.1967, 0.1992, 0.1687],\n",
      "        [0.1692, 0.2602, 0.1972, 0.2027, 0.1706],\n",
      "        [0.1714, 0.2486, 0.1996, 0.2061, 0.1743],\n",
      "        [0.1674, 0.2505, 0.2015, 0.2062, 0.1745]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "68/100 Running Loss: 1.5874872207641602\n",
      "tensor([[0.1677, 0.2654, 0.1968, 0.2014, 0.1687],\n",
      "        [0.1652, 0.2462, 0.2025, 0.2098, 0.1763],\n",
      "        [0.1651, 0.2467, 0.2024, 0.2097, 0.1761],\n",
      "        [0.1649, 0.2623, 0.2004, 0.2021, 0.1704],\n",
      "        [0.1692, 0.2614, 0.1977, 0.2021, 0.1696],\n",
      "        [0.1680, 0.2639, 0.1973, 0.2016, 0.1692],\n",
      "        [0.1683, 0.2682, 0.1968, 0.1988, 0.1679],\n",
      "        [0.1682, 0.2622, 0.1973, 0.2023, 0.1699],\n",
      "        [0.1706, 0.2500, 0.1999, 0.2059, 0.1737],\n",
      "        [0.1663, 0.2518, 0.2019, 0.2061, 0.1739]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "69/100 Running Loss: 1.5867528915405273\n",
      "tensor([[0.1666, 0.2676, 0.1970, 0.2010, 0.1679],\n",
      "        [0.1641, 0.2475, 0.2029, 0.2098, 0.1757],\n",
      "        [0.1640, 0.2480, 0.2029, 0.2096, 0.1755],\n",
      "        [0.1638, 0.2643, 0.2006, 0.2017, 0.1696],\n",
      "        [0.1681, 0.2635, 0.1978, 0.2017, 0.1688],\n",
      "        [0.1669, 0.2661, 0.1974, 0.2012, 0.1684],\n",
      "        [0.1672, 0.2705, 0.1969, 0.1983, 0.1670],\n",
      "        [0.1672, 0.2643, 0.1975, 0.2019, 0.1691],\n",
      "        [0.1696, 0.2514, 0.2002, 0.2057, 0.1731],\n",
      "        [0.1653, 0.2533, 0.2022, 0.2059, 0.1732]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "70/100 Running Loss: 1.5859886407852173\n",
      "tensor([[0.1655, 0.2699, 0.1971, 0.2006, 0.1670],\n",
      "        [0.1630, 0.2488, 0.2033, 0.2098, 0.1752],\n",
      "        [0.1628, 0.2494, 0.2033, 0.2096, 0.1749],\n",
      "        [0.1626, 0.2664, 0.2009, 0.2014, 0.1687],\n",
      "        [0.1671, 0.2656, 0.1980, 0.2013, 0.1680],\n",
      "        [0.1658, 0.2684, 0.1976, 0.2007, 0.1675],\n",
      "        [0.1661, 0.2729, 0.1970, 0.1979, 0.1661],\n",
      "        [0.1661, 0.2665, 0.1976, 0.2015, 0.1683],\n",
      "        [0.1687, 0.2529, 0.2005, 0.2055, 0.1724],\n",
      "        [0.1642, 0.2548, 0.2026, 0.2058, 0.1726]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "71/100 Running Loss: 1.5851967334747314\n",
      "tensor([[0.1643, 0.2723, 0.1972, 0.2001, 0.1661],\n",
      "        [0.1618, 0.2502, 0.2037, 0.2097, 0.1746],\n",
      "        [0.1616, 0.2508, 0.2037, 0.2095, 0.1743],\n",
      "        [0.1613, 0.2686, 0.2012, 0.2010, 0.1678],\n",
      "        [0.1660, 0.2679, 0.1981, 0.2009, 0.1671],\n",
      "        [0.1646, 0.2708, 0.1977, 0.2003, 0.1666],\n",
      "        [0.1649, 0.2755, 0.1971, 0.1973, 0.1652],\n",
      "        [0.1650, 0.2687, 0.1978, 0.2011, 0.1674],\n",
      "        [0.1678, 0.2545, 0.2008, 0.2053, 0.1717],\n",
      "        [0.1631, 0.2564, 0.2030, 0.2056, 0.1720]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "72/100 Running Loss: 1.5843735933303833\n",
      "tensor([[0.1631, 0.2748, 0.1973, 0.1996, 0.1652],\n",
      "        [0.1605, 0.2517, 0.2042, 0.2096, 0.1739],\n",
      "        [0.1604, 0.2523, 0.2042, 0.2095, 0.1737],\n",
      "        [0.1601, 0.2709, 0.2014, 0.2007, 0.1669],\n",
      "        [0.1649, 0.2703, 0.1983, 0.2004, 0.1662],\n",
      "        [0.1634, 0.2733, 0.1979, 0.1998, 0.1657],\n",
      "        [0.1637, 0.2781, 0.1971, 0.1968, 0.1642],\n",
      "        [0.1638, 0.2711, 0.1979, 0.2006, 0.1665],\n",
      "        [0.1668, 0.2561, 0.2010, 0.2050, 0.1710],\n",
      "        [0.1619, 0.2580, 0.2033, 0.2055, 0.1713]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "73/100 Running Loss: 1.5835212469100952\n",
      "tensor([[0.1619, 0.2775, 0.1974, 0.1991, 0.1642],\n",
      "        [0.1593, 0.2532, 0.2047, 0.2096, 0.1733],\n",
      "        [0.1591, 0.2539, 0.2047, 0.2094, 0.1730],\n",
      "        [0.1588, 0.2733, 0.2017, 0.2003, 0.1659],\n",
      "        [0.1637, 0.2727, 0.1984, 0.1999, 0.1652],\n",
      "        [0.1622, 0.2759, 0.1980, 0.1993, 0.1647],\n",
      "        [0.1625, 0.2809, 0.1972, 0.1962, 0.1632],\n",
      "        [0.1627, 0.2736, 0.1981, 0.2001, 0.1656],\n",
      "        [0.1658, 0.2578, 0.2013, 0.2048, 0.1703],\n",
      "        [0.1607, 0.2597, 0.2037, 0.2053, 0.1706]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "74/100 Running Loss: 1.5826375484466553\n",
      "tensor([[0.1606, 0.2803, 0.1974, 0.1985, 0.1631],\n",
      "        [0.1580, 0.2548, 0.2052, 0.2095, 0.1726],\n",
      "        [0.1578, 0.2555, 0.2051, 0.2093, 0.1723],\n",
      "        [0.1575, 0.2758, 0.2019, 0.1998, 0.1649],\n",
      "        [0.1625, 0.2753, 0.1985, 0.1994, 0.1642],\n",
      "        [0.1609, 0.2786, 0.1981, 0.1987, 0.1636],\n",
      "        [0.1612, 0.2838, 0.1973, 0.1956, 0.1621],\n",
      "        [0.1614, 0.2761, 0.1982, 0.1996, 0.1646],\n",
      "        [0.1647, 0.2596, 0.2016, 0.2045, 0.1695],\n",
      "        [0.1595, 0.2614, 0.2041, 0.2051, 0.1699]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "75/100 Running Loss: 1.5817209482192993\n",
      "tensor([[0.1593, 0.2832, 0.1975, 0.1979, 0.1620],\n",
      "        [0.1566, 0.2564, 0.2057, 0.2094, 0.1719],\n",
      "        [0.1564, 0.2572, 0.2056, 0.2092, 0.1716],\n",
      "        [0.1561, 0.2784, 0.2022, 0.1994, 0.1639],\n",
      "        [0.1613, 0.2780, 0.1986, 0.1989, 0.1632],\n",
      "        [0.1596, 0.2815, 0.1982, 0.1981, 0.1626],\n",
      "        [0.1599, 0.2868, 0.1973, 0.1950, 0.1610],\n",
      "        [0.1602, 0.2788, 0.1983, 0.1991, 0.1636],\n",
      "        [0.1637, 0.2615, 0.2019, 0.2042, 0.1688],\n",
      "        [0.1582, 0.2633, 0.2045, 0.2049, 0.1691]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "76/100 Running Loss: 1.58076810836792\n",
      "tensor([[0.1580, 0.2862, 0.1976, 0.1973, 0.1609],\n",
      "        [0.1552, 0.2580, 0.2063, 0.2094, 0.1711],\n",
      "        [0.1550, 0.2589, 0.2062, 0.2091, 0.1708],\n",
      "        [0.1547, 0.2811, 0.2025, 0.1989, 0.1628],\n",
      "        [0.1600, 0.2808, 0.1987, 0.1983, 0.1622],\n",
      "        [0.1583, 0.2845, 0.1983, 0.1975, 0.1615],\n",
      "        [0.1586, 0.2900, 0.1974, 0.1943, 0.1598],\n",
      "        [0.1589, 0.2817, 0.1984, 0.1985, 0.1625],\n",
      "        [0.1625, 0.2634, 0.2022, 0.2039, 0.1679],\n",
      "        [0.1569, 0.2652, 0.2050, 0.2047, 0.1683]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "77/100 Running Loss: 1.579772710800171\n",
      "tensor([[0.1566, 0.2894, 0.1976, 0.1966, 0.1597],\n",
      "        [0.1538, 0.2597, 0.2068, 0.2093, 0.1703],\n",
      "        [0.1535, 0.2607, 0.2068, 0.2091, 0.1700],\n",
      "        [0.1532, 0.2840, 0.2028, 0.1984, 0.1617],\n",
      "        [0.1587, 0.2837, 0.1988, 0.1977, 0.1610],\n",
      "        [0.1570, 0.2876, 0.1984, 0.1968, 0.1603],\n",
      "        [0.1572, 0.2932, 0.1974, 0.1936, 0.1586],\n",
      "        [0.1576, 0.2847, 0.1985, 0.1978, 0.1614],\n",
      "        [0.1614, 0.2654, 0.2025, 0.2036, 0.1671],\n",
      "        [0.1555, 0.2671, 0.2054, 0.2044, 0.1675]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "78/100 Running Loss: 1.5787365436553955\n",
      "tensor([[0.1552, 0.2928, 0.1977, 0.1959, 0.1584],\n",
      "        [0.1523, 0.2614, 0.2075, 0.2093, 0.1695],\n",
      "        [0.1520, 0.2625, 0.2073, 0.2090, 0.1691],\n",
      "        [0.1516, 0.2870, 0.2031, 0.1978, 0.1605],\n",
      "        [0.1574, 0.2868, 0.1989, 0.1970, 0.1599],\n",
      "        [0.1555, 0.2908, 0.1985, 0.1961, 0.1590],\n",
      "        [0.1558, 0.2967, 0.1974, 0.1928, 0.1573],\n",
      "        [0.1562, 0.2878, 0.1986, 0.1972, 0.1602],\n",
      "        [0.1601, 0.2675, 0.2029, 0.2033, 0.1663],\n",
      "        [0.1541, 0.2692, 0.2059, 0.2042, 0.1666]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "79/100 Running Loss: 1.5776551961898804\n",
      "tensor([[0.1537, 0.2963, 0.1978, 0.1951, 0.1571],\n",
      "        [0.1507, 0.2632, 0.2081, 0.2092, 0.1687],\n",
      "        [0.1505, 0.2644, 0.2080, 0.2089, 0.1683],\n",
      "        [0.1500, 0.2900, 0.2034, 0.1972, 0.1593],\n",
      "        [0.1560, 0.2900, 0.1990, 0.1963, 0.1587],\n",
      "        [0.1541, 0.2942, 0.1985, 0.1954, 0.1578],\n",
      "        [0.1544, 0.3002, 0.1975, 0.1920, 0.1559],\n",
      "        [0.1548, 0.2910, 0.1988, 0.1964, 0.1590],\n",
      "        [0.1589, 0.2696, 0.2032, 0.2029, 0.1654],\n",
      "        [0.1527, 0.2713, 0.2064, 0.2039, 0.1657]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "80/100 Running Loss: 1.576529860496521\n",
      "tensor([[0.1522, 0.2999, 0.1979, 0.1943, 0.1557],\n",
      "        [0.1492, 0.2650, 0.2088, 0.2092, 0.1678],\n",
      "        [0.1489, 0.2663, 0.2086, 0.2088, 0.1674],\n",
      "        [0.1484, 0.2932, 0.2037, 0.1966, 0.1581],\n",
      "        [0.1546, 0.2933, 0.1992, 0.1956, 0.1574],\n",
      "        [0.1525, 0.2978, 0.1987, 0.1946, 0.1565],\n",
      "        [0.1529, 0.3039, 0.1975, 0.1912, 0.1545],\n",
      "        [0.1533, 0.2943, 0.1989, 0.1957, 0.1578],\n",
      "        [0.1576, 0.2718, 0.2036, 0.2025, 0.1645],\n",
      "        [0.1512, 0.2735, 0.2069, 0.2036, 0.1647]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "81/100 Running Loss: 1.5753597021102905\n",
      "tensor([[0.1506, 0.3037, 0.1980, 0.1934, 0.1544],\n",
      "        [0.1475, 0.2670, 0.2095, 0.2091, 0.1670],\n",
      "        [0.1473, 0.2683, 0.2093, 0.2087, 0.1664],\n",
      "        [0.1467, 0.2965, 0.2040, 0.1960, 0.1567],\n",
      "        [0.1531, 0.2967, 0.1993, 0.1948, 0.1561],\n",
      "        [0.1509, 0.3015, 0.1988, 0.1937, 0.1551],\n",
      "        [0.1513, 0.3078, 0.1976, 0.1903, 0.1531],\n",
      "        [0.1517, 0.2978, 0.1991, 0.1949, 0.1565],\n",
      "        [0.1563, 0.2741, 0.2040, 0.2021, 0.1635],\n",
      "        [0.1497, 0.2758, 0.2075, 0.2033, 0.1637]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "82/100 Running Loss: 1.5741432905197144\n",
      "tensor([[0.1489, 0.3076, 0.1981, 0.1925, 0.1529],\n",
      "        [0.1459, 0.2690, 0.2102, 0.2090, 0.1660],\n",
      "        [0.1456, 0.2704, 0.2100, 0.2085, 0.1655],\n",
      "        [0.1449, 0.3000, 0.2044, 0.1953, 0.1554],\n",
      "        [0.1515, 0.3003, 0.1995, 0.1939, 0.1548],\n",
      "        [0.1492, 0.3053, 0.1989, 0.1928, 0.1537],\n",
      "        [0.1496, 0.3118, 0.1976, 0.1893, 0.1517],\n",
      "        [0.1501, 0.3015, 0.1993, 0.1940, 0.1552],\n",
      "        [0.1549, 0.2765, 0.2044, 0.2016, 0.1626],\n",
      "        [0.1481, 0.2782, 0.2080, 0.2030, 0.1627]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "83/100 Running Loss: 1.572879672050476\n",
      "tensor([[0.1471, 0.3118, 0.1981, 0.1915, 0.1514],\n",
      "        [0.1442, 0.2710, 0.2109, 0.2089, 0.1650],\n",
      "        [0.1439, 0.2725, 0.2107, 0.2084, 0.1645],\n",
      "        [0.1431, 0.3035, 0.2048, 0.1946, 0.1540],\n",
      "        [0.1499, 0.3041, 0.1996, 0.1931, 0.1534],\n",
      "        [0.1475, 0.3093, 0.1990, 0.1919, 0.1522],\n",
      "        [0.1479, 0.3159, 0.1977, 0.1883, 0.1502],\n",
      "        [0.1485, 0.3053, 0.1994, 0.1931, 0.1538],\n",
      "        [0.1535, 0.2790, 0.2048, 0.2012, 0.1616],\n",
      "        [0.1465, 0.2806, 0.2085, 0.2027, 0.1616]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "84/100 Running Loss: 1.571567177772522\n",
      "tensor([[0.1453, 0.3161, 0.1982, 0.1905, 0.1499],\n",
      "        [0.1424, 0.2732, 0.2116, 0.2087, 0.1640],\n",
      "        [0.1421, 0.2748, 0.2114, 0.2082, 0.1634],\n",
      "        [0.1413, 0.3073, 0.2051, 0.1938, 0.1525],\n",
      "        [0.1482, 0.3080, 0.1997, 0.1921, 0.1520],\n",
      "        [0.1457, 0.3135, 0.1992, 0.1909, 0.1507],\n",
      "        [0.1461, 0.3203, 0.1977, 0.1872, 0.1487],\n",
      "        [0.1468, 0.3093, 0.1995, 0.1921, 0.1523],\n",
      "        [0.1521, 0.2816, 0.2052, 0.2007, 0.1605],\n",
      "        [0.1449, 0.2831, 0.2091, 0.2024, 0.1605]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "85/100 Running Loss: 1.570204734802246\n",
      "tensor([[0.1434, 0.3207, 0.1982, 0.1893, 0.1483],\n",
      "        [0.1407, 0.2755, 0.2124, 0.2085, 0.1630],\n",
      "        [0.1403, 0.2772, 0.2122, 0.2080, 0.1623],\n",
      "        [0.1394, 0.3112, 0.2054, 0.1930, 0.1510],\n",
      "        [0.1464, 0.3121, 0.1999, 0.1912, 0.1505],\n",
      "        [0.1439, 0.3179, 0.1992, 0.1898, 0.1491],\n",
      "        [0.1442, 0.3248, 0.1977, 0.1861, 0.1471],\n",
      "        [0.1450, 0.3136, 0.1996, 0.1911, 0.1508],\n",
      "        [0.1506, 0.2843, 0.2056, 0.2001, 0.1594],\n",
      "        [0.1432, 0.2857, 0.2097, 0.2021, 0.1594]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "86/100 Running Loss: 1.5687885284423828\n",
      "tensor([[0.1415, 0.3255, 0.1982, 0.1881, 0.1466],\n",
      "        [0.1388, 0.2779, 0.2131, 0.2083, 0.1618],\n",
      "        [0.1385, 0.2796, 0.2129, 0.2078, 0.1612],\n",
      "        [0.1374, 0.3153, 0.2057, 0.1921, 0.1494],\n",
      "        [0.1447, 0.3163, 0.2000, 0.1901, 0.1489],\n",
      "        [0.1420, 0.3225, 0.1993, 0.1887, 0.1475],\n",
      "        [0.1423, 0.3296, 0.1977, 0.1849, 0.1454],\n",
      "        [0.1432, 0.3180, 0.1997, 0.1900, 0.1492],\n",
      "        [0.1491, 0.2872, 0.2060, 0.1995, 0.1582],\n",
      "        [0.1414, 0.2884, 0.2103, 0.2017, 0.1582]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "87/100 Running Loss: 1.56732177734375\n",
      "tensor([[0.1395, 0.3304, 0.1982, 0.1869, 0.1449],\n",
      "        [0.1370, 0.2803, 0.2139, 0.2081, 0.1607],\n",
      "        [0.1366, 0.2821, 0.2137, 0.2076, 0.1600],\n",
      "        [0.1354, 0.3196, 0.2060, 0.1911, 0.1478],\n",
      "        [0.1428, 0.3207, 0.2000, 0.1891, 0.1473],\n",
      "        [0.1400, 0.3274, 0.1993, 0.1875, 0.1458],\n",
      "        [0.1404, 0.3345, 0.1977, 0.1837, 0.1437],\n",
      "        [0.1413, 0.3226, 0.1997, 0.1888, 0.1476],\n",
      "        [0.1475, 0.2901, 0.2064, 0.1989, 0.1571],\n",
      "        [0.1396, 0.2912, 0.2109, 0.2013, 0.1570]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "88/100 Running Loss: 1.5657964944839478\n",
      "tensor([[0.1375, 0.3356, 0.1982, 0.1855, 0.1432],\n",
      "        [0.1350, 0.2828, 0.2147, 0.2079, 0.1596],\n",
      "        [0.1347, 0.2847, 0.2145, 0.2073, 0.1588],\n",
      "        [0.1333, 0.3238, 0.2064, 0.1903, 0.1462],\n",
      "        [0.1409, 0.3253, 0.2001, 0.1879, 0.1457],\n",
      "        [0.1380, 0.3324, 0.1993, 0.1862, 0.1441],\n",
      "        [0.1384, 0.3396, 0.1976, 0.1823, 0.1420],\n",
      "        [0.1393, 0.3275, 0.1997, 0.1876, 0.1459],\n",
      "        [0.1459, 0.2930, 0.2069, 0.1984, 0.1559],\n",
      "        [0.1377, 0.2940, 0.2115, 0.2009, 0.1558]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "89/100 Running Loss: 1.5642098188400269\n",
      "tensor([[0.1354, 0.3410, 0.1981, 0.1841, 0.1413],\n",
      "        [0.1331, 0.2854, 0.2155, 0.2076, 0.1584],\n",
      "        [0.1327, 0.2874, 0.2153, 0.2070, 0.1576],\n",
      "        [0.1311, 0.3282, 0.2068, 0.1894, 0.1445],\n",
      "        [0.1390, 0.3302, 0.2002, 0.1867, 0.1440],\n",
      "        [0.1360, 0.3376, 0.1993, 0.1849, 0.1423],\n",
      "        [0.1364, 0.3450, 0.1975, 0.1809, 0.1401],\n",
      "        [0.1373, 0.3325, 0.1998, 0.1863, 0.1442],\n",
      "        [0.1442, 0.2960, 0.2073, 0.1978, 0.1547],\n",
      "        [0.1359, 0.2970, 0.2122, 0.2004, 0.1546]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "90/100 Running Loss: 1.5625636577606201\n",
      "tensor([[0.1332, 0.3467, 0.1980, 0.1826, 0.1395],\n",
      "        [0.1311, 0.2880, 0.2164, 0.2073, 0.1572],\n",
      "        [0.1307, 0.2901, 0.2161, 0.2067, 0.1564],\n",
      "        [0.1289, 0.3327, 0.2072, 0.1884, 0.1428],\n",
      "        [0.1370, 0.3352, 0.2002, 0.1854, 0.1423],\n",
      "        [0.1338, 0.3430, 0.1993, 0.1834, 0.1405],\n",
      "        [0.1343, 0.3506, 0.1974, 0.1794, 0.1383],\n",
      "        [0.1353, 0.3377, 0.1998, 0.1849, 0.1424],\n",
      "        [0.1425, 0.2991, 0.2078, 0.1972, 0.1534],\n",
      "        [0.1339, 0.3001, 0.2128, 0.1999, 0.1533]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "91/100 Running Loss: 1.5608576536178589\n",
      "tensor([[0.1310, 0.3526, 0.1979, 0.1811, 0.1375],\n",
      "        [0.1291, 0.2907, 0.2172, 0.2070, 0.1560],\n",
      "        [0.1286, 0.2929, 0.2170, 0.2063, 0.1552],\n",
      "        [0.1266, 0.3374, 0.2075, 0.1874, 0.1411],\n",
      "        [0.1349, 0.3404, 0.2002, 0.1840, 0.1405],\n",
      "        [0.1317, 0.3486, 0.1992, 0.1819, 0.1386],\n",
      "        [0.1321, 0.3564, 0.1972, 0.1779, 0.1363],\n",
      "        [0.1331, 0.3430, 0.1998, 0.1836, 0.1405],\n",
      "        [0.1407, 0.3024, 0.2083, 0.1965, 0.1522],\n",
      "        [0.1320, 0.3033, 0.2134, 0.1994, 0.1520]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "92/100 Running Loss: 1.5590909719467163\n",
      "tensor([[0.1287, 0.3586, 0.1977, 0.1795, 0.1355],\n",
      "        [0.1270, 0.2936, 0.2180, 0.2067, 0.1547],\n",
      "        [0.1266, 0.2958, 0.2178, 0.2059, 0.1539],\n",
      "        [0.1242, 0.3423, 0.2078, 0.1863, 0.1393],\n",
      "        [0.1328, 0.3458, 0.2001, 0.1826, 0.1386],\n",
      "        [0.1294, 0.3545, 0.1991, 0.1804, 0.1366],\n",
      "        [0.1299, 0.3625, 0.1970, 0.1762, 0.1343],\n",
      "        [0.1309, 0.3486, 0.1997, 0.1821, 0.1386],\n",
      "        [0.1389, 0.3057, 0.2087, 0.1959, 0.1509],\n",
      "        [0.1299, 0.3066, 0.2141, 0.1988, 0.1506]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "93/100 Running Loss: 1.5572668313980103\n",
      "tensor([[0.1263, 0.3649, 0.1975, 0.1779, 0.1334],\n",
      "        [0.1250, 0.2966, 0.2189, 0.2062, 0.1534],\n",
      "        [0.1244, 0.2989, 0.2186, 0.2055, 0.1525],\n",
      "        [0.1219, 0.3475, 0.2081, 0.1851, 0.1375],\n",
      "        [0.1306, 0.3516, 0.2000, 0.1811, 0.1367],\n",
      "        [0.1270, 0.3605, 0.1990, 0.1789, 0.1346],\n",
      "        [0.1277, 0.3689, 0.1967, 0.1745, 0.1323],\n",
      "        [0.1287, 0.3544, 0.1996, 0.1806, 0.1367],\n",
      "        [0.1370, 0.3092, 0.2091, 0.1951, 0.1495],\n",
      "        [0.1278, 0.3100, 0.2147, 0.1982, 0.1492]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "94/100 Running Loss: 1.5553834438323975\n",
      "tensor([[0.1239, 0.3715, 0.1972, 0.1762, 0.1312],\n",
      "        [0.1228, 0.2997, 0.2197, 0.2058, 0.1520],\n",
      "        [0.1223, 0.3022, 0.2194, 0.2050, 0.1511],\n",
      "        [0.1194, 0.3530, 0.2082, 0.1838, 0.1356],\n",
      "        [0.1284, 0.3576, 0.1998, 0.1795, 0.1347],\n",
      "        [0.1246, 0.3669, 0.1987, 0.1773, 0.1325],\n",
      "        [0.1253, 0.3756, 0.1963, 0.1726, 0.1301],\n",
      "        [0.1263, 0.3605, 0.1995, 0.1790, 0.1346],\n",
      "        [0.1351, 0.3130, 0.2095, 0.1943, 0.1481],\n",
      "        [0.1256, 0.3137, 0.2153, 0.1976, 0.1478]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "95/100 Running Loss: 1.5534396171569824\n",
      "tensor([[0.1214, 0.3785, 0.1968, 0.1743, 0.1290],\n",
      "        [0.1206, 0.3030, 0.2205, 0.2053, 0.1506],\n",
      "        [0.1200, 0.3057, 0.2201, 0.2044, 0.1497],\n",
      "        [0.1169, 0.3587, 0.2083, 0.1824, 0.1336],\n",
      "        [0.1260, 0.3638, 0.1996, 0.1779, 0.1327],\n",
      "        [0.1221, 0.3736, 0.1984, 0.1756, 0.1303],\n",
      "        [0.1230, 0.3825, 0.1959, 0.1707, 0.1280],\n",
      "        [0.1239, 0.3670, 0.1992, 0.1773, 0.1325],\n",
      "        [0.1332, 0.3169, 0.2099, 0.1934, 0.1467],\n",
      "        [0.1235, 0.3176, 0.2158, 0.1968, 0.1463]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "96/100 Running Loss: 1.5514371395111084\n",
      "tensor([[0.1189, 0.3858, 0.1964, 0.1723, 0.1266],\n",
      "        [0.1183, 0.3065, 0.2212, 0.2047, 0.1492],\n",
      "        [0.1177, 0.3095, 0.2208, 0.2037, 0.1482],\n",
      "        [0.1144, 0.3647, 0.2084, 0.1809, 0.1316],\n",
      "        [0.1235, 0.3703, 0.1993, 0.1762, 0.1306],\n",
      "        [0.1195, 0.3807, 0.1980, 0.1737, 0.1281],\n",
      "        [0.1205, 0.3897, 0.1954, 0.1687, 0.1257],\n",
      "        [0.1214, 0.3738, 0.1988, 0.1756, 0.1304],\n",
      "        [0.1312, 0.3210, 0.2102, 0.1924, 0.1451],\n",
      "        [0.1212, 0.3217, 0.2163, 0.1960, 0.1448]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "97/100 Running Loss: 1.549372673034668\n",
      "tensor([[0.1162, 0.3935, 0.1958, 0.1702, 0.1242],\n",
      "        [0.1161, 0.3102, 0.2219, 0.2041, 0.1477],\n",
      "        [0.1154, 0.3134, 0.2214, 0.2030, 0.1467],\n",
      "        [0.1118, 0.3711, 0.2083, 0.1793, 0.1295],\n",
      "        [0.1211, 0.3771, 0.1990, 0.1744, 0.1284],\n",
      "        [0.1169, 0.3882, 0.1975, 0.1717, 0.1257],\n",
      "        [0.1180, 0.3973, 0.1947, 0.1666, 0.1234],\n",
      "        [0.1189, 0.3809, 0.1984, 0.1737, 0.1281],\n",
      "        [0.1292, 0.3253, 0.2105, 0.1914, 0.1436],\n",
      "        [0.1190, 0.3260, 0.2168, 0.1950, 0.1432]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "98/100 Running Loss: 1.5472458600997925\n",
      "tensor([[0.1135, 0.4016, 0.1951, 0.1680, 0.1217],\n",
      "        [0.1138, 0.3141, 0.2226, 0.2033, 0.1462],\n",
      "        [0.1131, 0.3175, 0.2220, 0.2022, 0.1452],\n",
      "        [0.1092, 0.3777, 0.2081, 0.1776, 0.1273],\n",
      "        [0.1185, 0.3843, 0.1985, 0.1725, 0.1262],\n",
      "        [0.1142, 0.3959, 0.1969, 0.1696, 0.1233],\n",
      "        [0.1153, 0.4051, 0.1941, 0.1645, 0.1211],\n",
      "        [0.1163, 0.3884, 0.1978, 0.1717, 0.1258],\n",
      "        [0.1271, 0.3299, 0.2107, 0.1903, 0.1420],\n",
      "        [0.1167, 0.3305, 0.2172, 0.1940, 0.1416]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "99/100 Running Loss: 1.5450515747070312\n",
      "tensor([[0.1107, 0.4101, 0.1943, 0.1657, 0.1192],\n",
      "        [0.1115, 0.3182, 0.2232, 0.2025, 0.1447],\n",
      "        [0.1108, 0.3218, 0.2226, 0.2012, 0.1436],\n",
      "        [0.1066, 0.3847, 0.2079, 0.1758, 0.1251],\n",
      "        [0.1159, 0.3918, 0.1979, 0.1704, 0.1239],\n",
      "        [0.1115, 0.4040, 0.1961, 0.1674, 0.1209],\n",
      "        [0.1127, 0.4132, 0.1933, 0.1622, 0.1186],\n",
      "        [0.1137, 0.3962, 0.1972, 0.1696, 0.1234],\n",
      "        [0.1250, 0.3346, 0.2109, 0.1891, 0.1404],\n",
      "        [0.1144, 0.3352, 0.2176, 0.1929, 0.1399]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "100/100 Running Loss: 1.542794108390808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.39      1.00      0.56        20\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.00      0.00      0.00         6\n",
      "\n",
      "   micro avg       0.39      0.39      0.39        51\n",
      "   macro avg       0.10      0.25      0.14        51\n",
      "weighted avg       0.15      0.39      0.22        51\n",
      "\n",
      "0.14084507042253522\n",
      "tensor([[0.2180, 0.1928, 0.1851, 0.1927, 0.2114],\n",
      "        [0.2188, 0.1914, 0.1857, 0.1920, 0.2122],\n",
      "        [0.2186, 0.1914, 0.1856, 0.1921, 0.2122],\n",
      "        [0.2188, 0.1917, 0.1859, 0.1916, 0.2119],\n",
      "        [0.2196, 0.1913, 0.1857, 0.1919, 0.2115],\n",
      "        [0.2196, 0.1910, 0.1857, 0.1921, 0.2116],\n",
      "        [0.2185, 0.1921, 0.1850, 0.1932, 0.2112],\n",
      "        [0.2193, 0.1914, 0.1855, 0.1924, 0.2114],\n",
      "        [0.2189, 0.1922, 0.1854, 0.1921, 0.2115],\n",
      "        [0.2178, 0.1925, 0.1852, 0.1926, 0.2119]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "1/100 Running Loss: 1.6168384552001953\n",
      "tensor([[0.2174, 0.1934, 0.1851, 0.1929, 0.2112],\n",
      "        [0.2181, 0.1919, 0.1857, 0.1923, 0.2121],\n",
      "        [0.2180, 0.1919, 0.1856, 0.1924, 0.2121],\n",
      "        [0.2182, 0.1923, 0.1859, 0.1919, 0.2118],\n",
      "        [0.2190, 0.1918, 0.1858, 0.1921, 0.2113],\n",
      "        [0.2191, 0.1915, 0.1857, 0.1923, 0.2113],\n",
      "        [0.2180, 0.1926, 0.1850, 0.1935, 0.2110],\n",
      "        [0.2187, 0.1919, 0.1855, 0.1927, 0.2112],\n",
      "        [0.2184, 0.1926, 0.1854, 0.1923, 0.2114],\n",
      "        [0.2172, 0.1930, 0.1852, 0.1928, 0.2118]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "2/100 Running Loss: 1.6165891885757446\n",
      "tensor([[0.2168, 0.1939, 0.1850, 0.1932, 0.2110],\n",
      "        [0.2175, 0.1923, 0.1856, 0.1925, 0.2120],\n",
      "        [0.2173, 0.1924, 0.1856, 0.1927, 0.2120],\n",
      "        [0.2176, 0.1928, 0.1859, 0.1921, 0.2117],\n",
      "        [0.2185, 0.1923, 0.1858, 0.1922, 0.2112],\n",
      "        [0.2185, 0.1920, 0.1858, 0.1925, 0.2112],\n",
      "        [0.2174, 0.1931, 0.1849, 0.1937, 0.2108],\n",
      "        [0.2181, 0.1925, 0.1855, 0.1929, 0.2110],\n",
      "        [0.2178, 0.1930, 0.1854, 0.1925, 0.2112],\n",
      "        [0.2165, 0.1934, 0.1852, 0.1931, 0.2118]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "3/100 Running Loss: 1.616341233253479\n",
      "tensor([[0.2163, 0.1944, 0.1850, 0.1935, 0.2109],\n",
      "        [0.2168, 0.1928, 0.1856, 0.1928, 0.2119],\n",
      "        [0.2167, 0.1928, 0.1856, 0.1930, 0.2120],\n",
      "        [0.2169, 0.1933, 0.1859, 0.1924, 0.2115],\n",
      "        [0.2179, 0.1927, 0.1858, 0.1924, 0.2111],\n",
      "        [0.2180, 0.1925, 0.1858, 0.1927, 0.2110],\n",
      "        [0.2169, 0.1935, 0.1849, 0.1940, 0.2107],\n",
      "        [0.2176, 0.1930, 0.1855, 0.1931, 0.2108],\n",
      "        [0.2173, 0.1934, 0.1855, 0.1927, 0.2111],\n",
      "        [0.2158, 0.1938, 0.1852, 0.1933, 0.2117]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "4/100 Running Loss: 1.6160950660705566\n",
      "tensor([[0.2157, 0.1949, 0.1849, 0.1938, 0.2108],\n",
      "        [0.2161, 0.1932, 0.1856, 0.1931, 0.2119],\n",
      "        [0.2160, 0.1933, 0.1856, 0.1932, 0.2119],\n",
      "        [0.2162, 0.1938, 0.1859, 0.1927, 0.2114],\n",
      "        [0.2174, 0.1931, 0.1859, 0.1926, 0.2110],\n",
      "        [0.2174, 0.1930, 0.1859, 0.1929, 0.2108],\n",
      "        [0.2163, 0.1939, 0.1849, 0.1943, 0.2106],\n",
      "        [0.2170, 0.1935, 0.1856, 0.1933, 0.2106],\n",
      "        [0.2168, 0.1938, 0.1855, 0.1929, 0.2110],\n",
      "        [0.2152, 0.1943, 0.1852, 0.1936, 0.2117]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/100 Running Loss: 1.6158514022827148\n",
      "tensor([[0.2151, 0.1953, 0.1849, 0.1940, 0.2107],\n",
      "        [0.2155, 0.1937, 0.1856, 0.1934, 0.2118],\n",
      "        [0.2153, 0.1937, 0.1855, 0.1935, 0.2119],\n",
      "        [0.2156, 0.1943, 0.1859, 0.1930, 0.2113],\n",
      "        [0.2168, 0.1936, 0.1859, 0.1928, 0.2109],\n",
      "        [0.2168, 0.1935, 0.1859, 0.1931, 0.2107],\n",
      "        [0.2158, 0.1943, 0.1850, 0.1945, 0.2104],\n",
      "        [0.2165, 0.1939, 0.1857, 0.1935, 0.2104],\n",
      "        [0.2162, 0.1942, 0.1855, 0.1931, 0.2109],\n",
      "        [0.2145, 0.1948, 0.1852, 0.1938, 0.2116]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "6/100 Running Loss: 1.6156107187271118\n",
      "tensor([[0.2145, 0.1957, 0.1849, 0.1943, 0.2106],\n",
      "        [0.2148, 0.1942, 0.1856, 0.1937, 0.2118],\n",
      "        [0.2146, 0.1942, 0.1855, 0.1938, 0.2119],\n",
      "        [0.2149, 0.1948, 0.1858, 0.1933, 0.2112],\n",
      "        [0.2163, 0.1940, 0.1859, 0.1930, 0.2108],\n",
      "        [0.2162, 0.1939, 0.1860, 0.1933, 0.2106],\n",
      "        [0.2152, 0.1948, 0.1850, 0.1948, 0.2103],\n",
      "        [0.2159, 0.1944, 0.1857, 0.1938, 0.2103],\n",
      "        [0.2157, 0.1946, 0.1855, 0.1933, 0.2108],\n",
      "        [0.2138, 0.1952, 0.1853, 0.1941, 0.2116]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "7/100 Running Loss: 1.6153712272644043\n",
      "tensor([[0.2138, 0.1962, 0.1849, 0.1946, 0.2105],\n",
      "        [0.2141, 0.1946, 0.1856, 0.1939, 0.2118],\n",
      "        [0.2139, 0.1947, 0.1855, 0.1941, 0.2118],\n",
      "        [0.2142, 0.1952, 0.1859, 0.1937, 0.2111],\n",
      "        [0.2157, 0.1944, 0.1860, 0.1932, 0.2107],\n",
      "        [0.2155, 0.1944, 0.1860, 0.1936, 0.2105],\n",
      "        [0.2146, 0.1952, 0.1850, 0.1950, 0.2102],\n",
      "        [0.2153, 0.1948, 0.1857, 0.1940, 0.2102],\n",
      "        [0.2152, 0.1950, 0.1856, 0.1935, 0.2107],\n",
      "        [0.2132, 0.1957, 0.1853, 0.1944, 0.2115]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "8/100 Running Loss: 1.6151316165924072\n",
      "tensor([[0.2132, 0.1966, 0.1849, 0.1949, 0.2104],\n",
      "        [0.2134, 0.1951, 0.1856, 0.1942, 0.2117],\n",
      "        [0.2133, 0.1951, 0.1855, 0.1943, 0.2118],\n",
      "        [0.2134, 0.1957, 0.1859, 0.1940, 0.2110],\n",
      "        [0.2152, 0.1949, 0.1860, 0.1934, 0.2105],\n",
      "        [0.2149, 0.1949, 0.1861, 0.1938, 0.2104],\n",
      "        [0.2140, 0.1956, 0.1850, 0.1953, 0.2101],\n",
      "        [0.2147, 0.1953, 0.1857, 0.1942, 0.2101],\n",
      "        [0.2146, 0.1955, 0.1856, 0.1937, 0.2106],\n",
      "        [0.2125, 0.1961, 0.1853, 0.1946, 0.2115]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "9/100 Running Loss: 1.6148898601531982\n",
      "tensor([[0.2126, 0.1971, 0.1848, 0.1952, 0.2103],\n",
      "        [0.2128, 0.1955, 0.1856, 0.1944, 0.2117],\n",
      "        [0.2126, 0.1956, 0.1855, 0.1946, 0.2117],\n",
      "        [0.2127, 0.1962, 0.1859, 0.1943, 0.2109],\n",
      "        [0.2146, 0.1953, 0.1861, 0.1936, 0.2104],\n",
      "        [0.2143, 0.1953, 0.1861, 0.1940, 0.2102],\n",
      "        [0.2134, 0.1960, 0.1850, 0.1955, 0.2100],\n",
      "        [0.2142, 0.1957, 0.1857, 0.1945, 0.2099],\n",
      "        [0.2141, 0.1959, 0.1856, 0.1939, 0.2105],\n",
      "        [0.2118, 0.1966, 0.1853, 0.1949, 0.2114]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "10/100 Running Loss: 1.6146498918533325\n",
      "tensor([[0.2120, 0.1976, 0.1848, 0.1954, 0.2101],\n",
      "        [0.2121, 0.1960, 0.1856, 0.1947, 0.2116],\n",
      "        [0.2119, 0.1961, 0.1855, 0.1949, 0.2116],\n",
      "        [0.2120, 0.1967, 0.1859, 0.1947, 0.2108],\n",
      "        [0.2140, 0.1957, 0.1861, 0.1939, 0.2103],\n",
      "        [0.2137, 0.1958, 0.1861, 0.1943, 0.2101],\n",
      "        [0.2129, 0.1965, 0.1850, 0.1958, 0.2099],\n",
      "        [0.2136, 0.1962, 0.1857, 0.1947, 0.2098],\n",
      "        [0.2135, 0.1963, 0.1857, 0.1941, 0.2104],\n",
      "        [0.2112, 0.1971, 0.1853, 0.1952, 0.2113]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "11/100 Running Loss: 1.6144081354141235\n",
      "tensor([[0.2114, 0.1980, 0.1848, 0.1957, 0.2100],\n",
      "        [0.2114, 0.1965, 0.1856, 0.1950, 0.2115],\n",
      "        [0.2112, 0.1966, 0.1855, 0.1951, 0.2116],\n",
      "        [0.2114, 0.1972, 0.1858, 0.1949, 0.2107],\n",
      "        [0.2134, 0.1962, 0.1862, 0.1941, 0.2101],\n",
      "        [0.2131, 0.1963, 0.1861, 0.1945, 0.2100],\n",
      "        [0.2123, 0.1969, 0.1850, 0.1960, 0.2099],\n",
      "        [0.2130, 0.1966, 0.1858, 0.1949, 0.2097],\n",
      "        [0.2130, 0.1967, 0.1857, 0.1944, 0.2103],\n",
      "        [0.2106, 0.1975, 0.1853, 0.1954, 0.2112]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "12/100 Running Loss: 1.6141670942306519\n",
      "tensor([[0.2109, 0.1985, 0.1848, 0.1960, 0.2099],\n",
      "        [0.2107, 0.1970, 0.1855, 0.1953, 0.2115],\n",
      "        [0.2106, 0.1971, 0.1854, 0.1953, 0.2115],\n",
      "        [0.2107, 0.1977, 0.1858, 0.1952, 0.2106],\n",
      "        [0.2128, 0.1967, 0.1862, 0.1943, 0.2100],\n",
      "        [0.2125, 0.1968, 0.1861, 0.1947, 0.2099],\n",
      "        [0.2117, 0.1973, 0.1850, 0.1962, 0.2098],\n",
      "        [0.2124, 0.1970, 0.1858, 0.1952, 0.2096],\n",
      "        [0.2124, 0.1970, 0.1858, 0.1946, 0.2102],\n",
      "        [0.2100, 0.1979, 0.1852, 0.1957, 0.2111]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "13/100 Running Loss: 1.6139283180236816\n",
      "tensor([[0.2103, 0.1989, 0.1847, 0.1963, 0.2098],\n",
      "        [0.2101, 0.1975, 0.1855, 0.1955, 0.2114],\n",
      "        [0.2099, 0.1976, 0.1854, 0.1956, 0.2115],\n",
      "        [0.2101, 0.1982, 0.1858, 0.1955, 0.2105],\n",
      "        [0.2122, 0.1972, 0.1863, 0.1945, 0.2098],\n",
      "        [0.2120, 0.1973, 0.1861, 0.1949, 0.2098],\n",
      "        [0.2111, 0.1977, 0.1850, 0.1965, 0.2097],\n",
      "        [0.2119, 0.1975, 0.1858, 0.1954, 0.2095],\n",
      "        [0.2119, 0.1974, 0.1858, 0.1948, 0.2101],\n",
      "        [0.2095, 0.1983, 0.1851, 0.1959, 0.2111]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "14/100 Running Loss: 1.6136897802352905\n",
      "tensor([[0.2097, 0.1994, 0.1847, 0.1966, 0.2097],\n",
      "        [0.2094, 0.1980, 0.1854, 0.1958, 0.2114],\n",
      "        [0.2093, 0.1981, 0.1854, 0.1959, 0.2114],\n",
      "        [0.2095, 0.1986, 0.1857, 0.1958, 0.2104],\n",
      "        [0.2116, 0.1976, 0.1863, 0.1948, 0.2097],\n",
      "        [0.2114, 0.1977, 0.1861, 0.1951, 0.2097],\n",
      "        [0.2106, 0.1981, 0.1849, 0.1967, 0.2097],\n",
      "        [0.2113, 0.1980, 0.1857, 0.1956, 0.2094],\n",
      "        [0.2113, 0.1978, 0.1858, 0.1951, 0.2100],\n",
      "        [0.2090, 0.1987, 0.1851, 0.1961, 0.2111]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "15/100 Running Loss: 1.6134532690048218\n",
      "tensor([[0.2092, 0.1999, 0.1846, 0.1968, 0.2096],\n",
      "        [0.2088, 0.1985, 0.1854, 0.1960, 0.2113],\n",
      "        [0.2086, 0.1986, 0.1853, 0.1961, 0.2114],\n",
      "        [0.2089, 0.1991, 0.1857, 0.1961, 0.2103],\n",
      "        [0.2111, 0.1980, 0.1863, 0.1951, 0.2095],\n",
      "        [0.2108, 0.1982, 0.1860, 0.1954, 0.2096],\n",
      "        [0.2100, 0.1986, 0.1849, 0.1970, 0.2096],\n",
      "        [0.2108, 0.1984, 0.1857, 0.1958, 0.2093],\n",
      "        [0.2107, 0.1982, 0.1859, 0.1953, 0.2098],\n",
      "        [0.2085, 0.1991, 0.1850, 0.1963, 0.2110]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "16/100 Running Loss: 1.6132208108901978\n",
      "tensor([[0.2086, 0.2004, 0.1845, 0.1971, 0.2094],\n",
      "        [0.2081, 0.1990, 0.1854, 0.1962, 0.2113],\n",
      "        [0.2080, 0.1991, 0.1853, 0.1963, 0.2114],\n",
      "        [0.2083, 0.1995, 0.1857, 0.1963, 0.2101],\n",
      "        [0.2105, 0.1984, 0.1863, 0.1953, 0.2094],\n",
      "        [0.2102, 0.1987, 0.1860, 0.1957, 0.2094],\n",
      "        [0.2094, 0.1990, 0.1849, 0.1972, 0.2095],\n",
      "        [0.2102, 0.1989, 0.1857, 0.1960, 0.2092],\n",
      "        [0.2102, 0.1987, 0.1859, 0.1956, 0.2097],\n",
      "        [0.2080, 0.1995, 0.1850, 0.1965, 0.2110]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "17/100 Running Loss: 1.6129878759384155\n",
      "tensor([[0.2079, 0.2009, 0.1845, 0.1974, 0.2093],\n",
      "        [0.2075, 0.1995, 0.1853, 0.1965, 0.2112],\n",
      "        [0.2073, 0.1996, 0.1852, 0.1966, 0.2113],\n",
      "        [0.2077, 0.2000, 0.1856, 0.1966, 0.2100],\n",
      "        [0.2100, 0.1989, 0.1863, 0.1956, 0.2092],\n",
      "        [0.2096, 0.1992, 0.1860, 0.1960, 0.2093],\n",
      "        [0.2089, 0.1994, 0.1849, 0.1975, 0.2094],\n",
      "        [0.2096, 0.1994, 0.1856, 0.1963, 0.2091],\n",
      "        [0.2096, 0.1991, 0.1859, 0.1958, 0.2096],\n",
      "        [0.2076, 0.1999, 0.1849, 0.1967, 0.2109]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "18/100 Running Loss: 1.6127527952194214\n",
      "tensor([[0.2073, 0.2014, 0.1844, 0.1977, 0.2091],\n",
      "        [0.2069, 0.1999, 0.1852, 0.1968, 0.2112],\n",
      "        [0.2067, 0.2001, 0.1851, 0.1969, 0.2113],\n",
      "        [0.2071, 0.2005, 0.1856, 0.1969, 0.2099],\n",
      "        [0.2094, 0.1993, 0.1863, 0.1959, 0.2090],\n",
      "        [0.2090, 0.1997, 0.1859, 0.1962, 0.2091],\n",
      "        [0.2083, 0.1998, 0.1849, 0.1977, 0.2093],\n",
      "        [0.2090, 0.1999, 0.1856, 0.1965, 0.2089],\n",
      "        [0.2090, 0.1995, 0.1859, 0.1961, 0.2094],\n",
      "        [0.2071, 0.2002, 0.1849, 0.1970, 0.2109]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "19/100 Running Loss: 1.6125162839889526\n",
      "tensor([[0.2068, 0.2020, 0.1844, 0.1979, 0.2089],\n",
      "        [0.2063, 0.2004, 0.1852, 0.1970, 0.2111],\n",
      "        [0.2061, 0.2005, 0.1851, 0.1971, 0.2112],\n",
      "        [0.2065, 0.2009, 0.1856, 0.1972, 0.2098],\n",
      "        [0.2089, 0.1998, 0.1863, 0.1962, 0.2088],\n",
      "        [0.2084, 0.2002, 0.1859, 0.1965, 0.2090],\n",
      "        [0.2078, 0.2003, 0.1848, 0.1979, 0.2091],\n",
      "        [0.2084, 0.2005, 0.1855, 0.1968, 0.2088],\n",
      "        [0.2084, 0.1999, 0.1860, 0.1964, 0.2093],\n",
      "        [0.2066, 0.2006, 0.1848, 0.1972, 0.2108]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "20/100 Running Loss: 1.6122796535491943\n",
      "tensor([[0.2062, 0.2025, 0.1843, 0.1982, 0.2088],\n",
      "        [0.2057, 0.2008, 0.1851, 0.1973, 0.2111],\n",
      "        [0.2055, 0.2009, 0.1850, 0.1974, 0.2112],\n",
      "        [0.2059, 0.2014, 0.1855, 0.1975, 0.2096],\n",
      "        [0.2083, 0.2003, 0.1864, 0.1965, 0.2086],\n",
      "        [0.2078, 0.2007, 0.1859, 0.1968, 0.2088],\n",
      "        [0.2073, 0.2007, 0.1848, 0.1982, 0.2090],\n",
      "        [0.2078, 0.2010, 0.1855, 0.1971, 0.2086],\n",
      "        [0.2078, 0.2004, 0.1860, 0.1967, 0.2091],\n",
      "        [0.2061, 0.2010, 0.1848, 0.1974, 0.2107]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "21/100 Running Loss: 1.6120408773422241\n",
      "tensor([[0.2056, 0.2030, 0.1842, 0.1985, 0.2086],\n",
      "        [0.2051, 0.2012, 0.1850, 0.1976, 0.2111],\n",
      "        [0.2049, 0.2014, 0.1849, 0.1977, 0.2112],\n",
      "        [0.2054, 0.2019, 0.1855, 0.1978, 0.2095],\n",
      "        [0.2077, 0.2008, 0.1864, 0.1968, 0.2084],\n",
      "        [0.2072, 0.2013, 0.1858, 0.1971, 0.2086],\n",
      "        [0.2068, 0.2012, 0.1847, 0.1985, 0.2088],\n",
      "        [0.2071, 0.2016, 0.1855, 0.1974, 0.2084],\n",
      "        [0.2073, 0.2008, 0.1860, 0.1970, 0.2089],\n",
      "        [0.2056, 0.2014, 0.1848, 0.1976, 0.2107]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "22/100 Running Loss: 1.6118005514144897\n",
      "tensor([[0.2050, 0.2036, 0.1842, 0.1988, 0.2084],\n",
      "        [0.2046, 0.2016, 0.1849, 0.1978, 0.2110],\n",
      "        [0.2044, 0.2018, 0.1848, 0.1979, 0.2112],\n",
      "        [0.2048, 0.2023, 0.1855, 0.1981, 0.2094],\n",
      "        [0.2071, 0.2013, 0.1864, 0.1971, 0.2082],\n",
      "        [0.2066, 0.2018, 0.1858, 0.1974, 0.2084],\n",
      "        [0.2062, 0.2017, 0.1847, 0.1988, 0.2087],\n",
      "        [0.2065, 0.2021, 0.1854, 0.1977, 0.2083],\n",
      "        [0.2067, 0.2012, 0.1860, 0.1973, 0.2088],\n",
      "        [0.2050, 0.2017, 0.1848, 0.1979, 0.2106]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "23/100 Running Loss: 1.6115572452545166\n",
      "tensor([[0.2044, 0.2041, 0.1841, 0.1991, 0.2082],\n",
      "        [0.2040, 0.2021, 0.1849, 0.1981, 0.2110],\n",
      "        [0.2038, 0.2022, 0.1847, 0.1982, 0.2111],\n",
      "        [0.2042, 0.2027, 0.1854, 0.1984, 0.2092],\n",
      "        [0.2066, 0.2017, 0.1863, 0.1974, 0.2080],\n",
      "        [0.2059, 0.2024, 0.1858, 0.1977, 0.2082],\n",
      "        [0.2057, 0.2021, 0.1847, 0.1990, 0.2085],\n",
      "        [0.2059, 0.2027, 0.1854, 0.1980, 0.2080],\n",
      "        [0.2062, 0.2016, 0.1860, 0.1975, 0.2086],\n",
      "        [0.2045, 0.2021, 0.1847, 0.1981, 0.2105]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "24/100 Running Loss: 1.611312985420227\n",
      "tensor([[0.2039, 0.2046, 0.1841, 0.1994, 0.2080],\n",
      "        [0.2034, 0.2025, 0.1848, 0.1984, 0.2109],\n",
      "        [0.2031, 0.2027, 0.1846, 0.1985, 0.2111],\n",
      "        [0.2036, 0.2032, 0.1854, 0.1987, 0.2091],\n",
      "        [0.2060, 0.2022, 0.1863, 0.1976, 0.2078],\n",
      "        [0.2053, 0.2029, 0.1858, 0.1980, 0.2080],\n",
      "        [0.2051, 0.2026, 0.1847, 0.1993, 0.2083],\n",
      "        [0.2053, 0.2033, 0.1854, 0.1983, 0.2078],\n",
      "        [0.2057, 0.2021, 0.1860, 0.1978, 0.2085],\n",
      "        [0.2039, 0.2025, 0.1847, 0.1984, 0.2104]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "25/100 Running Loss: 1.6110684871673584\n",
      "tensor([[0.2033, 0.2052, 0.1841, 0.1997, 0.2078],\n",
      "        [0.2027, 0.2029, 0.1847, 0.1987, 0.2109],\n",
      "        [0.2025, 0.2031, 0.1845, 0.1988, 0.2110],\n",
      "        [0.2030, 0.2036, 0.1854, 0.1991, 0.2089],\n",
      "        [0.2055, 0.2027, 0.1863, 0.1979, 0.2076],\n",
      "        [0.2047, 0.2034, 0.1858, 0.1983, 0.2078],\n",
      "        [0.2045, 0.2032, 0.1846, 0.1996, 0.2081],\n",
      "        [0.2046, 0.2039, 0.1854, 0.1986, 0.2076],\n",
      "        [0.2052, 0.2025, 0.1860, 0.1980, 0.2083],\n",
      "        [0.2034, 0.2029, 0.1847, 0.1986, 0.2103]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "26/100 Running Loss: 1.6108237504959106\n",
      "tensor([[0.2026, 0.2058, 0.1840, 0.2000, 0.2076],\n",
      "        [0.2021, 0.2034, 0.1847, 0.1990, 0.2108],\n",
      "        [0.2019, 0.2035, 0.1845, 0.1991, 0.2110],\n",
      "        [0.2024, 0.2041, 0.1854, 0.1994, 0.2087],\n",
      "        [0.2050, 0.2031, 0.1863, 0.1982, 0.2073],\n",
      "        [0.2042, 0.2039, 0.1857, 0.1986, 0.2076],\n",
      "        [0.2039, 0.2037, 0.1846, 0.1999, 0.2079],\n",
      "        [0.2040, 0.2045, 0.1853, 0.1989, 0.2073],\n",
      "        [0.2047, 0.2029, 0.1860, 0.1983, 0.2082],\n",
      "        [0.2028, 0.2034, 0.1847, 0.1989, 0.2102]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "27/100 Running Loss: 1.6105763912200928\n",
      "tensor([[0.2020, 0.2064, 0.1839, 0.2003, 0.2074],\n",
      "        [0.2015, 0.2038, 0.1847, 0.1993, 0.2108],\n",
      "        [0.2012, 0.2040, 0.1844, 0.1994, 0.2109],\n",
      "        [0.2018, 0.2046, 0.1853, 0.1997, 0.2085],\n",
      "        [0.2045, 0.2036, 0.1863, 0.1984, 0.2071],\n",
      "        [0.2036, 0.2045, 0.1857, 0.1988, 0.2074],\n",
      "        [0.2034, 0.2042, 0.1845, 0.2002, 0.2077],\n",
      "        [0.2035, 0.2050, 0.1853, 0.1991, 0.2071],\n",
      "        [0.2041, 0.2034, 0.1860, 0.1985, 0.2080],\n",
      "        [0.2022, 0.2038, 0.1847, 0.1992, 0.2101]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "28/100 Running Loss: 1.610324501991272\n",
      "tensor([[0.2014, 0.2070, 0.1839, 0.2006, 0.2072],\n",
      "        [0.2008, 0.2043, 0.1846, 0.1996, 0.2107],\n",
      "        [0.2006, 0.2044, 0.1844, 0.1997, 0.2108],\n",
      "        [0.2012, 0.2051, 0.1853, 0.2001, 0.2083],\n",
      "        [0.2040, 0.2041, 0.1863, 0.1987, 0.2069],\n",
      "        [0.2031, 0.2050, 0.1857, 0.1991, 0.2071],\n",
      "        [0.2028, 0.2047, 0.1845, 0.2005, 0.2075],\n",
      "        [0.2029, 0.2056, 0.1852, 0.1994, 0.2068],\n",
      "        [0.2036, 0.2039, 0.1860, 0.1988, 0.2078],\n",
      "        [0.2016, 0.2043, 0.1846, 0.1995, 0.2099]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "29/100 Running Loss: 1.610067367553711\n",
      "tensor([[0.2007, 0.2076, 0.1838, 0.2009, 0.2070],\n",
      "        [0.2002, 0.2048, 0.1846, 0.1999, 0.2106],\n",
      "        [0.1999, 0.2049, 0.1844, 0.2000, 0.2107],\n",
      "        [0.2006, 0.2056, 0.1853, 0.2004, 0.2081],\n",
      "        [0.2034, 0.2047, 0.1863, 0.1990, 0.2066],\n",
      "        [0.2025, 0.2055, 0.1857, 0.1994, 0.2069],\n",
      "        [0.2023, 0.2052, 0.1844, 0.2008, 0.2073],\n",
      "        [0.2023, 0.2062, 0.1852, 0.1997, 0.2065],\n",
      "        [0.2031, 0.2044, 0.1860, 0.1990, 0.2075],\n",
      "        [0.2010, 0.2048, 0.1846, 0.1998, 0.2098]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "30/100 Running Loss: 1.609804391860962\n",
      "tensor([[0.2000, 0.2082, 0.1837, 0.2012, 0.2068],\n",
      "        [0.1995, 0.2053, 0.1845, 0.2002, 0.2105],\n",
      "        [0.1993, 0.2054, 0.1843, 0.2003, 0.2106],\n",
      "        [0.2000, 0.2061, 0.1853, 0.2007, 0.2079],\n",
      "        [0.2029, 0.2052, 0.1863, 0.1993, 0.2063],\n",
      "        [0.2020, 0.2061, 0.1856, 0.1997, 0.2066],\n",
      "        [0.2017, 0.2058, 0.1844, 0.2011, 0.2070],\n",
      "        [0.2017, 0.2068, 0.1852, 0.2000, 0.2062],\n",
      "        [0.2025, 0.2049, 0.1860, 0.1993, 0.2073],\n",
      "        [0.2004, 0.2052, 0.1846, 0.2001, 0.2096]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "31/100 Running Loss: 1.6095349788665771\n",
      "tensor([[0.1993, 0.2089, 0.1837, 0.2015, 0.2066],\n",
      "        [0.1989, 0.2058, 0.1845, 0.2005, 0.2103],\n",
      "        [0.1986, 0.2059, 0.1843, 0.2007, 0.2105],\n",
      "        [0.1993, 0.2066, 0.1853, 0.2011, 0.2077],\n",
      "        [0.2023, 0.2058, 0.1863, 0.1995, 0.2060],\n",
      "        [0.2014, 0.2067, 0.1856, 0.2000, 0.2063],\n",
      "        [0.2011, 0.2063, 0.1843, 0.2014, 0.2068],\n",
      "        [0.2012, 0.2075, 0.1851, 0.2003, 0.2059],\n",
      "        [0.2020, 0.2054, 0.1860, 0.1996, 0.2071],\n",
      "        [0.1997, 0.2057, 0.1846, 0.2005, 0.2094]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "32/100 Running Loss: 1.6092591285705566\n",
      "tensor([[0.1986, 0.2096, 0.1836, 0.2018, 0.2064],\n",
      "        [0.1982, 0.2063, 0.1845, 0.2008, 0.2102],\n",
      "        [0.1980, 0.2065, 0.1843, 0.2010, 0.2104],\n",
      "        [0.1987, 0.2072, 0.1853, 0.2014, 0.2074],\n",
      "        [0.2018, 0.2063, 0.1863, 0.1998, 0.2058],\n",
      "        [0.2008, 0.2073, 0.1856, 0.2003, 0.2059],\n",
      "        [0.2006, 0.2069, 0.1843, 0.2017, 0.2066],\n",
      "        [0.2006, 0.2081, 0.1851, 0.2006, 0.2056],\n",
      "        [0.2014, 0.2059, 0.1860, 0.1998, 0.2069],\n",
      "        [0.1991, 0.2063, 0.1846, 0.2008, 0.2092]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "33/100 Running Loss: 1.6089746952056885\n",
      "tensor([[0.1979, 0.2103, 0.1836, 0.2021, 0.2061],\n",
      "        [0.1975, 0.2068, 0.1844, 0.2011, 0.2101],\n",
      "        [0.1973, 0.2070, 0.1842, 0.2013, 0.2102],\n",
      "        [0.1981, 0.2077, 0.1854, 0.2018, 0.2070],\n",
      "        [0.2012, 0.2069, 0.1863, 0.2001, 0.2055],\n",
      "        [0.2002, 0.2080, 0.1856, 0.2006, 0.2056],\n",
      "        [0.2000, 0.2075, 0.1843, 0.2019, 0.2063],\n",
      "        [0.1999, 0.2088, 0.1850, 0.2009, 0.2053],\n",
      "        [0.2009, 0.2065, 0.1859, 0.2001, 0.2066],\n",
      "        [0.1984, 0.2068, 0.1846, 0.2011, 0.2090]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "34/100 Running Loss: 1.608683466911316\n",
      "tensor([[0.1972, 0.2110, 0.1835, 0.2025, 0.2058],\n",
      "        [0.1968, 0.2074, 0.1844, 0.2015, 0.2099],\n",
      "        [0.1966, 0.2076, 0.1842, 0.2016, 0.2101],\n",
      "        [0.1974, 0.2083, 0.1854, 0.2022, 0.2066],\n",
      "        [0.2006, 0.2076, 0.1863, 0.2003, 0.2052],\n",
      "        [0.1996, 0.2086, 0.1856, 0.2009, 0.2053],\n",
      "        [0.1994, 0.2081, 0.1842, 0.2022, 0.2060],\n",
      "        [0.1993, 0.2095, 0.1850, 0.2012, 0.2050],\n",
      "        [0.2003, 0.2070, 0.1859, 0.2004, 0.2064],\n",
      "        [0.1978, 0.2074, 0.1846, 0.2015, 0.2087]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "35/100 Running Loss: 1.608385443687439\n",
      "tensor([[0.1964, 0.2118, 0.1834, 0.2028, 0.2055],\n",
      "        [0.1961, 0.2080, 0.1844, 0.2018, 0.2097],\n",
      "        [0.1958, 0.2082, 0.1842, 0.2019, 0.2098],\n",
      "        [0.1968, 0.2089, 0.1855, 0.2026, 0.2062],\n",
      "        [0.2000, 0.2082, 0.1863, 0.2006, 0.2048],\n",
      "        [0.1990, 0.2093, 0.1855, 0.2012, 0.2050],\n",
      "        [0.1987, 0.2088, 0.1842, 0.2025, 0.2057],\n",
      "        [0.1987, 0.2101, 0.1849, 0.2015, 0.2047],\n",
      "        [0.1997, 0.2076, 0.1859, 0.2006, 0.2061],\n",
      "        [0.1971, 0.2080, 0.1846, 0.2018, 0.2084]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "36/100 Running Loss: 1.608082890510559\n",
      "tensor([[0.1957, 0.2125, 0.1834, 0.2031, 0.2052],\n",
      "        [0.1954, 0.2086, 0.1844, 0.2022, 0.2094],\n",
      "        [0.1951, 0.2088, 0.1842, 0.2023, 0.2096],\n",
      "        [0.1961, 0.2096, 0.1855, 0.2030, 0.2058],\n",
      "        [0.1994, 0.2088, 0.1863, 0.2009, 0.2045],\n",
      "        [0.1983, 0.2100, 0.1855, 0.2015, 0.2046],\n",
      "        [0.1981, 0.2094, 0.1842, 0.2028, 0.2054],\n",
      "        [0.1980, 0.2108, 0.1849, 0.2018, 0.2045],\n",
      "        [0.1991, 0.2082, 0.1859, 0.2009, 0.2058],\n",
      "        [0.1963, 0.2087, 0.1847, 0.2022, 0.2081]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "37/100 Running Loss: 1.6077747344970703\n",
      "tensor([[0.1949, 0.2133, 0.1833, 0.2035, 0.2049],\n",
      "        [0.1947, 0.2092, 0.1844, 0.2026, 0.2091],\n",
      "        [0.1944, 0.2095, 0.1842, 0.2027, 0.2092],\n",
      "        [0.1954, 0.2103, 0.1855, 0.2034, 0.2054],\n",
      "        [0.1988, 0.2095, 0.1863, 0.2012, 0.2041],\n",
      "        [0.1977, 0.2107, 0.1855, 0.2018, 0.2043],\n",
      "        [0.1975, 0.2101, 0.1842, 0.2031, 0.2051],\n",
      "        [0.1973, 0.2115, 0.1849, 0.2022, 0.2042],\n",
      "        [0.1985, 0.2089, 0.1859, 0.2012, 0.2055],\n",
      "        [0.1956, 0.2093, 0.1847, 0.2026, 0.2079]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "38/100 Running Loss: 1.6074625253677368\n",
      "tensor([[0.1942, 0.2141, 0.1832, 0.2039, 0.2046],\n",
      "        [0.1940, 0.2098, 0.1844, 0.2030, 0.2088],\n",
      "        [0.1937, 0.2101, 0.1842, 0.2032, 0.2089],\n",
      "        [0.1947, 0.2109, 0.1856, 0.2038, 0.2049],\n",
      "        [0.1982, 0.2102, 0.1863, 0.2015, 0.2038],\n",
      "        [0.1971, 0.2114, 0.1855, 0.2021, 0.2039],\n",
      "        [0.1968, 0.2108, 0.1841, 0.2034, 0.2048],\n",
      "        [0.1966, 0.2122, 0.1848, 0.2025, 0.2038],\n",
      "        [0.1979, 0.2095, 0.1859, 0.2015, 0.2052],\n",
      "        [0.1949, 0.2100, 0.1847, 0.2029, 0.2076]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "39/100 Running Loss: 1.607147216796875\n",
      "tensor([[0.1934, 0.2149, 0.1832, 0.2042, 0.2043],\n",
      "        [0.1932, 0.2104, 0.1844, 0.2034, 0.2085],\n",
      "        [0.1929, 0.2107, 0.1842, 0.2036, 0.2086],\n",
      "        [0.1940, 0.2116, 0.1856, 0.2043, 0.2045],\n",
      "        [0.1976, 0.2108, 0.1863, 0.2018, 0.2035],\n",
      "        [0.1964, 0.2121, 0.1855, 0.2025, 0.2035],\n",
      "        [0.1962, 0.2116, 0.1841, 0.2037, 0.2044],\n",
      "        [0.1960, 0.2129, 0.1848, 0.2029, 0.2035],\n",
      "        [0.1973, 0.2102, 0.1859, 0.2018, 0.2048],\n",
      "        [0.1942, 0.2106, 0.1847, 0.2033, 0.2072]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "40/100 Running Loss: 1.6068284511566162\n",
      "tensor([[0.1927, 0.2157, 0.1831, 0.2045, 0.2040],\n",
      "        [0.1925, 0.2111, 0.1844, 0.2039, 0.2082],\n",
      "        [0.1922, 0.2114, 0.1842, 0.2040, 0.2083],\n",
      "        [0.1933, 0.2123, 0.1856, 0.2047, 0.2041],\n",
      "        [0.1970, 0.2115, 0.1863, 0.2021, 0.2031],\n",
      "        [0.1957, 0.2128, 0.1855, 0.2028, 0.2031],\n",
      "        [0.1955, 0.2123, 0.1841, 0.2040, 0.2041],\n",
      "        [0.1953, 0.2136, 0.1848, 0.2032, 0.2031],\n",
      "        [0.1966, 0.2108, 0.1859, 0.2021, 0.2045],\n",
      "        [0.1935, 0.2112, 0.1848, 0.2037, 0.2069]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "41/100 Running Loss: 1.6065047979354858\n",
      "tensor([[0.1919, 0.2165, 0.1831, 0.2048, 0.2036],\n",
      "        [0.1917, 0.2117, 0.1844, 0.2043, 0.2078],\n",
      "        [0.1914, 0.2120, 0.1842, 0.2044, 0.2080],\n",
      "        [0.1925, 0.2130, 0.1857, 0.2051, 0.2037],\n",
      "        [0.1963, 0.2122, 0.1864, 0.2024, 0.2027],\n",
      "        [0.1951, 0.2135, 0.1855, 0.2032, 0.2028],\n",
      "        [0.1948, 0.2131, 0.1840, 0.2043, 0.2038],\n",
      "        [0.1946, 0.2144, 0.1848, 0.2036, 0.2027],\n",
      "        [0.1960, 0.2115, 0.1859, 0.2024, 0.2042],\n",
      "        [0.1928, 0.2119, 0.1848, 0.2041, 0.2065]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "42/100 Running Loss: 1.6061733961105347\n",
      "tensor([[0.1912, 0.2172, 0.1831, 0.2052, 0.2032],\n",
      "        [0.1909, 0.2124, 0.1844, 0.2047, 0.2075],\n",
      "        [0.1906, 0.2127, 0.1842, 0.2049, 0.2077],\n",
      "        [0.1918, 0.2137, 0.1857, 0.2055, 0.2033],\n",
      "        [0.1956, 0.2129, 0.1864, 0.2028, 0.2023],\n",
      "        [0.1944, 0.2142, 0.1855, 0.2035, 0.2024],\n",
      "        [0.1941, 0.2139, 0.1840, 0.2046, 0.2034],\n",
      "        [0.1939, 0.2151, 0.1848, 0.2039, 0.2023],\n",
      "        [0.1953, 0.2122, 0.1859, 0.2027, 0.2039],\n",
      "        [0.1920, 0.2125, 0.1849, 0.2044, 0.2062]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "43/100 Running Loss: 1.6058316230773926\n",
      "tensor([[0.1905, 0.2180, 0.1831, 0.2055, 0.2029],\n",
      "        [0.1902, 0.2130, 0.1845, 0.2051, 0.2072],\n",
      "        [0.1898, 0.2133, 0.1842, 0.2053, 0.2074],\n",
      "        [0.1910, 0.2144, 0.1858, 0.2059, 0.2029],\n",
      "        [0.1950, 0.2136, 0.1865, 0.2031, 0.2019],\n",
      "        [0.1937, 0.2150, 0.1855, 0.2039, 0.2019],\n",
      "        [0.1933, 0.2147, 0.1839, 0.2050, 0.2031],\n",
      "        [0.1932, 0.2159, 0.1847, 0.2043, 0.2019],\n",
      "        [0.1947, 0.2129, 0.1859, 0.2030, 0.2035],\n",
      "        [0.1913, 0.2132, 0.1849, 0.2048, 0.2058]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "44/100 Running Loss: 1.6054823398590088\n",
      "tensor([[0.1897, 0.2188, 0.1831, 0.2059, 0.2025],\n",
      "        [0.1894, 0.2137, 0.1845, 0.2055, 0.2069],\n",
      "        [0.1890, 0.2140, 0.1842, 0.2056, 0.2071],\n",
      "        [0.1901, 0.2152, 0.1858, 0.2063, 0.2025],\n",
      "        [0.1943, 0.2143, 0.1865, 0.2034, 0.2015],\n",
      "        [0.1929, 0.2158, 0.1855, 0.2042, 0.2015],\n",
      "        [0.1926, 0.2155, 0.1839, 0.2053, 0.2028],\n",
      "        [0.1924, 0.2168, 0.1847, 0.2047, 0.2015],\n",
      "        [0.1940, 0.2136, 0.1860, 0.2033, 0.2032],\n",
      "        [0.1905, 0.2138, 0.1850, 0.2052, 0.2055]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "45/100 Running Loss: 1.6051249504089355\n",
      "tensor([[0.1889, 0.2197, 0.1831, 0.2063, 0.2021],\n",
      "        [0.1886, 0.2144, 0.1845, 0.2059, 0.2066],\n",
      "        [0.1882, 0.2147, 0.1843, 0.2060, 0.2068],\n",
      "        [0.1893, 0.2159, 0.1859, 0.2067, 0.2021],\n",
      "        [0.1936, 0.2151, 0.1865, 0.2037, 0.2010],\n",
      "        [0.1922, 0.2166, 0.1855, 0.2046, 0.2010],\n",
      "        [0.1918, 0.2163, 0.1839, 0.2056, 0.2024],\n",
      "        [0.1917, 0.2176, 0.1847, 0.2050, 0.2010],\n",
      "        [0.1933, 0.2143, 0.1860, 0.2036, 0.2028],\n",
      "        [0.1897, 0.2145, 0.1850, 0.2056, 0.2052]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "46/100 Running Loss: 1.604759693145752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1881, 0.2205, 0.1830, 0.2066, 0.2017],\n",
      "        [0.1877, 0.2151, 0.1846, 0.2063, 0.2064],\n",
      "        [0.1874, 0.2154, 0.1843, 0.2064, 0.2065],\n",
      "        [0.1884, 0.2167, 0.1860, 0.2071, 0.2018],\n",
      "        [0.1929, 0.2159, 0.1866, 0.2040, 0.2006],\n",
      "        [0.1914, 0.2175, 0.1856, 0.2049, 0.2006],\n",
      "        [0.1911, 0.2171, 0.1838, 0.2059, 0.2021],\n",
      "        [0.1909, 0.2184, 0.1847, 0.2053, 0.2006],\n",
      "        [0.1927, 0.2149, 0.1860, 0.2039, 0.2024],\n",
      "        [0.1888, 0.2152, 0.1851, 0.2060, 0.2049]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "47/100 Running Loss: 1.604384422302246\n",
      "tensor([[0.1873, 0.2214, 0.1830, 0.2070, 0.2013],\n",
      "        [0.1868, 0.2158, 0.1846, 0.2067, 0.2061],\n",
      "        [0.1865, 0.2161, 0.1843, 0.2068, 0.2063],\n",
      "        [0.1876, 0.2175, 0.1860, 0.2075, 0.2014],\n",
      "        [0.1922, 0.2167, 0.1866, 0.2044, 0.2002],\n",
      "        [0.1907, 0.2183, 0.1856, 0.2053, 0.2001],\n",
      "        [0.1903, 0.2179, 0.1838, 0.2062, 0.2018],\n",
      "        [0.1901, 0.2193, 0.1847, 0.2057, 0.2001],\n",
      "        [0.1920, 0.2156, 0.1861, 0.2042, 0.2021],\n",
      "        [0.1880, 0.2159, 0.1851, 0.2063, 0.2046]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "48/100 Running Loss: 1.6039988994598389\n",
      "tensor([[0.1864, 0.2224, 0.1830, 0.2073, 0.2009],\n",
      "        [0.1859, 0.2165, 0.1846, 0.2071, 0.2059],\n",
      "        [0.1856, 0.2168, 0.1844, 0.2072, 0.2060],\n",
      "        [0.1867, 0.2183, 0.1861, 0.2079, 0.2010],\n",
      "        [0.1914, 0.2175, 0.1866, 0.2047, 0.1997],\n",
      "        [0.1899, 0.2192, 0.1856, 0.2056, 0.1997],\n",
      "        [0.1895, 0.2188, 0.1837, 0.2065, 0.2015],\n",
      "        [0.1893, 0.2203, 0.1847, 0.2060, 0.1997],\n",
      "        [0.1913, 0.2164, 0.1861, 0.2045, 0.2017],\n",
      "        [0.1871, 0.2167, 0.1852, 0.2067, 0.2043]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "49/100 Running Loss: 1.6036025285720825\n",
      "tensor([[0.1855, 0.2234, 0.1830, 0.2077, 0.2004],\n",
      "        [0.1850, 0.2172, 0.1847, 0.2075, 0.2056],\n",
      "        [0.1846, 0.2175, 0.1844, 0.2077, 0.2058],\n",
      "        [0.1857, 0.2191, 0.1862, 0.2083, 0.2006],\n",
      "        [0.1906, 0.2184, 0.1867, 0.2050, 0.1993],\n",
      "        [0.1891, 0.2201, 0.1856, 0.2060, 0.1992],\n",
      "        [0.1887, 0.2196, 0.1837, 0.2069, 0.2011],\n",
      "        [0.1885, 0.2212, 0.1847, 0.2063, 0.1992],\n",
      "        [0.1905, 0.2171, 0.1862, 0.2048, 0.2013],\n",
      "        [0.1862, 0.2174, 0.1852, 0.2071, 0.2040]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "50/100 Running Loss: 1.6031956672668457\n",
      "tensor([[0.1846, 0.2244, 0.1830, 0.2080, 0.2000],\n",
      "        [0.1840, 0.2180, 0.1847, 0.2079, 0.2054],\n",
      "        [0.1836, 0.2183, 0.1844, 0.2081, 0.2056],\n",
      "        [0.1848, 0.2200, 0.1862, 0.2088, 0.2003],\n",
      "        [0.1899, 0.2192, 0.1867, 0.2053, 0.1988],\n",
      "        [0.1882, 0.2211, 0.1857, 0.2063, 0.1988],\n",
      "        [0.1879, 0.2205, 0.1837, 0.2072, 0.2008],\n",
      "        [0.1877, 0.2221, 0.1847, 0.2067, 0.1988],\n",
      "        [0.1898, 0.2179, 0.1862, 0.2051, 0.2010],\n",
      "        [0.1853, 0.2182, 0.1853, 0.2075, 0.2037]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "51/100 Running Loss: 1.6027785539627075\n",
      "tensor([[0.1837, 0.2255, 0.1829, 0.2084, 0.1995],\n",
      "        [0.1830, 0.2188, 0.1847, 0.2083, 0.2052],\n",
      "        [0.1826, 0.2191, 0.1844, 0.2085, 0.2053],\n",
      "        [0.1838, 0.2208, 0.1863, 0.2092, 0.1999],\n",
      "        [0.1890, 0.2201, 0.1868, 0.2057, 0.1984],\n",
      "        [0.1874, 0.2220, 0.1857, 0.2066, 0.1983],\n",
      "        [0.1870, 0.2214, 0.1836, 0.2075, 0.2004],\n",
      "        [0.1869, 0.2231, 0.1847, 0.2070, 0.1983],\n",
      "        [0.1890, 0.2187, 0.1863, 0.2054, 0.2006],\n",
      "        [0.1843, 0.2190, 0.1854, 0.2079, 0.2034]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "52/100 Running Loss: 1.6023489236831665\n",
      "tensor([[0.1827, 0.2266, 0.1829, 0.2088, 0.1990],\n",
      "        [0.1820, 0.2196, 0.1847, 0.2088, 0.2049],\n",
      "        [0.1816, 0.2199, 0.1844, 0.2089, 0.2051],\n",
      "        [0.1828, 0.2217, 0.1863, 0.2096, 0.1995],\n",
      "        [0.1882, 0.2211, 0.1868, 0.2060, 0.1979],\n",
      "        [0.1865, 0.2230, 0.1857, 0.2070, 0.1978],\n",
      "        [0.1862, 0.2223, 0.1836, 0.2079, 0.2000],\n",
      "        [0.1860, 0.2242, 0.1847, 0.2073, 0.1978],\n",
      "        [0.1882, 0.2195, 0.1863, 0.2058, 0.2002],\n",
      "        [0.1833, 0.2198, 0.1854, 0.2083, 0.2031]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "53/100 Running Loss: 1.6019090414047241\n",
      "tensor([[0.1818, 0.2277, 0.1829, 0.2091, 0.1985],\n",
      "        [0.1810, 0.2204, 0.1847, 0.2092, 0.2047],\n",
      "        [0.1805, 0.2208, 0.1844, 0.2094, 0.2049],\n",
      "        [0.1817, 0.2227, 0.1864, 0.2101, 0.1992],\n",
      "        [0.1874, 0.2220, 0.1869, 0.2063, 0.1974],\n",
      "        [0.1856, 0.2240, 0.1858, 0.2073, 0.1973],\n",
      "        [0.1853, 0.2233, 0.1836, 0.2083, 0.1996],\n",
      "        [0.1851, 0.2253, 0.1847, 0.2077, 0.1973],\n",
      "        [0.1874, 0.2204, 0.1864, 0.2061, 0.1998],\n",
      "        [0.1823, 0.2207, 0.1855, 0.2087, 0.2028]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "54/100 Running Loss: 1.6014569997787476\n",
      "tensor([[0.1807, 0.2289, 0.1828, 0.2095, 0.1980],\n",
      "        [0.1799, 0.2213, 0.1848, 0.2096, 0.2044],\n",
      "        [0.1795, 0.2217, 0.1845, 0.2098, 0.2046],\n",
      "        [0.1806, 0.2236, 0.1864, 0.2105, 0.1988],\n",
      "        [0.1865, 0.2230, 0.1869, 0.2067, 0.1969],\n",
      "        [0.1847, 0.2251, 0.1858, 0.2077, 0.1968],\n",
      "        [0.1844, 0.2242, 0.1836, 0.2086, 0.1991],\n",
      "        [0.1842, 0.2264, 0.1847, 0.2080, 0.1967],\n",
      "        [0.1866, 0.2212, 0.1864, 0.2064, 0.1994],\n",
      "        [0.1812, 0.2215, 0.1856, 0.2091, 0.2025]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "55/100 Running Loss: 1.6009931564331055\n",
      "tensor([[0.1797, 0.2301, 0.1827, 0.2100, 0.1975],\n",
      "        [0.1788, 0.2222, 0.1848, 0.2101, 0.2041],\n",
      "        [0.1783, 0.2226, 0.1845, 0.2103, 0.2043],\n",
      "        [0.1795, 0.2246, 0.1865, 0.2110, 0.1985],\n",
      "        [0.1855, 0.2240, 0.1870, 0.2070, 0.1964],\n",
      "        [0.1837, 0.2262, 0.1858, 0.2081, 0.1962],\n",
      "        [0.1835, 0.2252, 0.1836, 0.2090, 0.1987],\n",
      "        [0.1832, 0.2275, 0.1847, 0.2084, 0.1962],\n",
      "        [0.1857, 0.2221, 0.1865, 0.2068, 0.1989],\n",
      "        [0.1802, 0.2224, 0.1857, 0.2096, 0.2022]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "56/100 Running Loss: 1.6005159616470337\n",
      "tensor([[0.1786, 0.2314, 0.1827, 0.2104, 0.1970],\n",
      "        [0.1777, 0.2231, 0.1848, 0.2106, 0.2039],\n",
      "        [0.1772, 0.2235, 0.1845, 0.2107, 0.2040],\n",
      "        [0.1784, 0.2256, 0.1865, 0.2114, 0.1981],\n",
      "        [0.1846, 0.2251, 0.1870, 0.2074, 0.1959],\n",
      "        [0.1827, 0.2273, 0.1858, 0.2084, 0.1957],\n",
      "        [0.1825, 0.2262, 0.1836, 0.2094, 0.1982],\n",
      "        [0.1822, 0.2287, 0.1847, 0.2088, 0.1956],\n",
      "        [0.1848, 0.2231, 0.1865, 0.2071, 0.1985],\n",
      "        [0.1791, 0.2234, 0.1857, 0.2100, 0.2018]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "57/100 Running Loss: 1.600027322769165\n",
      "tensor([[0.1775, 0.2327, 0.1826, 0.2108, 0.1965],\n",
      "        [0.1765, 0.2241, 0.1849, 0.2110, 0.2035],\n",
      "        [0.1760, 0.2245, 0.1846, 0.2112, 0.2037],\n",
      "        [0.1772, 0.2267, 0.1866, 0.2119, 0.1977],\n",
      "        [0.1836, 0.2262, 0.1870, 0.2078, 0.1953],\n",
      "        [0.1817, 0.2285, 0.1858, 0.2088, 0.1951],\n",
      "        [0.1816, 0.2273, 0.1836, 0.2098, 0.1978],\n",
      "        [0.1812, 0.2299, 0.1846, 0.2092, 0.1951],\n",
      "        [0.1839, 0.2240, 0.1866, 0.2075, 0.1981],\n",
      "        [0.1780, 0.2243, 0.1858, 0.2104, 0.2014]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "58/100 Running Loss: 1.599527359008789\n",
      "tensor([[0.1763, 0.2341, 0.1825, 0.2112, 0.1959],\n",
      "        [0.1753, 0.2250, 0.1849, 0.2115, 0.2032],\n",
      "        [0.1749, 0.2254, 0.1846, 0.2117, 0.2034],\n",
      "        [0.1759, 0.2278, 0.1866, 0.2124, 0.1972],\n",
      "        [0.1827, 0.2273, 0.1871, 0.2082, 0.1948],\n",
      "        [0.1806, 0.2297, 0.1858, 0.2092, 0.1945],\n",
      "        [0.1806, 0.2284, 0.1836, 0.2101, 0.1973],\n",
      "        [0.1802, 0.2311, 0.1846, 0.2095, 0.1945],\n",
      "        [0.1829, 0.2250, 0.1866, 0.2078, 0.1976],\n",
      "        [0.1768, 0.2254, 0.1859, 0.2109, 0.2011]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "59/100 Running Loss: 1.5990103483200073\n",
      "tensor([[0.1751, 0.2355, 0.1824, 0.2116, 0.1953],\n",
      "        [0.1742, 0.2260, 0.1850, 0.2120, 0.2029],\n",
      "        [0.1737, 0.2265, 0.1847, 0.2122, 0.2030],\n",
      "        [0.1746, 0.2290, 0.1867, 0.2129, 0.1968],\n",
      "        [0.1816, 0.2285, 0.1871, 0.2085, 0.1942],\n",
      "        [0.1796, 0.2310, 0.1858, 0.2096, 0.1940],\n",
      "        [0.1796, 0.2295, 0.1836, 0.2105, 0.1968],\n",
      "        [0.1791, 0.2324, 0.1846, 0.2099, 0.1939],\n",
      "        [0.1819, 0.2261, 0.1867, 0.2082, 0.1971],\n",
      "        [0.1756, 0.2264, 0.1859, 0.2114, 0.2007]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "60/100 Running Loss: 1.5984777212142944\n",
      "tensor([[0.1739, 0.2370, 0.1824, 0.2120, 0.1947],\n",
      "        [0.1730, 0.2270, 0.1850, 0.2125, 0.2025],\n",
      "        [0.1725, 0.2275, 0.1847, 0.2127, 0.2027],\n",
      "        [0.1733, 0.2303, 0.1867, 0.2134, 0.1963],\n",
      "        [0.1806, 0.2297, 0.1871, 0.2089, 0.1936],\n",
      "        [0.1784, 0.2323, 0.1858, 0.2100, 0.1934],\n",
      "        [0.1785, 0.2307, 0.1836, 0.2109, 0.1963],\n",
      "        [0.1780, 0.2338, 0.1846, 0.2103, 0.1933],\n",
      "        [0.1809, 0.2271, 0.1868, 0.2085, 0.1967],\n",
      "        [0.1744, 0.2275, 0.1860, 0.2119, 0.2002]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "61/100 Running Loss: 1.5979280471801758\n",
      "tensor([[0.1726, 0.2386, 0.1823, 0.2124, 0.1940],\n",
      "        [0.1717, 0.2281, 0.1851, 0.2130, 0.2021],\n",
      "        [0.1712, 0.2286, 0.1848, 0.2131, 0.2023],\n",
      "        [0.1720, 0.2316, 0.1867, 0.2139, 0.1958],\n",
      "        [0.1795, 0.2310, 0.1872, 0.2093, 0.1931],\n",
      "        [0.1773, 0.2337, 0.1858, 0.2104, 0.1928],\n",
      "        [0.1775, 0.2319, 0.1836, 0.2112, 0.1957],\n",
      "        [0.1769, 0.2352, 0.1846, 0.2107, 0.1927],\n",
      "        [0.1799, 0.2282, 0.1868, 0.2089, 0.1962],\n",
      "        [0.1731, 0.2286, 0.1860, 0.2124, 0.1998]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "62/100 Running Loss: 1.5973613262176514\n",
      "tensor([[0.1713, 0.2403, 0.1822, 0.2129, 0.1934],\n",
      "        [0.1704, 0.2292, 0.1852, 0.2135, 0.2017],\n",
      "        [0.1699, 0.2297, 0.1848, 0.2136, 0.2019],\n",
      "        [0.1706, 0.2329, 0.1868, 0.2143, 0.1953],\n",
      "        [0.1784, 0.2323, 0.1872, 0.2097, 0.1925],\n",
      "        [0.1761, 0.2351, 0.1859, 0.2108, 0.1921],\n",
      "        [0.1764, 0.2332, 0.1837, 0.2116, 0.1952],\n",
      "        [0.1757, 0.2366, 0.1845, 0.2111, 0.1921],\n",
      "        [0.1788, 0.2294, 0.1869, 0.2093, 0.1957],\n",
      "        [0.1718, 0.2298, 0.1861, 0.2129, 0.1994]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "63/100 Running Loss: 1.5967727899551392\n",
      "tensor([[0.1700, 0.2419, 0.1822, 0.2133, 0.1926],\n",
      "        [0.1691, 0.2304, 0.1852, 0.2140, 0.2013],\n",
      "        [0.1686, 0.2309, 0.1849, 0.2141, 0.2015],\n",
      "        [0.1693, 0.2343, 0.1868, 0.2148, 0.1948],\n",
      "        [0.1772, 0.2336, 0.1873, 0.2100, 0.1919],\n",
      "        [0.1749, 0.2366, 0.1859, 0.2112, 0.1915],\n",
      "        [0.1752, 0.2346, 0.1837, 0.2120, 0.1946],\n",
      "        [0.1745, 0.2381, 0.1845, 0.2115, 0.1914],\n",
      "        [0.1777, 0.2306, 0.1869, 0.2096, 0.1951],\n",
      "        [0.1705, 0.2311, 0.1861, 0.2134, 0.1989]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "64/100 Running Loss: 1.596161127090454\n",
      "tensor([[0.1687, 0.2437, 0.1821, 0.2137, 0.1918],\n",
      "        [0.1678, 0.2315, 0.1853, 0.2144, 0.2010],\n",
      "        [0.1672, 0.2321, 0.1849, 0.2146, 0.2011],\n",
      "        [0.1679, 0.2358, 0.1869, 0.2153, 0.1942],\n",
      "        [0.1760, 0.2350, 0.1873, 0.2104, 0.1913],\n",
      "        [0.1737, 0.2381, 0.1859, 0.2116, 0.1908],\n",
      "        [0.1741, 0.2359, 0.1837, 0.2123, 0.1940],\n",
      "        [0.1733, 0.2397, 0.1845, 0.2118, 0.1907],\n",
      "        [0.1766, 0.2318, 0.1870, 0.2100, 0.1946],\n",
      "        [0.1691, 0.2323, 0.1862, 0.2139, 0.1985]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "65/100 Running Loss: 1.595519781112671\n",
      "tensor([[0.1673, 0.2455, 0.1821, 0.2141, 0.1910],\n",
      "        [0.1664, 0.2328, 0.1853, 0.2149, 0.2006],\n",
      "        [0.1659, 0.2333, 0.1850, 0.2151, 0.2007],\n",
      "        [0.1664, 0.2373, 0.1869, 0.2157, 0.1936],\n",
      "        [0.1748, 0.2365, 0.1873, 0.2107, 0.1907],\n",
      "        [0.1724, 0.2397, 0.1859, 0.2120, 0.1901],\n",
      "        [0.1729, 0.2374, 0.1837, 0.2127, 0.1933],\n",
      "        [0.1720, 0.2413, 0.1845, 0.2122, 0.1900],\n",
      "        [0.1754, 0.2331, 0.1870, 0.2104, 0.1940],\n",
      "        [0.1677, 0.2336, 0.1862, 0.2145, 0.1980]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "66/100 Running Loss: 1.5948522090911865\n",
      "tensor([[0.1659, 0.2474, 0.1820, 0.2145, 0.1902],\n",
      "        [0.1651, 0.2341, 0.1853, 0.2154, 0.2002],\n",
      "        [0.1645, 0.2346, 0.1849, 0.2156, 0.2004],\n",
      "        [0.1649, 0.2389, 0.1870, 0.2162, 0.1930],\n",
      "        [0.1735, 0.2380, 0.1874, 0.2111, 0.1900],\n",
      "        [0.1711, 0.2414, 0.1858, 0.2123, 0.1894],\n",
      "        [0.1717, 0.2389, 0.1837, 0.2131, 0.1927],\n",
      "        [0.1707, 0.2430, 0.1845, 0.2126, 0.1892],\n",
      "        [0.1742, 0.2345, 0.1871, 0.2107, 0.1935],\n",
      "        [0.1663, 0.2350, 0.1863, 0.2150, 0.1975]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "67/100 Running Loss: 1.5941565036773682\n",
      "tensor([[0.1645, 0.2493, 0.1819, 0.2149, 0.1894],\n",
      "        [0.1637, 0.2353, 0.1852, 0.2159, 0.1999],\n",
      "        [0.1631, 0.2358, 0.1849, 0.2161, 0.2001],\n",
      "        [0.1635, 0.2406, 0.1870, 0.2166, 0.1924],\n",
      "        [0.1722, 0.2396, 0.1874, 0.2114, 0.1893],\n",
      "        [0.1697, 0.2431, 0.1858, 0.2127, 0.1886],\n",
      "        [0.1704, 0.2405, 0.1837, 0.2134, 0.1920],\n",
      "        [0.1694, 0.2448, 0.1845, 0.2129, 0.1884],\n",
      "        [0.1730, 0.2359, 0.1871, 0.2111, 0.1929],\n",
      "        [0.1648, 0.2364, 0.1863, 0.2155, 0.1969]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "68/100 Running Loss: 1.5934356451034546\n",
      "tensor([[0.1630, 0.2513, 0.1818, 0.2153, 0.1886],\n",
      "        [0.1622, 0.2364, 0.1852, 0.2165, 0.1997],\n",
      "        [0.1617, 0.2369, 0.1849, 0.2166, 0.1999],\n",
      "        [0.1620, 0.2422, 0.1869, 0.2171, 0.1918],\n",
      "        [0.1709, 0.2413, 0.1874, 0.2117, 0.1886],\n",
      "        [0.1684, 0.2449, 0.1858, 0.2130, 0.1879],\n",
      "        [0.1692, 0.2421, 0.1837, 0.2138, 0.1913],\n",
      "        [0.1680, 0.2466, 0.1845, 0.2133, 0.1876],\n",
      "        [0.1718, 0.2374, 0.1872, 0.2114, 0.1922],\n",
      "        [0.1634, 0.2378, 0.1862, 0.2160, 0.1965]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "69/100 Running Loss: 1.5926885604858398\n",
      "tensor([[0.1616, 0.2533, 0.1816, 0.2158, 0.1878],\n",
      "        [0.1607, 0.2375, 0.1852, 0.2170, 0.1995],\n",
      "        [0.1601, 0.2381, 0.1849, 0.2172, 0.1997],\n",
      "        [0.1605, 0.2438, 0.1869, 0.2175, 0.1913],\n",
      "        [0.1696, 0.2431, 0.1875, 0.2120, 0.1878],\n",
      "        [0.1670, 0.2468, 0.1858, 0.2133, 0.1871],\n",
      "        [0.1679, 0.2438, 0.1837, 0.2141, 0.1906],\n",
      "        [0.1667, 0.2485, 0.1844, 0.2136, 0.1868],\n",
      "        [0.1705, 0.2389, 0.1872, 0.2118, 0.1916],\n",
      "        [0.1620, 0.2392, 0.1861, 0.2166, 0.1961]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "70/100 Running Loss: 1.5919181108474731\n",
      "tensor([[0.1601, 0.2555, 0.1814, 0.2161, 0.1869],\n",
      "        [0.1591, 0.2387, 0.1852, 0.2176, 0.1994],\n",
      "        [0.1585, 0.2393, 0.1849, 0.2178, 0.1995],\n",
      "        [0.1589, 0.2453, 0.1870, 0.2180, 0.1908],\n",
      "        [0.1682, 0.2449, 0.1875, 0.2123, 0.1871],\n",
      "        [0.1655, 0.2488, 0.1858, 0.2137, 0.1862],\n",
      "        [0.1665, 0.2455, 0.1837, 0.2144, 0.1898],\n",
      "        [0.1652, 0.2505, 0.1844, 0.2139, 0.1859],\n",
      "        [0.1692, 0.2405, 0.1872, 0.2122, 0.1909],\n",
      "        [0.1605, 0.2407, 0.1860, 0.2171, 0.1957]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "71/100 Running Loss: 1.591127634048462\n",
      "tensor([[0.1585, 0.2576, 0.1812, 0.2165, 0.1861],\n",
      "        [0.1575, 0.2399, 0.1852, 0.2182, 0.1991],\n",
      "        [0.1569, 0.2405, 0.1849, 0.2184, 0.1993],\n",
      "        [0.1572, 0.2469, 0.1871, 0.2185, 0.1903],\n",
      "        [0.1668, 0.2468, 0.1875, 0.2126, 0.1862],\n",
      "        [0.1641, 0.2507, 0.1857, 0.2140, 0.1854],\n",
      "        [0.1652, 0.2473, 0.1837, 0.2148, 0.1890],\n",
      "        [0.1638, 0.2525, 0.1843, 0.2143, 0.1851],\n",
      "        [0.1679, 0.2421, 0.1872, 0.2125, 0.1902],\n",
      "        [0.1589, 0.2420, 0.1861, 0.2176, 0.1953]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "72/100 Running Loss: 1.5903130769729614\n",
      "tensor([[0.1570, 0.2599, 0.1811, 0.2168, 0.1852],\n",
      "        [0.1558, 0.2412, 0.1853, 0.2188, 0.1989],\n",
      "        [0.1552, 0.2418, 0.1849, 0.2190, 0.1990],\n",
      "        [0.1555, 0.2486, 0.1872, 0.2190, 0.1898],\n",
      "        [0.1654, 0.2488, 0.1875, 0.2129, 0.1853],\n",
      "        [0.1626, 0.2528, 0.1856, 0.2143, 0.1846],\n",
      "        [0.1638, 0.2492, 0.1837, 0.2151, 0.1882],\n",
      "        [0.1623, 0.2546, 0.1842, 0.2146, 0.1843],\n",
      "        [0.1665, 0.2439, 0.1873, 0.2129, 0.1895],\n",
      "        [0.1573, 0.2433, 0.1862, 0.2182, 0.1950]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "73/100 Running Loss: 1.589469313621521\n",
      "tensor([[0.1553, 0.2623, 0.1810, 0.2171, 0.1843],\n",
      "        [0.1541, 0.2426, 0.1853, 0.2194, 0.1986],\n",
      "        [0.1535, 0.2432, 0.1850, 0.2196, 0.1987],\n",
      "        [0.1537, 0.2503, 0.1873, 0.2195, 0.1892],\n",
      "        [0.1639, 0.2509, 0.1875, 0.2133, 0.1845],\n",
      "        [0.1611, 0.2550, 0.1856, 0.2147, 0.1837],\n",
      "        [0.1624, 0.2511, 0.1836, 0.2154, 0.1875],\n",
      "        [0.1608, 0.2568, 0.1841, 0.2149, 0.1834],\n",
      "        [0.1652, 0.2456, 0.1872, 0.2132, 0.1888],\n",
      "        [0.1556, 0.2447, 0.1864, 0.2188, 0.1946]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "74/100 Running Loss: 1.5885900259017944\n",
      "tensor([[0.1536, 0.2648, 0.1809, 0.2174, 0.1833],\n",
      "        [0.1523, 0.2440, 0.1854, 0.2200, 0.1983],\n",
      "        [0.1517, 0.2446, 0.1850, 0.2202, 0.1984],\n",
      "        [0.1519, 0.2522, 0.1874, 0.2200, 0.1885],\n",
      "        [0.1624, 0.2530, 0.1874, 0.2136, 0.1835],\n",
      "        [0.1595, 0.2572, 0.1855, 0.2150, 0.1828],\n",
      "        [0.1609, 0.2531, 0.1836, 0.2157, 0.1866],\n",
      "        [0.1592, 0.2591, 0.1840, 0.2152, 0.1825],\n",
      "        [0.1638, 0.2474, 0.1872, 0.2136, 0.1880],\n",
      "        [0.1538, 0.2462, 0.1865, 0.2193, 0.1941]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "75/100 Running Loss: 1.587668538093567\n",
      "tensor([[0.1519, 0.2674, 0.1808, 0.2177, 0.1822],\n",
      "        [0.1505, 0.2454, 0.1855, 0.2207, 0.1980],\n",
      "        [0.1499, 0.2460, 0.1850, 0.2209, 0.1982],\n",
      "        [0.1500, 0.2541, 0.1875, 0.2204, 0.1879],\n",
      "        [0.1609, 0.2553, 0.1874, 0.2139, 0.1825],\n",
      "        [0.1579, 0.2595, 0.1855, 0.2152, 0.1818],\n",
      "        [0.1594, 0.2552, 0.1836, 0.2160, 0.1858],\n",
      "        [0.1576, 0.2614, 0.1840, 0.2155, 0.1816],\n",
      "        [0.1624, 0.2493, 0.1872, 0.2139, 0.1872],\n",
      "        [0.1521, 0.2477, 0.1866, 0.2199, 0.1937]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "76/100 Running Loss: 1.5867023468017578\n",
      "tensor([[0.1500, 0.2700, 0.1808, 0.2180, 0.1812],\n",
      "        [0.1486, 0.2468, 0.1855, 0.2214, 0.1977],\n",
      "        [0.1479, 0.2474, 0.1850, 0.2217, 0.1980],\n",
      "        [0.1481, 0.2562, 0.1876, 0.2209, 0.1872],\n",
      "        [0.1593, 0.2576, 0.1874, 0.2142, 0.1815],\n",
      "        [0.1563, 0.2619, 0.1855, 0.2155, 0.1808],\n",
      "        [0.1578, 0.2574, 0.1836, 0.2163, 0.1848],\n",
      "        [0.1559, 0.2638, 0.1839, 0.2157, 0.1806],\n",
      "        [0.1609, 0.2512, 0.1872, 0.2143, 0.1864],\n",
      "        [0.1502, 0.2492, 0.1867, 0.2206, 0.1933]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "77/100 Running Loss: 1.585695505142212\n",
      "tensor([[0.1480, 0.2728, 0.1808, 0.2184, 0.1801],\n",
      "        [0.1466, 0.2483, 0.1855, 0.2221, 0.1974],\n",
      "        [0.1460, 0.2488, 0.1850, 0.2224, 0.1977],\n",
      "        [0.1460, 0.2583, 0.1877, 0.2215, 0.1865],\n",
      "        [0.1577, 0.2599, 0.1875, 0.2144, 0.1805],\n",
      "        [0.1545, 0.2645, 0.1855, 0.2157, 0.1797],\n",
      "        [0.1562, 0.2597, 0.1836, 0.2166, 0.1839],\n",
      "        [0.1542, 0.2664, 0.1839, 0.2160, 0.1795],\n",
      "        [0.1593, 0.2531, 0.1873, 0.2147, 0.1857],\n",
      "        [0.1483, 0.2506, 0.1868, 0.2213, 0.1929]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "78/100 Running Loss: 1.5846513509750366\n",
      "tensor([[0.1461, 0.2757, 0.1807, 0.2186, 0.1789],\n",
      "        [0.1447, 0.2498, 0.1855, 0.2229, 0.1971],\n",
      "        [0.1440, 0.2504, 0.1850, 0.2231, 0.1975],\n",
      "        [0.1439, 0.2604, 0.1877, 0.2221, 0.1859],\n",
      "        [0.1560, 0.2624, 0.1875, 0.2147, 0.1794],\n",
      "        [0.1527, 0.2672, 0.1855, 0.2159, 0.1787],\n",
      "        [0.1546, 0.2620, 0.1836, 0.2169, 0.1829],\n",
      "        [0.1524, 0.2690, 0.1838, 0.2162, 0.1784],\n",
      "        [0.1576, 0.2550, 0.1875, 0.2150, 0.1849],\n",
      "        [0.1463, 0.2522, 0.1869, 0.2220, 0.1926]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "79/100 Running Loss: 1.583581566810608\n",
      "tensor([[0.1440, 0.2788, 0.1807, 0.2189, 0.1776],\n",
      "        [0.1426, 0.2515, 0.1855, 0.2236, 0.1968],\n",
      "        [0.1420, 0.2521, 0.1850, 0.2239, 0.1971],\n",
      "        [0.1418, 0.2627, 0.1877, 0.2227, 0.1851],\n",
      "        [0.1542, 0.2650, 0.1876, 0.2149, 0.1783],\n",
      "        [0.1508, 0.2699, 0.1856, 0.2161, 0.1775],\n",
      "        [0.1528, 0.2644, 0.1837, 0.2172, 0.1819],\n",
      "        [0.1506, 0.2718, 0.1838, 0.2164, 0.1774],\n",
      "        [0.1559, 0.2571, 0.1876, 0.2154, 0.1840],\n",
      "        [0.1443, 0.2538, 0.1870, 0.2227, 0.1921]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "80/100 Running Loss: 1.5824848413467407\n",
      "tensor([[0.1420, 0.2820, 0.1806, 0.2191, 0.1763],\n",
      "        [0.1406, 0.2533, 0.1855, 0.2243, 0.1964],\n",
      "        [0.1399, 0.2539, 0.1850, 0.2245, 0.1967],\n",
      "        [0.1397, 0.2650, 0.1877, 0.2232, 0.1844],\n",
      "        [0.1523, 0.2678, 0.1876, 0.2151, 0.1772],\n",
      "        [0.1489, 0.2729, 0.1856, 0.2163, 0.1764],\n",
      "        [0.1510, 0.2669, 0.1837, 0.2175, 0.1808],\n",
      "        [0.1487, 0.2748, 0.1838, 0.2166, 0.1762],\n",
      "        [0.1542, 0.2592, 0.1877, 0.2157, 0.1831],\n",
      "        [0.1423, 0.2555, 0.1870, 0.2235, 0.1917]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "81/100 Running Loss: 1.5813547372817993\n",
      "tensor([[0.1399, 0.2855, 0.1805, 0.2192, 0.1749],\n",
      "        [0.1385, 0.2553, 0.1854, 0.2249, 0.1959],\n",
      "        [0.1378, 0.2559, 0.1849, 0.2251, 0.1963],\n",
      "        [0.1375, 0.2675, 0.1876, 0.2238, 0.1835],\n",
      "        [0.1505, 0.2706, 0.1877, 0.2152, 0.1760],\n",
      "        [0.1470, 0.2760, 0.1856, 0.2164, 0.1751],\n",
      "        [0.1493, 0.2695, 0.1838, 0.2177, 0.1797],\n",
      "        [0.1467, 0.2779, 0.1837, 0.2167, 0.1750],\n",
      "        [0.1524, 0.2614, 0.1878, 0.2160, 0.1822],\n",
      "        [0.1402, 0.2574, 0.1871, 0.2241, 0.1912]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "82/100 Running Loss: 1.5801903009414673\n",
      "tensor([[0.1377, 0.2892, 0.1803, 0.2193, 0.1734],\n",
      "        [0.1364, 0.2573, 0.1854, 0.2255, 0.1954],\n",
      "        [0.1357, 0.2580, 0.1848, 0.2257, 0.1958],\n",
      "        [0.1353, 0.2702, 0.1876, 0.2243, 0.1826],\n",
      "        [0.1486, 0.2736, 0.1877, 0.2154, 0.1747],\n",
      "        [0.1450, 0.2792, 0.1855, 0.2165, 0.1738],\n",
      "        [0.1474, 0.2723, 0.1838, 0.2179, 0.1786],\n",
      "        [0.1448, 0.2812, 0.1836, 0.2167, 0.1737],\n",
      "        [0.1507, 0.2639, 0.1879, 0.2163, 0.1813],\n",
      "        [0.1382, 0.2595, 0.1871, 0.2247, 0.1906]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "83/100 Running Loss: 1.578993320465088\n",
      "tensor([[0.1355, 0.2931, 0.1801, 0.2194, 0.1720],\n",
      "        [0.1342, 0.2595, 0.1853, 0.2261, 0.1948],\n",
      "        [0.1335, 0.2602, 0.1847, 0.2264, 0.1952],\n",
      "        [0.1331, 0.2731, 0.1875, 0.2248, 0.1816],\n",
      "        [0.1466, 0.2768, 0.1876, 0.2155, 0.1735],\n",
      "        [0.1429, 0.2826, 0.1854, 0.2166, 0.1725],\n",
      "        [0.1456, 0.2753, 0.1838, 0.2180, 0.1774],\n",
      "        [0.1428, 0.2846, 0.1835, 0.2167, 0.1724],\n",
      "        [0.1488, 0.2664, 0.1879, 0.2165, 0.1804],\n",
      "        [0.1360, 0.2617, 0.1871, 0.2252, 0.1899]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "84/100 Running Loss: 1.5777578353881836\n",
      "tensor([[0.1332, 0.2972, 0.1798, 0.2193, 0.1704],\n",
      "        [0.1320, 0.2619, 0.1852, 0.2267, 0.1942],\n",
      "        [0.1313, 0.2626, 0.1846, 0.2269, 0.1945],\n",
      "        [0.1308, 0.2761, 0.1874, 0.2252, 0.1805],\n",
      "        [0.1447, 0.2801, 0.1874, 0.2156, 0.1722],\n",
      "        [0.1408, 0.2863, 0.1852, 0.2166, 0.1711],\n",
      "        [0.1437, 0.2785, 0.1837, 0.2180, 0.1762],\n",
      "        [0.1407, 0.2883, 0.1833, 0.2167, 0.1710],\n",
      "        [0.1469, 0.2690, 0.1879, 0.2168, 0.1794],\n",
      "        [0.1339, 0.2640, 0.1871, 0.2258, 0.1892]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "85/100 Running Loss: 1.5764751434326172\n",
      "tensor([[0.1309, 0.3015, 0.1795, 0.2192, 0.1687],\n",
      "        [0.1298, 0.2643, 0.1851, 0.2273, 0.1935],\n",
      "        [0.1291, 0.2651, 0.1845, 0.2275, 0.1938],\n",
      "        [0.1286, 0.2792, 0.1873, 0.2255, 0.1793],\n",
      "        [0.1426, 0.2836, 0.1873, 0.2156, 0.1708],\n",
      "        [0.1387, 0.2901, 0.1850, 0.2166, 0.1696],\n",
      "        [0.1417, 0.2817, 0.1836, 0.2180, 0.1749],\n",
      "        [0.1386, 0.2921, 0.1832, 0.2166, 0.1695],\n",
      "        [0.1450, 0.2719, 0.1879, 0.2169, 0.1784],\n",
      "        [0.1317, 0.2665, 0.1871, 0.2263, 0.1884]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "86/100 Running Loss: 1.575149655342102\n",
      "tensor([[0.1286, 0.3061, 0.1792, 0.2191, 0.1670],\n",
      "        [0.1276, 0.2669, 0.1850, 0.2278, 0.1927],\n",
      "        [0.1269, 0.2678, 0.1844, 0.2280, 0.1930],\n",
      "        [0.1262, 0.2826, 0.1872, 0.2258, 0.1781],\n",
      "        [0.1405, 0.2874, 0.1872, 0.2156, 0.1693],\n",
      "        [0.1365, 0.2941, 0.1849, 0.2165, 0.1681],\n",
      "        [0.1397, 0.2852, 0.1835, 0.2180, 0.1736],\n",
      "        [0.1365, 0.2961, 0.1830, 0.2165, 0.1679],\n",
      "        [0.1431, 0.2748, 0.1878, 0.2170, 0.1773],\n",
      "        [0.1295, 0.2692, 0.1871, 0.2268, 0.1875]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/100 Running Loss: 1.5737767219543457\n",
      "tensor([[0.1262, 0.3109, 0.1788, 0.2189, 0.1652],\n",
      "        [0.1253, 0.2697, 0.1849, 0.2283, 0.1918],\n",
      "        [0.1246, 0.2706, 0.1842, 0.2285, 0.1921],\n",
      "        [0.1239, 0.2862, 0.1871, 0.2260, 0.1768],\n",
      "        [0.1384, 0.2914, 0.1870, 0.2155, 0.1677],\n",
      "        [0.1343, 0.2983, 0.1846, 0.2163, 0.1664],\n",
      "        [0.1376, 0.2888, 0.1833, 0.2180, 0.1722],\n",
      "        [0.1343, 0.3003, 0.1827, 0.2163, 0.1663],\n",
      "        [0.1411, 0.2780, 0.1877, 0.2171, 0.1761],\n",
      "        [0.1272, 0.2720, 0.1870, 0.2272, 0.1866]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "88/100 Running Loss: 1.5723507404327393\n",
      "tensor([[0.1238, 0.3159, 0.1784, 0.2186, 0.1633],\n",
      "        [0.1230, 0.2726, 0.1848, 0.2287, 0.1909],\n",
      "        [0.1222, 0.2736, 0.1841, 0.2289, 0.1911],\n",
      "        [0.1214, 0.2902, 0.1869, 0.2262, 0.1754],\n",
      "        [0.1362, 0.2956, 0.1868, 0.2154, 0.1661],\n",
      "        [0.1320, 0.3028, 0.1843, 0.2161, 0.1647],\n",
      "        [0.1356, 0.2927, 0.1831, 0.2178, 0.1708],\n",
      "        [0.1322, 0.3047, 0.1825, 0.2160, 0.1646],\n",
      "        [0.1391, 0.2813, 0.1876, 0.2171, 0.1749],\n",
      "        [0.1249, 0.2750, 0.1870, 0.2276, 0.1856]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "89/100 Running Loss: 1.5708671808242798\n",
      "tensor([[0.1213, 0.3213, 0.1780, 0.2182, 0.1612],\n",
      "        [0.1207, 0.2757, 0.1846, 0.2291, 0.1900],\n",
      "        [0.1199, 0.2767, 0.1840, 0.2293, 0.1902],\n",
      "        [0.1189, 0.2943, 0.1866, 0.2262, 0.1739],\n",
      "        [0.1339, 0.3000, 0.1866, 0.2152, 0.1644],\n",
      "        [0.1297, 0.3075, 0.1841, 0.2158, 0.1629],\n",
      "        [0.1334, 0.2968, 0.1829, 0.2176, 0.1693],\n",
      "        [0.1299, 0.3094, 0.1822, 0.2157, 0.1628],\n",
      "        [0.1369, 0.2849, 0.1875, 0.2170, 0.1736],\n",
      "        [0.1225, 0.2782, 0.1869, 0.2279, 0.1845]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "90/100 Running Loss: 1.5693283081054688\n",
      "tensor([[0.1187, 0.3269, 0.1775, 0.2178, 0.1591],\n",
      "        [0.1183, 0.2788, 0.1844, 0.2294, 0.1890],\n",
      "        [0.1175, 0.2799, 0.1838, 0.2296, 0.1892],\n",
      "        [0.1164, 0.2986, 0.1864, 0.2262, 0.1724],\n",
      "        [0.1316, 0.3045, 0.1863, 0.2149, 0.1626],\n",
      "        [0.1273, 0.3123, 0.1837, 0.2156, 0.1611],\n",
      "        [0.1312, 0.3011, 0.1827, 0.2174, 0.1677],\n",
      "        [0.1275, 0.3142, 0.1818, 0.2154, 0.1610],\n",
      "        [0.1347, 0.2887, 0.1874, 0.2169, 0.1722],\n",
      "        [0.1201, 0.2817, 0.1868, 0.2281, 0.1834]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "91/100 Running Loss: 1.5677393674850464\n",
      "tensor([[0.1161, 0.3328, 0.1770, 0.2172, 0.1569],\n",
      "        [0.1159, 0.2822, 0.1842, 0.2297, 0.1880],\n",
      "        [0.1151, 0.2834, 0.1835, 0.2299, 0.1881],\n",
      "        [0.1138, 0.3031, 0.1861, 0.2262, 0.1708],\n",
      "        [0.1292, 0.3093, 0.1861, 0.2146, 0.1608],\n",
      "        [0.1248, 0.3174, 0.1834, 0.2152, 0.1592],\n",
      "        [0.1289, 0.3056, 0.1825, 0.2171, 0.1660],\n",
      "        [0.1251, 0.3193, 0.1815, 0.2150, 0.1591],\n",
      "        [0.1325, 0.2927, 0.1872, 0.2168, 0.1708],\n",
      "        [0.1176, 0.2852, 0.1866, 0.2283, 0.1822]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "92/100 Running Loss: 1.5660984516143799\n",
      "tensor([[0.1134, 0.3390, 0.1764, 0.2166, 0.1546],\n",
      "        [0.1134, 0.2858, 0.1840, 0.2299, 0.1869],\n",
      "        [0.1126, 0.2870, 0.1833, 0.2302, 0.1870],\n",
      "        [0.1112, 0.3077, 0.1859, 0.2260, 0.1691],\n",
      "        [0.1268, 0.3142, 0.1858, 0.2142, 0.1590],\n",
      "        [0.1223, 0.3227, 0.1830, 0.2148, 0.1572],\n",
      "        [0.1266, 0.3103, 0.1823, 0.2167, 0.1642],\n",
      "        [0.1227, 0.3247, 0.1811, 0.2145, 0.1571],\n",
      "        [0.1302, 0.2968, 0.1871, 0.2166, 0.1694],\n",
      "        [0.1152, 0.2889, 0.1865, 0.2285, 0.1810]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "93/100 Running Loss: 1.5644022226333618\n",
      "tensor([[0.1107, 0.3454, 0.1758, 0.2159, 0.1522],\n",
      "        [0.1109, 0.2894, 0.1838, 0.2301, 0.1857],\n",
      "        [0.1100, 0.2908, 0.1830, 0.2304, 0.1858],\n",
      "        [0.1086, 0.3125, 0.1856, 0.2258, 0.1674],\n",
      "        [0.1243, 0.3194, 0.1855, 0.2138, 0.1571],\n",
      "        [0.1197, 0.3282, 0.1826, 0.2143, 0.1552],\n",
      "        [0.1242, 0.3152, 0.1820, 0.2162, 0.1624],\n",
      "        [0.1201, 0.3302, 0.1807, 0.2140, 0.1550],\n",
      "        [0.1279, 0.3010, 0.1869, 0.2164, 0.1678],\n",
      "        [0.1127, 0.2927, 0.1863, 0.2286, 0.1797]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "94/100 Running Loss: 1.5626572370529175\n",
      "tensor([[0.1080, 0.3520, 0.1752, 0.2151, 0.1498],\n",
      "        [0.1084, 0.2931, 0.1835, 0.2303, 0.1846],\n",
      "        [0.1075, 0.2946, 0.1828, 0.2305, 0.1846],\n",
      "        [0.1060, 0.3174, 0.1853, 0.2256, 0.1657],\n",
      "        [0.1218, 0.3247, 0.1852, 0.2132, 0.1551],\n",
      "        [0.1171, 0.3339, 0.1822, 0.2137, 0.1531],\n",
      "        [0.1218, 0.3203, 0.1817, 0.2157, 0.1605],\n",
      "        [0.1176, 0.3360, 0.1802, 0.2133, 0.1529],\n",
      "        [0.1255, 0.3054, 0.1867, 0.2161, 0.1662],\n",
      "        [0.1101, 0.2965, 0.1862, 0.2287, 0.1785]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "95/100 Running Loss: 1.5608561038970947\n",
      "tensor([[0.1052, 0.3589, 0.1745, 0.2142, 0.1473],\n",
      "        [0.1058, 0.2969, 0.1833, 0.2305, 0.1835],\n",
      "        [0.1049, 0.2985, 0.1825, 0.2307, 0.1834],\n",
      "        [0.1034, 0.3223, 0.1850, 0.2253, 0.1640],\n",
      "        [0.1192, 0.3303, 0.1848, 0.2126, 0.1531],\n",
      "        [0.1145, 0.3399, 0.1817, 0.2130, 0.1509],\n",
      "        [0.1193, 0.3257, 0.1814, 0.2151, 0.1585],\n",
      "        [0.1150, 0.3420, 0.1798, 0.2126, 0.1507],\n",
      "        [0.1231, 0.3100, 0.1865, 0.2158, 0.1646],\n",
      "        [0.1076, 0.3005, 0.1860, 0.2287, 0.1772]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "96/100 Running Loss: 1.558998703956604\n",
      "tensor([[0.1023, 0.3660, 0.1737, 0.2132, 0.1447],\n",
      "        [0.1033, 0.3008, 0.1830, 0.2306, 0.1823],\n",
      "        [0.1024, 0.3024, 0.1822, 0.2308, 0.1822],\n",
      "        [0.1007, 0.3275, 0.1847, 0.2249, 0.1623],\n",
      "        [0.1166, 0.3361, 0.1844, 0.2119, 0.1509],\n",
      "        [0.1118, 0.3461, 0.1812, 0.2123, 0.1487],\n",
      "        [0.1168, 0.3312, 0.1811, 0.2144, 0.1565],\n",
      "        [0.1123, 0.3482, 0.1793, 0.2117, 0.1485],\n",
      "        [0.1207, 0.3147, 0.1863, 0.2154, 0.1629],\n",
      "        [0.1051, 0.3045, 0.1857, 0.2288, 0.1759]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "97/100 Running Loss: 1.5570815801620483\n",
      "tensor([[0.0994, 0.3735, 0.1729, 0.2121, 0.1421],\n",
      "        [0.1008, 0.3047, 0.1827, 0.2307, 0.1810],\n",
      "        [0.0998, 0.3064, 0.1819, 0.2309, 0.1809],\n",
      "        [0.0980, 0.3327, 0.1843, 0.2245, 0.1604],\n",
      "        [0.1140, 0.3421, 0.1840, 0.2111, 0.1487],\n",
      "        [0.1091, 0.3525, 0.1807, 0.2114, 0.1464],\n",
      "        [0.1143, 0.3370, 0.1807, 0.2137, 0.1543],\n",
      "        [0.1096, 0.3547, 0.1787, 0.2108, 0.1462],\n",
      "        [0.1183, 0.3195, 0.1861, 0.2150, 0.1612],\n",
      "        [0.1025, 0.3087, 0.1855, 0.2288, 0.1745]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "98/100 Running Loss: 1.5551061630249023\n",
      "tensor([[0.0965, 0.3813, 0.1720, 0.2109, 0.1393],\n",
      "        [0.0982, 0.3088, 0.1824, 0.2308, 0.1798],\n",
      "        [0.0973, 0.3106, 0.1816, 0.2310, 0.1796],\n",
      "        [0.0953, 0.3382, 0.1839, 0.2240, 0.1585],\n",
      "        [0.1113, 0.3484, 0.1836, 0.2103, 0.1465],\n",
      "        [0.1063, 0.3591, 0.1802, 0.2104, 0.1440],\n",
      "        [0.1117, 0.3430, 0.1802, 0.2129, 0.1521],\n",
      "        [0.1069, 0.3614, 0.1781, 0.2098, 0.1438],\n",
      "        [0.1158, 0.3245, 0.1858, 0.2145, 0.1594],\n",
      "        [0.1000, 0.3129, 0.1853, 0.2287, 0.1731]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "99/100 Running Loss: 1.5530728101730347\n",
      "tensor([[0.0936, 0.3894, 0.1710, 0.2095, 0.1365],\n",
      "        [0.0957, 0.3129, 0.1821, 0.2308, 0.1785],\n",
      "        [0.0947, 0.3148, 0.1812, 0.2310, 0.1783],\n",
      "        [0.0927, 0.3438, 0.1835, 0.2235, 0.1566],\n",
      "        [0.1086, 0.3549, 0.1831, 0.2093, 0.1441],\n",
      "        [0.1036, 0.3660, 0.1795, 0.2093, 0.1415],\n",
      "        [0.1092, 0.3493, 0.1797, 0.2120, 0.1498],\n",
      "        [0.1041, 0.3684, 0.1774, 0.2088, 0.1413],\n",
      "        [0.1133, 0.3297, 0.1854, 0.2140, 0.1576],\n",
      "        [0.0974, 0.3172, 0.1850, 0.2287, 0.1717]], grad_fn=<SliceBackward>) tensor([1, 3, 3, 2, 1, 1, 1, 1, 2, 3])\n",
      "100/100 Running Loss: 1.550980567932129\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.39      1.00      0.56        20\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.00      0.00      0.00         6\n",
      "\n",
      "   micro avg       0.39      0.39      0.39        51\n",
      "   macro avg       0.10      0.25      0.14        51\n",
      "weighted avg       0.15      0.39      0.22        51\n",
      "\n",
      "0.14084507042253522\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_idx, test_idx in skf.split(x, y):\n",
    "    lm = LinearModel(x.shape[1], profiles.category_1.nunique())\n",
    "    lm.fit(x[train_idx], y[train_idx], n_epochs=100)\n",
    "    \n",
    "    y_preds = lm.forward(x[test_idx])\n",
    "    y_preds = torch.argmax(y_preds, dim=1).detach().numpy()\n",
    "    y_true = y[test_idx].detach().numpy()\n",
    "    \n",
    "    print(classification_report(y_true, y_preds))\n",
    "    print(f1_score(y_true, y_preds, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "train = dataset.data.train_mask\n",
    "test = dataset.data.test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> (1/10) Running loss: 1.945223331451416\n",
      "---> (2/10) Running loss: 1.7960033416748047\n",
      "---> (3/10) Running loss: 1.550907015800476\n",
      "---> (4/10) Running loss: 1.2255967855453491\n",
      "---> (5/10) Running loss: 0.880815327167511\n",
      "---> (6/10) Running loss: 0.5701509118080139\n",
      "---> (7/10) Running loss: 0.34409472346305847\n",
      "---> (8/10) Running loss: 0.19866172969341278\n",
      "---> (9/10) Running loss: 0.11486313492059708\n",
      "---> (10/10) Running loss: 0.0645274966955185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.945223331451416,\n",
       " 1.7960033416748047,\n",
       " 1.550907015800476,\n",
       " 1.2255967855453491,\n",
       " 0.880815327167511,\n",
       " 0.5701509118080139,\n",
       " 0.34409472346305847,\n",
       " 0.19866172969341278,\n",
       " 0.11486313492059708,\n",
       " 0.0645274966955185]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.optim import Adam\n",
    "from torch.nn import NLLLoss\n",
    "\n",
    "\n",
    "class GCNModel(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_hidden_units, n_classes, lr=0.01, n_hidden_layers=1, **kwargs):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.convs = [GCNConv(n_features, n_hidden_units)] + [GCNConv(n_hidden_units, n_hidden_units) for _ in range(n_hidden_layers-1)]\n",
    "        self.convs = torch.nn.Sequential(*self.convs)\n",
    "        self.output = GCNConv(n_hidden_units, n_classes)\n",
    "        \n",
    "        self.loss = NLLLoss()\n",
    "        self.optimizer = Adam(self.parameters(), lr=lr, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, apply_activation=True):\n",
    "        for layer in self.convs:\n",
    "            x = F.relu(layer(x, edge_index))\n",
    "        return F.log_softmax(self.output(x, edge_index), dim=1) if apply_activation else x\n",
    "    \n",
    "    def fit(self, data, epochs=10, mask=None):\n",
    "        self.train()\n",
    "        history = []\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.forward(data.x, data.edge_index)\n",
    "            loss = self.loss(outputs[mask], data.y[mask])\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "            print(\"---> ({}/{}) Running loss: {}\".format(epoch+1, epochs, loss.item()))\n",
    "            history.append(loss.item())\n",
    "\n",
    "        return history\n",
    "\n",
    "gcn = GCNModel(dataset.num_features, 64, dataset.num_classes, n_hidden_layers=2)\n",
    "gcn.fit(dataset.data, mask=train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7905282012131185"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = gcn.forward(dataset.data.x, dataset.data.edge_index), \n",
    "preds = torch.argmax(y_preds[0], dim=1)\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(preds[test], dataset.data.y[test], average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
